##################################
# TBD
##################################
1. netlink send 함수는 rpci어디에 있는가?

static struct pernet_operations rdma_dev_net_ops = {
	.init = rdma_dev_init_net,
	.exit = rdma_dev_exit_net,
	.id = &rdma_dev_net_id,
	.size = sizeof(struct rdma_dev_net),
};

ib_core_init(void)
-> register_pernet_device(&rdma_dev_net_ops);

struct netlink_kernel_cfg cfg = {
	.input	= rdma_nl_rcv,
};

#define NETLINK_RDMA		20

net_ns_init()
-> setup_net()
->-> list_for_each_entry(ops, &pernet_list, list) ops_init(ops, net);
->->-> ops->init(net);
->->->-> rdma_dev_init_net()
->->->->-> rdma_nl_net_init()
->->->->->-> netlink_kernel_create(net, NETLINK_RDMA, &cfg);


-> rdma_nl_rcv()
->-> rdma_nl_rcv_skb()
->-> rdma_nl_rcv_msg()
->->-> cb_table = get_cb_table(skb, index, op)
->->-> cb_table[op].doit(skb, nlh, extack)
->->->-> nldev_newlink()
->->->->-> ops = link_ops_get(type);
->->->->-> ops->newlink(ibdev_name, ndev) -> rpci_newlink()
->->->->->-> rdev = ib_alloc_device(rpci_dev, ib_dev);
->->->->->->->
->->->->->-> rpci_init(rdev);
->->->->->->-> rpci_init_device_param(rdev)
->->->->->->-> rpci_init_ports(rdev)
->->->->->->-> rpci_init_pools(rdev)
->->->->->-> rpci_pci_init(rdev, inited_pdev);
->->->->->-> rdev->ndev = ndev;
->->->->->-> rpci_set_mtu(rdev, 0x400);
->->->->->-> rpci_register_device(rdev, ibdev_name)
->->->->->->-> set rpci->ib_dev's fields
->->->->->->-> ib_set_device_ops(dev, &rpci_dev_ops)
->->->->->->->-> ib_device->ops->{mmap,...} = rpci_dev_ops->{mmap,...}
->->->->->->-> ib_device_set_netdev(&rpci->ib_dev, rpci->ndev, 1)

static const struct ib_device_ops rpci_dev_ops = {
	.mmap = rpci_mmap,
};

static struct rdma_link_ops rpci_link_ops = {
	.type = "rpci",
	.newlink = rpci_newlink,
};

static struct pci_driver rpci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= rpci_pci_table,
	.probe		= rpci_pci_probe,
	.remove		= rpci_pci_remove,
};


rpci_module_init()
-> pci_register_driver(&rpci_pci_driver)
->-> __pci_register_driver(driver, THIS_MODULE, KBUILD_MODNAME)
->->-> driver_register();
-> rdma_link_register(&rpci_link_ops)
->-> list_add(&rpci_link_ops->list, &link_ops)

rpci_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-> pci_enable_device(pdev)
-> rpci_alloc_bars(pdev);
-> pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
-> pci_set_master(pdev);
-> inited_pdev = pdev;

##################################
# /dev/infiniband/rdma_cm
##################################

static struct miscdevice ucma_misc = {
	.minor          = MISC_DYNAMIC_MINOR,
	.name           = "rdma_cm",
	.nodename       = "infiniband/rdma_cm",
	.mode           = 0666,
	.fops           = &ucma_fops,
};

static const struct file_operations ucma_fops = {
	.owner   = THIS_MODULE,
	.open    = ucma_open,
	.release = ucma_close,
	.write   = ucma_write,
	.poll    = ucma_poll,
	.llseek  = no_llseek,
};

static ssize_t (*ucma_cmd_table[])(struct ucma_file *file,
				   const char __user *inbuf,
				   int in_len, int out_len) = {
	[RDMA_USER_CM_CMD_CREATE_ID] 	 = ucma_create_id,
	[RDMA_USER_CM_CMD_DESTROY_ID]	 = ucma_destroy_id,
	[RDMA_USER_CM_CMD_BIND_IP]	 = ucma_bind_ip,
	[RDMA_USER_CM_CMD_RESOLVE_IP]	 = ucma_resolve_ip,
	[RDMA_USER_CM_CMD_RESOLVE_ROUTE] = ucma_resolve_route,
	[RDMA_USER_CM_CMD_QUERY_ROUTE]	 = ucma_query_route,
	[RDMA_USER_CM_CMD_CONNECT]	 = ucma_connect,
	[RDMA_USER_CM_CMD_LISTEN]	 = ucma_listen,
	[RDMA_USER_CM_CMD_ACCEPT]	 = ucma_accept,
	[RDMA_USER_CM_CMD_REJECT]	 = ucma_reject,
	[RDMA_USER_CM_CMD_DISCONNECT]	 = ucma_disconnect,
	[RDMA_USER_CM_CMD_INIT_QP_ATTR]	 = ucma_init_qp_attr,
	[RDMA_USER_CM_CMD_GET_EVENT]	 = ucma_get_event,
	[RDMA_USER_CM_CMD_GET_OPTION]	 = NULL,
	[RDMA_USER_CM_CMD_SET_OPTION]	 = ucma_set_option,
	[RDMA_USER_CM_CMD_NOTIFY]	 = ucma_notify,
	[RDMA_USER_CM_CMD_JOIN_IP_MCAST] = ucma_join_ip_multicast,
	[RDMA_USER_CM_CMD_LEAVE_MCAST]	 = ucma_leave_multicast,
	[RDMA_USER_CM_CMD_MIGRATE_ID]	 = ucma_migrate_id,
	[RDMA_USER_CM_CMD_QUERY]	 = ucma_query,
	[RDMA_USER_CM_CMD_BIND]		 = ucma_bind,
	[RDMA_USER_CM_CMD_RESOLVE_ADDR]	 = ucma_resolve_addr,
	[RDMA_USER_CM_CMD_JOIN_MCAST]	 = ucma_join_multicast
};

ucma_init()
> misc_register(&ucma_misc);
> device_create_file(ucma_misc.this_device, &dev_attr_abi_version);
> ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
> ib_register_client(&rdma_cma_client);
>> assign_client_id(client);
>> xa_for_each_marked (&devices, index, device, DEVICE_REGISTERED) add_client_context(device, client);

ucam_open()
> file = kmalloc(sizeof *file, GFP_KERNEL); // struct ucma_file *file
> file->close_wq = alloc_ordered_workqueue("ucma_close_id", WQ_MEM_RECLAIM);
> INIT_LIST_HEAD(&file->event_list); INIT_LIST_HEAD(&file->ctx_list); init_waitqueue_head(&file->poll_wait); mutex_init(&file->mut);
> filp->private_data = file; file->filp = filp;
> stream_open(inode, filp);

ucma_poll()
> struct ucma_file *file = filp->private_data;
> poll_wait(filp, &file->poll_wait, wait);
> if (!list_empty(&file->event_list)) mask = EPOLLIN | EPOLLRDNORM;
> return mask

static ssize_t ucma_write(struct file *filp, const char __user *buf, size_t len, loff_t *pos)
> struct ucma_file *file = filp->private_data;
> copy_from_user(&hdr, buf, sizeof(hdr))
> hdr.cmd = array_index_nospec(hdr.cmd, ARRAY_SIZE(ucma_cmd_table));
> ucma_cmd_table[hdr.cmd](file, buf + sizeof(hdr), hdr.in, hdr.out);

// ucma_bind when hdr.cmd is [RDMA_USER_CM_CMD_BIND]
static ssize_t ucma_bind(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
>>> ctx = xa_load(&ctx_table, id);
> rdma_bind_addr(ctx->cm_id, (struct sockaddr *) &cmd.addr);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> cma_check_linklocal(&id->route.addr.dev_addr, addr);
>>> sin6 = (struct sockaddr_in6 *) addr;
>>> dev_addr->bound_dev_if = sin6->sin6_scope_id;
>> memcpy(cma_src_addr(id_priv), addr, rdma_addr_size(addr));
>> daddr = cma_dst_addr(id_priv); daddr->sa_family = addr->sa_family;
>>> (struct sockaddr *) &id_priv->id.route.addr.dst_addr;
>> daddr->sa_family = addr->sa_family;
>> cma_get_port(id_priv);
>>> if (cma_family(id_priv) != AF_IB) ps = cma_select_inet_ps(id_priv);
>>> else ps = cma_select_ib_ps(id_priv);
>>> if (cma_any_port(cma_src_addr(id_priv))) >>>  cma_alloc_any_port(ps, id_priv);
>>> else cma_use_port(ps, id_priv);
>>>>>> case RDMA_PS_IB: return &pernet->ib_ps;
>>>>> return xa_load(xa, snum);

static ssize_t ucma_create_id(struct ucma_file *file, const char __user *inbuf,	int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ucma_get_qp_type(&cmd, &qp_type);
>> switch (cmd->ps) {
>> case RDMA_PS_TCP: *qp_type = IB_QPT_RC;
>> case RDMA_PS_{UDP,IPOIB}: *qp_type = IB_QPT_UD;
>> case RDMA_PS_IB: *qp_type = cmd->qp_type;
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> ctx->uid = cmd.uid;
> cm_id = __rdma_create_id(current->nsproxy->net_ns, ucma_event_handler, ctx, cmd.ps, qp_type, NULL);
>> kzalloc and setting id_priv
> resp.id = ctx->id;
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))
> ctx->cm_id = cm_id;

static ssize_t ucma_query(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> response = u64_to_user_ptr(cmd.response);
> ctx = ucma_get_ctx(file, cmd.id);
> switch (cmd.option) {
> case RDMA_USER_CM_QUERY_ADDR: ret = ucma_query_addr(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_PATH: ret = ucma_query_path(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_GID: ret = ucma_query_gid(ctx, response, out_len);
> ucma_put_ctx(ctx);

static ssize_t ucma_destroy_id(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = _ucma_find_context(cmd.id, file);
> ctx->destroying = 1;
> flush_workqueue(ctx->file->close_wq);
> if (!ctx->closing) {
>  wait_for_completion(&ctx->comp);
tbd
>  rdma_destroy_id(ctx->cm_id);}
> resp.events_reported = ucma_free_ctx(ctx);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

static ssize_t ucma_resolve_addr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
> rdma_resolve_addr(ctx->cm_id, (struct sockaddr *) &cmd.src_addr, (struct sockaddr *) &cmd.dst_addr, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> memcpy(cma_dst_addr(id_priv), dst_addr, rdma_addr_size(dst_addr));
>> if (id_priv->state == RDMA_CM_IDLE) {
>>  cma_bind_addr(id, src_addr, dst_addr);
>>> if (!src_addr || !src_addr->sa_family) {
>>>  src_addr = &id->route.addr.src_addr;
>>>  src_addr->sa_family = dst_addr->sa_family;
>>>  if (IS_ENABLED(CONFIG_IPV6) && dst_addr->sa_family == AF_INET6) {
>>>   set src_addr6, dst_addr6, src_addr6->sin6_scope_id
>>>   if (ipv6_addr_type(&dst_addr6->sin6_addr) & IPV6_ADDR_LINKLOCAL) id->route.addr.dev_addr.bound_dev_if = dst_addr6->sin6_scope_id;
>>>   else if (dst_addr->sa_family == AF_IB) src_addr->sib_pkey = dst_addr->sib_pkey;
>>>  }
>>> }
>>> return rdma_bind_addr(id, src_addr);
>>  if (ret) memset(cma_dst_addr(id_priv), 0, rdma_addr_size(dst_addr));
>> }
>> if (cma_any_addr(dst_addr)) cma_resolve_loopback(id_priv);
>>> // cma_resolve_loopback()
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> if (!id_priv->cma_dev) cma_bind_loopback(id_priv);
>>>> list_for_each_entry(cur_dev, &dev_list, list) {
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p) {
>>>>   if (!ib_get_cached_port_state(cur_dev->device, p, &port_state) && port_state == IB_PORT_ACTIVE) {
>>>> 	cma_dev = cur_dev;
>>>> 	goto port_found;
>>>> port_found:
>>>> rdma_query_gid(cma_dev->device, p, 0, &gid);
>>>> ib_get_cached_pkey(cma_dev->device, p, 0, &pkey);
>>>>> read_lock_irqsave(&device->cache.lock, flags);
>>>>> cache = device->port_data[port_num].cache.pkey;
>>>>> *pkey = cache->table[index];
>>>> id_priv->id.route.addr.dev_addr.dev_type = (rdma_protocol_ib(cma_dev->device, p)) ? ARPHRD_INFINIBAND : ARPHRD_ETHER;
>>>> rdma_addr_set_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>>>>  memcpy(dev_addr->src_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>> ib_addr_set_pkey(&id_priv->id.route.addr.dev_addr, pkey);
>>>>> dev_addr->broadcast[8] = pkey >> 8;dev_addr->broadcast[9] = (unsigned char) pkey;
>>>> id_priv->id.port_num = p;
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>>> _cma_attach_to_dev(id_priv, cma_dev);
>>>>>> cma_ref_dev(cma_dev);
>>>>>> id_priv->cma_dev = cma_dev;
>>>>>> id_priv->id.device = cma_dev->device;
>>>>>> id_priv->id.route.addr.dev_addr.transport = rdma_node_get_transport(cma_dev->device->node_type);
>>>>>> list_add_tail(&id_priv->list, &cma_dev->id_list);
>>>>>> if (id_priv->res.kern_name) rdma_restrack_kadd(&id_priv->res);
>>>>>>> set_kern_name(res);
>>>>>>> rdma_restrack_add(res);
>>>>>>>> struct ib_device *dev = res_to_dev(res);
>>>>>>>> rt = &dev->res[res->type];
>>>>>>>> kref_init(&res->kref);
>>>>>>>> init_completion(&res->comp);
>>>>>>>> if (res->type == RDMA_RESTRACK_QP) {
>>>>>>>>  struct ib_qp *qp = container_of(res, struct ib_qp, res);
>>>>>>>>  ret = xa_insert(&rt->xa, qp->qp_num, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : qp->qp_num;
>>>>>>>> } else if (res->type == RDMA_RESTRACK_COUNTER) {
>>>>>>>>  struct rdma_counter *counter;
>>>>>>>>  counter = container_of(res, struct rdma_counter, res);
>>>>>>>>  ret = xa_insert(&rt->xa, counter->id, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : counter->id;
>>>>>>>> } else ret = xa_alloc_cyclic(&rt->xa, &res->id, res, xa_limit_32b, &rt->next_id, GFP_KERNEL);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_kadd);
>>>>>> else rdma_restrack_uadd(&id_priv->res);
>>>>>>> rdma_restrack_add(res);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_uadd);
>>>>> id_priv->gid_type = cma_dev->default_gid_type[id_priv->id.port_num - rdma_start_port(cma_dev->device)];
>>>> cma_set_loopback(cma_src_addr(id_priv));
>>>>> switch (addr->sa_family)
>>>>> case AF_INET: ((struct sockaddr_in *) addr)->sin_addr.s_addr = htonl(INADDR_LOOPBACK);
>>>>> case AF_INET6: ipv6_addr_set(&((struct sockaddr_in6 *) addr)->sin6_addr, 0, 0, 0, htonl(1));
>>>>> default: ib_addr_set(&((struct sockaddr_ib *) addr)->sib_addr, 0, 0, 0, htonl(1));
>>> rdma_addr_get_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>> rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, &gid);
>>> cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_RESOLVED, RDMA_CM_EVENT_ADDR_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else {
>>  if (dst_addr->sa_family == AF_IB) cma_resolve_ib_addr(id_priv);
>>>  work = kzalloc(sizeof *work, GFP_KERNEL);
>>>  if (!id_priv->cma_dev) ret = cma_resolve_ib_dev(id_priv);
>>>> addr = cma_dst_addr(id_priv); dgid = &addr->sib_addr; pkey = ntohs(addr->sib_pkey);
>>>> list_for_each_entry(cur_dev, &dev_list, list)
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p)
>>>>   for (i = 0; !rdma_query_gid(cur_dev->device, p, i, &gid); i++) 
>>>>    if (!memcmp(&gid, dgid, sizeof(gid))) cma_dev = cur_dev; sgid = gid; id_priv->id.port_num = p; goto found;
>>>>    if (!cma_dev && (gid.global.subnet_prefix == dgid->global.subnet_prefix) && port_state == IB_PORT_ACTIVE) cma_dev = cur_dev; sgid = gid; id_priv->id.port_num = p; goto found;
>>>> found:
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>> addr = (struct sockaddr_ib *)cma_src_addr(id_priv);
>>>> memcpy(&addr->sib_addr, &sgid, sizeof(sgid));
>>>> cma_translate_ib(addr, &id_priv->id.route.addr.dev_addr);
>>>>> dev_addr->dev_type = ARPHRD_INFINIBAND;
>>>>> rdma_addr_set_sgid(dev_addr, (union ib_gid *) &sib->sib_addr);
>>>>> ib_addr_set_pkey(dev_addr, ntohs(sib->sib_pkey));
>>>  rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, (union ib_gid *)&(((struct sockaddr_ib *) &id_priv->id.route.addr.dst_addr)->sib_addr));
>>>>  memcpy(dev_addr->dst_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>  cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_RESOLVED, RDMA_CM_EVENT_ADDR_RESOLVED
>>>  queue_work(cma_wq, &work->work);
>>  else rdma_resolve_ip(cma_src_addr(id_priv), dst_addr, &id->route.addr.dev_addr, timeout_ms, addr_handler, false, id_priv);
>>>  req = kzalloc(sizeof *req, GFP_KERNEL);
>>>  src_in = &req->src_addr; dst_in = &req->dst_addr;
>>>  if (src_addr) memcpy(src_in, src_addr, rdma_addr_size(src_addr));
>>>  else src_in->sa_family = dst_addr->sa_family;
>>>  memcpy(dst_in, dst_addr, rdma_addr_size(dst_addr));
>>>  req's addr, callback, context, resolve_by_gid_attr as addr, callback, context, resolve_by_gid_attr;
>>>  INIT_DELAYED_WORK(&req->work, process_one_req);
>>>  req->seq = (u32)atomic_inc_return(&ib_nl_addr_request_seq);
>>>  req->status = addr_resolve(src_in, dst_in, addr, true, req->resolve_by_gid_attr, req->seq);
>>>> if (resolve_by_gid_attr) set_addr_netns_by_gid_rcu(addr);
>>>> if (src_in->sa_family == AF_INET) {
>>>>  addr4_resolve(src_in, dst_in, addr, &rt);
>>>>> *src_in = src_sock; *dst_in = dst_sock;
>>>>> src_ip = src_in->sin_addr.s_addr; dst_ip = dst_in->sin_addr.s_addr;
>>>>> memset(&fl4, 0, sizeof(fl4));
>>>>> f14's daddr, saddr, flowi4_oif as dst_ip, src_ip, addr->bound_dev_if;
>>>>> rt = ip_route_output_key(addr->net, &fl4);
>>>>> ret = PTR_ERR_OR_ZERO(rt);
>>>>> src_in->sin_addr.s_addr = fl4.saddr;
>>>>> addr->hoplimit = ip4_dst_hoplimit(&rt->dst);
>>>>  dst = &rt->dst;
>>>> } else addr6_resolve(src_in, dst_in, addr, &dst);
>>>> ret = rdma_set_src_addr_rcu(addr, &ndev_flags, dst_in, dst);
>>>>> *ndev_flags = ndev->flags;
>>>>> if (ndev->flags & IFF_LOOPBACK) ndev = rdma_find_ndev_for_src_ip_rcu(dev_net(ndev), dst_in);
>>>>>> switch (src_in->sa_family) {
>>>>>> case AF_INET: dev = __ip_dev_find(net, ((const struct sockaddr_in *)src_in)->sin_addr.s_addr, false);
>>>>>> case AF_INET6: for_each_netdev_rcu(net, dev) ipv6_chk_addr(net, &((const struct sockaddr_in6 *)src_in)->sin6_addr, dev, 1))
>>>>> return copy_src_l2_addr(dev_addr, dst_in, dst, ndev);
>>>>>> if (dst->dev->flags & IFF_LOOPBACK) rdma_translate_ip(dst_in, dev_addr);
>>>>>>> if (dev_addr->bound_dev_if) {
>>>>>>>  dev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if);
>>>>>>>>  dev = dev_get_by_index_rcu(net, ifindex);
>>>>>>>  rdma_copy_src_l2_addr(dev_addr, dev);
>>>>>>>> dev_addr->dev_type = dev->type;
>>>>>>>> memcpy(dev_addr->src_dev_addr, dev->dev_addr, MAX_ADDR_LEN);
>>>>>>>> memcpy(dev_addr->broadcast, dev->broadcast, MAX_ADDR_LEN);
>>>>>>>> dev_addr->bound_dev_if = dev->ifindex;
>>>>>>>  dev_put(dev);
>>>>>>> }
>>>>>>> dev = rdma_find_ndev_for_src_ip_rcu(dev_addr->net, addr);
>>>>>> else rdma_copy_src_l2_addr(dev_addr, dst->dev);
>>>>>> if (has_gateway(dst, dst_in->sa_family) && ndev->type != ARPHRD_INFINIBAND)
>>>>>>  dev_addr->network = dst_in->sa_family == AF_INET ? RDMA_NETWORK_IPV4:RDMA_NETWORK_IPV6;
>>>>>> else dev_addr->network = RDMA_NETWORK_IB;
>>>> if (!ret && resolve_neigh) addr_resolve_neigh(dst, dst_in, addr, ndev_flags, seq);
>>>>> if (ndev_flags & IFF_LOOPBACK) memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN);
>>>>> else {if (!(ndev_flags & IFF_NOARP)) ret = fetch_ha(dst, addr, dst_in, seq);}
>>>>>> *dst_in4 = dst_in; *dst_in6 = dst_in;
>>>>>> const void *daddr = (dst_in->sa_family == AF_INET) ? (const void *)&dst_in4->sin_addr.s_addr : (const void *)&dst_in6->sin6_addr;
>>>>>> sa_family_t family = dst_in->sa_family;
>>>>>> if (has_gateway(dst, family) && dev_addr->network == RDMA_NETWORK_IB) ib_nl_fetch_ha(dev_addr, daddr, seq, family);
>>>>>> else dst_fetch_ha(dst, dev_addr, daddr);
>>>> if (src_in->sa_family == AF_INET) ip_rt_put(rt);
>>>> else dst_release(dst);
> ucma_put_ctx(ctx);

static ssize_t ucma_connect(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ucma_copy_conn_param(ctx->cm_id, &conn_param, &cmd.conn_param);
>> dst's value as src's value
>> dst->private_data = src->private_data;
>> dst->private_data_len = src->private_data_len;
>> dst->responder_resources =src->responder_resources;
>> dst->initiator_depth = src->initiator_depth;
>> dst->flow_control = src->flow_control;
>> dst->retry_count = src->retry_count;
>> dst->rnr_retry_count = src->rnr_retry_count;
>> dst->srq = src->srq;
>> dst->qp_num = src->qp_num;
>> dst->qkey = (id->route.addr.src_addr.ss_family == AF_IB) ? src->qkey : 0;
> rdma_connect(ctx->cm_id, &conn_param);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (!id->qp) id_priv->qp_num = conn_param->qp_num; id_priv->srq = conn_param->srq;
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (id->qp_type == IB_QPT_UD) cma_resolve_ib_udp(id_priv, conn_param);
>>> memset(&req, 0, sizeof req);
>>> offset = cma_user_data_offset(id_priv);
>>> req.private_data_len = offset + conn_param->private_data_len;
>>> if (req.private_data_len < conn_param->private_data_len)
>>> if (req.private_data_len) private_data = kzalloc(req.private_data_len, GFP_ATOMIC);
>>> else private_data = NULL;
>>> if (conn_param->private_data && conn_param->private_data_len)
>>>  memcpy(private_data + offset, conn_param->private_data, conn_param->private_data_len);
>>> if (private_data) {
>>>  ret = cma_format_hdr(private_data, id_priv);
>>>> cma_hdr->cma_version = CMA_VERSION;
>>>> if (cma_family(id_priv) == AF_INET) {
>>>>  src4 = cma_src_addr(id_priv); dst4 = cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 4);
>>>>  cma_hdr's src_addr.ip4_addr, dst_addr.ip4_addr, port as src4->sin_addr.s_addr, dst4->sin_addr.s_addr, src4->sin_port;
>>>> } else if (cma_family(id_priv) == AF_INET6) {
>>>>  src6 = cma_src_addr(id_priv); dst6 = cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 6);
>>>>  cma_hdr's src_addr.ip6, dst_addr.ip6, port as src6->sin6_addr, dst6->sin6_addr, src6->sin6_port
>>>> }
>>>  req.private_data = private_data;
>>> }
>>> id = ib_create_cm_id(id_priv->id.device, cma_sidr_rep_handler, id_priv);
>>>> cm_id_priv = kzalloc(sizeof *cm_id_priv, GFP_KERNEL);
>>>> cm_id_priv->id's state, device, cm_handler, context, remote_cm_qpn as IB_CM_IDLE, device, cm_handler, context, 1
>>>> xa_alloc_cyclic_irq(&cm.local_id_table, &id, NULL, xa_limit_32b, &cm.local_id_next, GFP_KERNEL);
>>>> cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
>>>> xa_store_irq(&cm.local_id_table, cm_local_id(cm_id_priv->id.local_id), cm_id_priv, GFP_KERNEL);
>>>> return &cm_id_priv->id;
>>> id_priv->cm_id.ib = id;
>>> req's path, sgid_attr, service_id, timeout_ms, max_cm_retries
>>>  as id_priv->id.route.path_rec, id_priv->id.route.addr.dev_addr.sgid_attr, rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv)), 1 << (CMA_CM_RESPONSE_TIMEOUT - 8), CMA_MAX_CM_RETRIES;
>>> ib_send_cm_sidr_req(id_priv->cm_id.ib, &req);
>>>> cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>> ret = cm_init_av_by_path(param->path, param->sgid_attr, &cm_id_priv->av, cm_id_priv);
>>>> cm_id's service_id, service_mask as param->service_id, ~cpu_to_be64(0);
>>>> cm_id_priv's timeout_ms, max_cm_retries as param->timeout_ms, param->max_cm_retries;
>>>> cm_alloc_msg(cm_id_priv, &msg);
>>>> cm_format_sidr_req((struct cm_sidr_req_msg *) msg->mad, cm_id_priv, param);
>>>> msg's timeout_ms, context[1] as cm_id_priv->timeout_ms, IB_CM_SIDR_REQ_SENT;
>>>> if (cm_id->state == IB_CM_IDLE) ib_post_send_mad(msg, NULL);
>>>>> for (; send_buf; send_buf = next_send_buf) {
>>>>> 	mad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private, send_buf);
>>>>> 	mad_agent_priv = mad_send_wr->mad_agent_priv;
>>>>> 	ib_mad_enforce_security(mad_agent_priv, mad_send_wr->send_wr.pkey_index);
>>>>> 	next_send_buf = send_buf->next;
>>>>> 	mad_send_wr->send_wr.ah = send_buf->ah;
>>>>> 	if (((struct ib_mad_hdr *) send_buf->mad)->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {
>>>>> 	 ret = handle_outgoing_dr_smp(mad_agent_priv, mad_send_wr);
>>>>>   mad_send_wr's tid, timeout, max_retries, retries_left as send_buf->mad)->tid, msecs_to_jiffies(send_buf->timeout_ms), send_buf->retries;, send_buf->retries;
>>>>> 	send_buf->retries = 0;
>>>>>   mad_send_wr's refcount, struct as 1 + (mad_send_wr->timeout > 0), IB_WC_SUCCESS;
>>>>> 	list_add_tail(&mad_send_wr->agent_list, &mad_agent_priv->send_list);
>>>>> 	if (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {
>>>>> 	 ib_send_rmpp_mad(mad_send_wr);
>>>>> 	 if (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)
>>>>> 	  ret = ib_send_mad(mad_send_wr);
>>>>> 	} else ib_send_mad(mad_send_wr);
>>>>> }
>>>> cm_id->state = IB_CM_SIDR_REQ_SENT;
>>>> cm_id_priv->msg = msg;
>>  else cma_connect_ib(id_priv, conn_param);
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) cma_connect_iw(id_priv, conn_param);
> ucma_put_ctx(ctx);

static ssize_t ucma_get_event(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> uevent = list_entry(file->event_list.next, struct ucma_event, list);
> if (uevent->resp.event == RDMA_CM_EVENT_CONNECT_REQUEST) {
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> INIT_WORK(&ctx->close_work, ucma_close_id);
>> init_completion(&ctx->comp);
>> INIT_LIST_HEAD(&ctx->mc_list);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> uevent->ctx->backlog++;
> ctx->cm_id = uevent->cm_id;
> ctx->cm_id->context = ctx;
> uevent->resp.id = ctx->id;
> }
> copy_to_user(u64_to_user_ptr(cmd.response), &uevent->resp, min_t(size_t, out_len, sizeof(uevent->resp)))
> list_del(&uevent->list);
> uevent->ctx->events_reported++;
> if (uevent->mc) uevent->mc->events_reported++;
> kfree(uevent);

static ssize_t ucma_resolve_route(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ret = rdma_resolve_route(ctx->cm_id, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (rdma_cap_ib_sa(id->device, id->port_num)) cma_resolve_ib_route(id_priv, timeout_ms);
>>> struct rdma_route *route = &id_priv->id.route;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> route->path_rec = kmalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> cma_query_ib_route(id_priv, timeout_ms, work);
>>>> struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>> memset(&path_rec, 0, sizeof path_rec);
>>>> if (rdma_cap_opa_ah(id_priv->id.device, id_priv->id.port_num)) path_rec.rec_type = SA_PATH_REC_TYPE_OPA;
>>>> else path_rec.rec_type = SA_PATH_REC_TYPE_IB;
>>>> rdma_addr_get_sgid(dev_addr, &path_rec.sgid);
>>>> rdma_addr_get_dgid(dev_addr, &path_rec.dgid);
>>>> path_rec's pkey, numb_path, reversible as cpu_to_be16(ib_addr_get_pkey(dev_addr)), 1, 1
>>>> path_rec.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));
>>>>> if (addr->sa_family == AF_IB) return ((struct sockaddr_ib *) addr)->sib_sid;
>>>>> return cpu_to_be64(((u64)id->ps << 16) + be16_to_cpu(cma_port(addr)));
>>>> comp_mask = IB_SA_PATH_REC_DGID | IB_SA_PATH_REC_SGID |IB_SA_PATH_REC_PKEY | IB_SA_PATH_REC_NUMB_PATH | IB_SA_PATH_REC_REVERSIBLE | IB_SA_PATH_REC_SERVICE_ID;
>>>> switch (cma_family(id_priv))
>>>> case AF_INET:
>>>>  path_rec.qos_class = cpu_to_be16((u16) id_priv->tos);
>>>>  comp_mask |= IB_SA_PATH_REC_QOS_CLASS;
>>>> case AF_INET6:
>>>>  sin6 = cma_src_addr(id_priv);
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sin6->sin6_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> case AF_IB:
>>>>  sib = cma_src_addr(id_priv);
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sib->sib_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> id_priv->query_id = ib_sa_path_rec_get(&sa_client, id_priv->id.device,id_priv->id.port_num, &path_rec, comp_mask, timeout_ms, GFP_KERNEL, cma_query_handler, work, &id_priv->query);
>>>>> struct ib_sa_device *sa_dev = ib_get_client_data(device, &sa_client);
>>>>> port  = &sa_dev->port[port_num - sa_dev->start_port];
>>>>> agent = port->agent;
>>>>> query = kzalloc(sizeof(*query), gfp_mask);
>>>>> query->sa_query.port     = port;
>>>>> if (rec->rec_type == SA_PATH_REC_TYPE_OPA) {
>>>>> 	status = opa_pr_query_possible(client, device, port_num, rec);
>>>>> 	else if (status == PR_OPA_SUPPORTED) query->sa_query.flags |= IB_SA_QUERY_OPA;
>>>>> 	else query->conv_pr = kmalloc(sizeof(*query->conv_pr), gfp_mask);
>>>>> }
>>>>> ret = alloc_mad(&query->sa_query, gfp_mask);
>>>>> ib_sa_client_get(client);
>>>>> query's sa_query.client, callback, context as client, callback, context
>>>>> mad = query->sa_query.mad_buf->mad;
>>>>> init_mad(&query->sa_query, agent);
>>>>> query->sa_query.callback = callback ? ib_sa_path_rec_callback : NULL;
>>>>> query->sa_query.release  = ib_sa_path_rec_release;
>>>>> mad's mad_hdr.method, mad_hdr.attr_id, sa_hdr.comp_mask as IB_MGMT_METHOD_GET, cpu_to_be16(IB_SA_ATTR_PATH_REC), comp_mask
>>>>> if (query->sa_query.flags & IB_SA_QUERY_OPA) ib_pack(opa_path_rec_table, ARRAY_SIZE(opa_path_rec_table), rec, mad->data);
>>>>> else if (query->conv_pr) {
>>>>> 	sa_convert_path_opa_to_ib(query->conv_pr, rec);
>>>>> 	ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), query->conv_pr, mad->data);
>>>>> } else ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), rec, mad->data);
>>>>> *sa_query = &query->sa_query;
>>>>> query->sa_query.flags |= IB_SA_ENABLE_LOCAL_SERVICE;
>>>>> query->sa_query.mad_buf->context[1] = (query->conv_pr) ? query->conv_pr : rec;
>>>>> send_mad(&query->sa_query, timeout_ms, gfp_mask);
>>>> return (id_priv->query_id < 0) ? id_priv->query_id : 0;
>> else if (rdma_protocol_roce(id->device, id->port_num)) cma_resolve_iboe_route(id_priv);
>>> struct rdma_route *route = &id_priv->id.route;
>>> struct rdma_addr *addr = &route->addr;
>>> u8 default_roce_tos = id_priv->cma_dev->default_roce_tos[id_priv->id.port_num - rdma_start_port(id_priv->cma_dev->device)];
>>> u8 tos = id_priv->tos_set ? id_priv->tos : default_roce_tos;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> route->path_rec = kzalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> route->num_paths = 1;
>>> ndev = cma_iboe_set_path_rec_l2_fields(id_priv);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr, &route->path_rec->sgid);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.dst_addr, &route->path_rec->dgid);
>>> if (((struct sockaddr *)&id_priv->id.route.addr.dst_addr)->sa_family != AF_IB)
>>>  route->path_rec->hop_limit = addr->dev_addr.hoplimit;
>>> else route->path_rec->hop_limit = 1;
>>> route->path_rec's reversible, pkey, mtu_selector, sl, traffic_class, mtu, rate_selector, rate as 1, cpu_to_be16(0xffff), IB_SA_EQ, iboe_tos_to_sl(ndev, tos), tos, iboe_get_mtu(ndev->mtu), IB_SA_EQ, iboe_get_rate(ndev)
>>> dev_put(ndev);
>>>> this_cpu_dec(*dev->pcpu_refcnt);
>>> route->path_rec's packet_life_time_selector, packet_life_time as IB_SA_EQ, CMA_IBOE_PACKET_LIFETIME
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else if (rdma_protocol_iwarp(id->device, id->port_num)) cma_resolve_iw_route(id_priv);
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else ret = -ENOSYS;
> ucma_put_ctx(ctx);
> return ret;

static ssize_t ucma_init_qp_attr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> resp.qp_attr_mask = 0;
> memset(&qp_attr, 0, sizeof qp_attr);
> qp_attr.qp_state = cmd.qp_state;
> rdma_init_qp_attr(ctx->cm_id, &qp_attr, &resp.qp_attr_mask);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (!id_priv->cm_id.ib || (id_priv->id.qp_type == IB_QPT_UD)) cma_ib_init_qp_attr(id_priv, qp_attr, qp_attr_mask);
>>>  struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>  if (rdma_cap_eth_ah(id_priv->id.device, id_priv->id.port_num)) pkey = 0xffff;
>>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_ETH_AH;
>>>  else pkey = ib_addr_get_pkey(dev_addr);
>>>>  ((u16)dev_addr->broadcast[8] << 8) | (u16)dev_addr->broadcast[9];
>>>  ret = ib_find_cached_pkey(id_priv->id.device, id_priv->id.port_num, pkey, &qp_attr->pkey_index);
>>>>  if (!rdma_is_port_valid(device, port_num))
>>>>> (port >= rdma_start_port(device) && port <= rdma_end_port(device));
>>>>  cache = device->port_data[port_num].cache.pkey;
>>>>  *index = -1;
>>>>  for (i = 0; i < cache->table_len; ++i)
>>>>  	if ((cache->table[i] & 0x7fff) == (pkey & 0x7fff)) {
>>>>  	 if (cache->table[i] & 0x8000) *index = i; ret = 0;
>>>>  	 else partial_ix = i;
>>>>  	}
>>>>  if (ret && partial_ix >= 0) *index = partial_ix; ret = 0;
>>>  qp_attr->port_num = id_priv->id.port_num;
>>>  *qp_attr_mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>  if (id_priv->id.qp_type == IB_QPT_UD) {
>>>   cma_set_qkey(id_priv, 0);
>>>>   if (qkey) id_priv->qkey = qkey;
>>>>   switch (id_priv->id.ps)
>>>>   case RDMA_PS_UDP, RDMA_PS_IB: id_priv->qkey = RDMA_UDP_QKEY;
>>>>   case RDMA_PS_IPOIB:
>>>>    ib_addr_get_mgid(&id_priv->id.route.addr.dev_addr, &rec.mgid);
>>>>    ib_sa_get_mcmember_rec(id_priv->id.device, id_priv->id.port_num, &rec.mgid, &rec);
>>>   qp_attr->qkey = id_priv->qkey;
>>>   *qp_attr_mask |= IB_QP_QKEY;
>>>  } else {
>>>   qp_attr->qp_access_flags = 0;
>>>   *qp_attr_mask |= IB_QP_ACCESS_FLAGS;
>>>  }
>>  else ret = ib_cm_init_qp_attr(id_priv->cm_id.ib, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>  switch (qp_attr->qp_state)
>>>  case IB_QPS_INIT: ret = cm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state)
>>>>  case IB_CM_{REQ_SENT, MRA_REQ_RCVD, REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED}:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>>   qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE;
>>>>   if (cm_id_priv->responder_resources) qp_attr->qp_access_flags |= IB_ACCESS_REMOTE_READ | IB_ACCESS_REMOTE_ATOMIC;
>>>>   qp_attr->pkey_index = cm_id_priv->av.pkey_index;
>>>>   qp_attr->port_num = cm_id_priv->av.port->port_num;
>>>  case IB_QPS_RTR: ret = cm_init_qp_rtr_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  case IB_CM_{REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED}:
>>>>  	*qp_attr_mask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN | IB_QP_RQ_PSN;
>>>>    qp_attr's ah_attr, path_mtu, dest_qp_num, rq_psn as cm_id_priv->av.ah_attr, cm_id_priv->path_mtu, be32_to_cpu(cm_id_priv->remote_qpn), be32_to_cpu(cm_id_priv->rq_psn)
>>>>  	if (cm_id_priv->qp_type == IB_QPT_RC || cm_id_priv->qp_type == IB_QPT_XRC_TGT) {
>>>>  	 *qp_attr_mask |= IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER;
>>>>     qp_attr's max_dest_rd_atomic, min_rnr_timer as cm_id_priv->responder_resources, 0
>>>>  	}
>>>>  	if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>  	 *qp_attr_mask |= IB_QP_ALT_PATH;
>>>>     qp_attr's alt_port_num, alt_pkey_index, alt_timeout, alt_ah_attr as cm_id_priv->alt_av value
>>>>  	}
>>>  case IB_QPS_RTS: ret = cm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state) {
>>>>  case IB_CM_{REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED:
>>>>   if (cm_id_priv->id.lap_state == IB_CM_LAP_UNINIT) {
>>>>    *qp_attr_mask = IB_QP_STATE | IB_QP_SQ_PSN;
>>>>    qp_attr->sq_psn = be32_to_cpu(cm_id_priv->sq_psn);
>>>>    switch (cm_id_priv->qp_type) {
>>>>    case IB_QPT_{RC, XRC_INI}:
>>>>    	*qp_attr_mask |= IB_QP_RETRY_CNT | IB_QP_RNR_RETRY | IB_QP_MAX_QP_RD_ATOMIC;
>>>>            qp_attr's retry_cnt, rnr_retry, max_rd_atomic as cm_id_priv->retry_count, cm_id_priv->rnr_retry_count, cm_id_priv->initiator_depth
>>>>    case IB_QPT_XRC_TGT:
>>>>    	*qp_attr_mask |= IB_QP_TIMEOUT;
>>>>    	qp_attr->timeout = cm_id_priv->av.timeout;
>>>>    }
>>>>    if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>    	*qp_attr_mask |= IB_QP_PATH_MIG_STATE;
>>>>    	qp_attr->path_mig_state = IB_MIG_REARM;
>>>>    }
>>>>   } else {
>>>>    *qp_attr_mask = IB_QP_ALT_PATH | IB_QP_PATH_MIG_STATE;
>>>>    qp_attr's alt_port_num, alt_pkey_index, alt_timeout, alt_ah_attr, path_mig_state as cm_id_priv->alt_av value
>>>>    qp_attr->path_mig_state = IB_MIG_REARM;
>>>>   }
>>  if (qp_attr->qp_state == IB_QPS_RTR) qp_attr->rq_psn = id_priv->seq_num;
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) {
>>  if (!id_priv->cm_id.iw) {
>>   qp_attr->qp_access_flags = 0;
>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>  }
>>  else ret = iw_cm_init_qp_attr(id_priv->cm_id.iw, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct iwcm_id_private, id);
>>>  switch (qp_attr->qp_state) {
>>>  case IB_QPS_{INIT, RTR}: iwcm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state) {
>>>>  case IW_CM_STATE_{IDLE, CONN_SENT, CONN_RECV, ESTABLISHED}:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>>>  	qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE|IB_ACCESS_REMOTE_READ;
>>>  case IB_QPS_RTS: iwcm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state)
>>>>  case IW_CM_STATE_{IDLE, CONN_SENT, CONN_RECV, ESTABLISHED}: *qp_attr_mask = 0;
>> qp_attr->port_num = id_priv->id.port_num;
>> *qp_attr_mask |= IB_QP_PORT;
>> }
>> if ((*qp_attr_mask & IB_QP_TIMEOUT) && id_priv->timeout_set)
>>   qp_attr->timeout = id_priv->timeout;
>> EXPORT_SYMBOL(rdma_init_qp_attr);
> ib_copy_qp_attr_to_user(ctx->cm_id->device, &resp, &qp_attr);
>> dst's value as src's value
>> ib_copy_ah_attr_to_user(device, &dst->ah_attr, &src->ah_attr);
>>> struct rdma_ah_attr *src = ah_attr;
>>> memset(&dst->grh.reserved, 0, sizeof(dst->grh.reserved));
>>> if ((ah_attr->type == RDMA_AH_ATTR_TYPE_OPA) &&
>>>  (rdma_ah_get_dlid(ah_attr) > be16_to_cpu(IB_LID_PERMISSIVE)) &&
>>>  (!rdma_ah_conv_opa_to_ib(device, &conv_ah, ah_attr)))
>>> src = &conv_ah;
>>> dst->dlid		   = rdma_ah_get_dlid(src);
>>> dst->sl	           = rdma_ah_get_sl(src);
>>> dst->src_path_bits	   = rdma_ah_get_path_bits(src);
>>> dst->static_rate	   = rdma_ah_get_static_rate(src);
>>> dst->is_global         = rdma_ah_get_ah_flags(src) & IB_AH_GRH ? 1 : 0;
>>> if (dst->is_global) {
>>>  const struct ib_global_route *grh = rdma_ah_read_grh(src);
>>>  memcpy(dst->grh.dgid, grh->dgid.raw, sizeof(grh->dgid));
>>>  dst->grh's flow_label, sgid_index, hop_limit, traffic_class as grh->flow_label, grh->sgid_index, grh->hop_limit, grh->traffic_class
>>> }
>>> dst's port_num, reserved as rdma_ah_get_port_num(src), 0;
>> ib_copy_ah_attr_to_user(device, &dst->alt_ah_attr, &src->alt_ah_attr);
>> dst's pkey_index, alt_pkey_index, en_sqd_async_notify, sq_draining, max_rd_atomic, max_dest_rd_atomic, min_rnr_timer, port_num, timeout, retry_cnt, rnr_retry, alt_port_num, alt_timeout as src's value
>> memset(dst->reserved, 0, sizeof(dst->reserved));
>> EXPORT_SYMBOL(ib_copy_qp_attr_to_user);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

rpci_ep_module_init()
-> pci_epf_register_driver(&rpci_ep_driver);
-> rdma_link_register(&rpci_ep_link_ops);
->-> list_add(&ops->list, &link_ops)
-> irq_scheduled_flag = false;

rpci_ep_probe(epf)
-> epf->header = &rpci_ep_pci_header;
-> rdev = ib_alloc_device(rpci_ep_dev, ib_dev);
->-> device = alloc ib_device structure  
->-> device->groups[] = &ib_dev_attr_group
->-> rdma_init_coredev(&device->coredev, device, &init_net);
->->-> coredev->dev.class = &
->->-> coredev->dev.class = &ib_class;
->->-> coredev->dev.groups = dev->groups;
->->-> coredev->owner = dev;
->->-> INIT_LIST_HEAD(&coredev->port_list);
-> epf_set_drvdata(epf, rdev);
->-> epf->dev->driver_data = rdev
-> rdev->epf = epf
-> rdev->status = false
-> init rdev->{cmd_handler, irq_gen_handler)
-> rdev->wq = alloc_workqueue("rpci_ep_wq", WQ_MEM_RECLAIM | WQ_HIGHPRI, 0)
-> rpci_ep_init(rdev);
->-> rpci_ep_init_device_param(rdev);
->->-> init rdev->attr
->-> rpci_ep_init_ports(rdev);
->-> rpci_ep_init_pools(rdev);
-> dma_set_coherent_mask(dev, DMA_BIT_MASK(32))
->-> mask = (dma_addr_t)mask;
->-> dev->coherent_dma_mask = mask;

static struct rdma_link_ops rpci_ep_link_ops = {
	.type = "rpci-ep",
	.newlink = rpci_ep_newlink,
};
rpci_ep_newlink(ibdev_name,ndev)
-> rdev = inited_rdev;
-> rdev->ndev = ndev;
-> rpci_ep_set_mtu(rdev, 0x400);
-> pci_epc_start(rdev->epf->epc);
->-> epc->ops->start(epc);
->->-> pci->ops->start_link(pci)
-> rpci_ep_register_device(rdev, ibdev_name);
-> rdev->ready = true;

static const struct dw_pcie_ops dw_pcie_ep_ops = {
        .read_dbi = NULL,
        .write_dbi = NULL,
};

static const struct pci_epc_ops epc_ops = {
        .write_header           = dw_pcie_ep_write_header,
        .set_bar                = dw_pcie_ep_set_bar,
        .clear_bar              = dw_pcie_ep_clear_bar,
        .map_addr               = dw_pcie_ep_map_addr,
        .unmap_addr             = dw_pcie_ep_unmap_addr,
        .set_msi                = dw_pcie_ep_set_msi,
        .get_msi                = dw_pcie_ep_get_msi,
        .set_msix               = dw_pcie_ep_set_msix,
        .get_msix               = dw_pcie_ep_get_msix,
        .raise_irq              = dw_pcie_ep_raise_irq,
        .start                  = dw_pcie_ep_start,
        .stop                   = dw_pcie_ep_stop,
        .get_features           = dw_pcie_ep_get_features,
};

struct ib_qp *rpci_create_qp(struct ib_pd *ibpd, struct ib_qp_init_attr *init, struct ib_udata *udata)
-> rpci_ep_qp_chk_init(rpci, init);
-> qp = rpci_ep_alloc(&rpci->qp_pool);
	struct rpci_ep_pool_entry *elem;
	ib_device_try_get(&pool->rpci->ib_dev)
	elem = kzalloc(rpci_ep_type_info[pool->type].size, (pool->flags & RPCI_POOL_ATOMIC) ?  GFP_ATOMIC : GFP_KERNEL);
	elem->pool = pool;
-> rpci_ep_add_index(qp);
	elem->index = alloc_index(pool);
	insert_index(pool, elem);
-> rpci_ep_qp_from_init(rpci, qp, pd, init, uresp, ibpd, udata);
	qp->pd			= pd;
	qp->rcq			= rcq;
	qp->scq			= scq;
	qp->srq			= srq;
	rpci_ep_qp_init_misc(rpci, qp, init);
		qp->sq_sig_type		= init->sq_sig_type;
		qp->attr.path_mtu	= 1;
		qp->mtu			= ib_mtu_enum_to_int(qp->attr.path_mtu);
		qpn			= qp->pelem.index;
		port			= &rpci->port;
		qp->ibqp.qp_num		= 0;
		port->qp_smi_index	= qpn;
		qp->attr.port_num	= init->port_num;
	rpci_ep_qp_init_req(rpci, qp, init, udata, uresp);
		sock_create_kern(&init_net, AF_INET, SOCK_DGRAM, 0, &qp->sk);
		qp->sk->sk->sk_user_data = qp;
		qp->rpci = rpci;
		qp->src_port = RPCI_ROCE_V2_SPORT + (hash_32_generic(qp_num(qp), 14) & 0x3fff);
		qp->sq.max_wr		= init->cap.max_send_wr;
		qp->sq.max_sge		= init->cap.max_send_sge;
		qp->sq.max_inline	= init->cap.max_inline_data;
		wqe_size = max_t(int, sizeof(struct rxe_send_wqe) + qp->sq.max_sge * sizeof(struct ib_sge), sizeof(struct rxe_send_wqe) + qp->sq.max_inline);
		qp->sq.queue = rpci_ep_queue_init(rpci, &qp->sq.max_wr, wqe_size);
			q = kmalloc(sizeof(*q), GFP_KERNEL);
			q->rpci = rpci;
			q->elem_size = elem_size;
			elem_size = roundup_pow_of_two(elem_size);
			q->log2_elem_size = order_base_2(elem_size);
			num_slots = *num_elem + 1;
			num_slots = roundup_pow_of_two(num_slots);
			q->index_mask = num_slots - 1;
			buf_size = sizeof(struct rpci_queue_buf) + num_slots * elem_size;
			q->buf = vmalloc_user(buf_size);
			q->buf->log2_elem_size = q->log2_elem_size;
			q->buf->index_mask = q->index_mask;
			q->buf_size = buf_size;
			*num_elem = num_slots - 1;
		ep_do_mmap_info(rpci, uresp ? &uresp->sq_mi : NULL, udata, qp->sq.queue->buf, qp->sq.queue->buf_size, &qp->sq.queue->ip);
			if (outbuf) {
				ip = rpci_ep_create_mmap_info(rpci, buf_size, udata, buf);
					ip = kmalloc(sizeof(*ip), GFP_KERNEL);
					size = PAGE_ALIGN(size);
					ip->info.offset = rpci->mmap_offset;
					rpci->mmap_offset += ALIGN(size, SHMLBA);
					INIT_LIST_HEAD(&ip->pending_mmaps);
					ip->info.size = size;
					ip->context = container_of(udata, struct uverbs_attr_bundle, driver_udata)->context;
					ip->obj = obj;
				copy_to_user(outbuf, &ip->info, sizeof(ip->info));
				list_add(&ip->pending_mmaps, &rpci->pending_mmaps);
	}
	*ip_p = ip;
		qp->req.wqe_index	= producer_index(qp->sq.queue);
		qp->req.state		= QP_STATE_RESET;
		qp->req.opcode		= -1;
		qp->comp.opcode		= -1;
		rpci_ep_init_task(rpci, &qp->req.task, qp, rpci_ep_requester, "req");
			task->obj	= obj;
			task->arg	= arg;
			task->func	= func;
			snprintf(task->name, sizeof(task->name), "%s", name);
			task->destroyed	= false;
			tasklet_init(&task->tasklet, rpci_ep_do_task, (unsigned long)task);
			task->state = TASK_STATE_START;
		rpci_ep_init_task(rpci, &qp->comp.task, qp, rpci_ep_completer, "comp");
		qp->qp_timeout_jiffies = 0; /* Can't be set for UD/UC in modify_qp */
	rpci_ep_qp_init_resp(rpci, qp, init, udata, uresp);
		if (!qp->srq) {
			qp->rq.max_wr		= init->cap.max_recv_wr;
			qp->rq.max_sge		= init->cap.max_recv_sge;
			wqe_size = rcv_wqe_size(qp->rq.max_sge);
			qp->rq.queue = rpci_ep_queue_init(rpci, &qp->rq.max_wr, wqe_size);
			ep_do_mmap_info(rpci, uresp ? &uresp->rq_mi : NULL, udata, qp->rq.queue->buf, qp->rq.queue->buf_size, &qp->rq.queue->ip);
		}
		rpci_ep_init_task(rpci, &qp->resp.task, qp, rpci_ep_responder, "resp");
		qp->resp.opcode		= OPCODE_NONE;
		qp->resp.msn		= 0;
		qp->resp.state		= QP_STATE_RESET;
	qp->attr.qp_state = IB_QPS_RESET;
	qp->valid = 1;


int rpci_ep_requester(void *arg)
> struct rpci_ep_qp *qp = (struct rpci_ep_qp *)arg;
> next_wqe:
> wqe = req_next_wqe(qp);
>> struct rxe_send_wqe *wqe = queue_head(qp->sq.queue);
>> wqe = addr_from_index(qp->sq.queue, qp->req.wqe_index);
    return q->buf->data + ((q->buf->consumer_index & q->index_mask) << q->log2_elem_size);
>> wqe->mask = wr_opcode_mask(wqe->wr.opcode, qp);
>> return wqe
	if (wqe->mask & WR_REG_MASK) {
		if (wqe->wr.opcode == IB_WR_LOCAL_INV) {
			struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
			struct rpci_ep_mem *rmr;
			rmr = rpci_ep_pool_get_index(&rpci->mr_pool, wqe->wr.ex.invalidate_rkey >> 8);
			rmr->state = RPCI_MEM_STATE_FREE;
			wqe->state = wqe_state_done;
			wqe->status = IB_WC_SUCCESS;
		} else if (wqe->wr.opcode == IB_WR_REG_MR) {
			struct rpci_ep_mem *rmr = to_rmr(wqe->wr.wr.reg.mr);
			rmr->state = RPCI_MEM_STATE_VALID;
			rmr->access = wqe->wr.wr.reg.access;
			rmr->lkey = wqe->wr.wr.reg.key;
			rmr->rkey = wqe->wr.wr.reg.key;
			rmr->iova = wqe->wr.wr.reg.mr->iova;
			wqe->state = wqe_state_done;
			wqe->status = IB_WC_SUCCESS;
		}
		if ((wqe->wr.send_flags & IB_SEND_SIGNALED) || qp->sq_sig_type == IB_SIGNAL_ALL_WR)
			rpci_ep_run_task(&qp->comp.task, 1);
	}
	opcode = next_opcode(qp, wqe, wqe->wr.opcode);
	mask = rpci_ep_opcode[opcode].mask;
	mtu = get_mtu(qp);
	payload = (mask & RPCI_WRITE_OR_SEND) ? wqe->dma.resid : 0;
	skb = init_req_packet(qp, wqe, opcode, payload, &pkt);
	> bth_init(pkt, pkt->opcode, solicited, 0, pad, pkey, qp_num, ack_req, pkt->psn);
	>> set bth's opcode, flags, pkey, qpn, apsn as @pkt->opcode, @pad, @pkey, @pkt->psn
	fill_packet(qp, wqe, &pkt, skb, payload)
		struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
		rpci_ep_prepare(pkt, skb, &crc);
		if (pkt->mask & RPCI_WRITE_OR_SEND) {
			if (wqe->wr.send_flags & IB_SEND_INLINE) {
				u8 *tmp = &wqe->dma.inline_data[wqe->dma.sge_offset];
				crc = rpci_crc32(rpci, crc, tmp, paylen);
				memcpy(payload_addr(pkt), tmp, paylen);
				wqe->dma.resid -= paylen;
				wqe->dma.sge_offset += paylen;
			} else {
				err = ep_copy_data(qp->pd, 0, &wqe->dma, payload_addr(pkt), paylen, from_mem_obj, &crc);
					struct rxe_sge		*sge	= &dma->sge[dma->cur_sge];
					int			offset	= dma->sge_offset;
					if (sge->length && (offset < sge->length))
						mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
					while (length > 0) {
						bytes = length;
						if (offset >= sge->length) {
							sge++;
							dma->cur_sge++;
							offset = 0;
							if (sge->length) {
								mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
							} else {
								continue;
							}
						}
						if (bytes > sge->length - offset) bytes = sge->length - offset;
						if (bytes > 0) {
							iova = sge->addr + offset;
							err = rpci_ep_mem_copy(mem, iova, addr, bytes, dir, crcp);
								if (mem->type == RPCI_MEM_TYPE_DMA) {
									u8 *src, *dest;
									src  = (dir == to_mem_obj) ?  addr : ((void *)(uintptr_t)iova);
									dest = (dir == to_mem_obj) ?  ((void *)(uintptr_t)iova) : addr;
									memcpy(dest, src, length);
							offset	+= bytes;
							resid	-= bytes;
							length	-= bytes;
							addr	+= bytes;
						}
					}
					dma->sge_offset = offset;
					dma->resid	= resid;
			}
		}
		p = payload_addr(pkt) + paylen + bth_pad(pkt);
		*p = ~crc;
	save_state(wqe, qp, &rollback_wqe, &rollback_psn);
	update_wqe_state(qp, wqe, &pkt);
	update_wqe_psn(qp, wqe, &pkt, payload);
	rpci_ep_xmit_packet(qp, &pkt, skb);
> update_state(qp, wqe, &pkt, payload);
>> qp->req.opcode = opcode;
>> pkt->mask & RPCI_END_MASK
    qp->req.wqe_index = next_index(qp->sq.queue, qp->req.wqe_index);
>> qp->need_req_skb = 0;
>> if (qp->qp_timeout_jiffies && !timer_pending(&qp->retrans_timer))
    mod_timer(&qp->retrans_timer, jiffies + qp->qp_timeout_jiffies);


/* implements a simple circular buffer that can optionally be
 * shared between user space and the kernel and can be resized

 * the requested element size is rounded up to a power of 2
 * and the number of elements in the buffer is also rounded
 * up to a power of 2. Since the queue is empty when the
 * producer and consumer indices match the maximum capacity
 * of the queue is one less than the number of element slots
 */

/* this data structure is shared between user space and kernel
 * space for those cases where the queue is shared. It contains
 * the producer and consumer indices. Is also contains a copy
 * of the queue size parameters for user space to use but the
 * kernel must use the parameters in the rpci_queue struct
 * this MUST MATCH the corresponding librpci struct
 * for performance reasons arrange to have producer and consumer
 * pointers in separate cache lines
 * the kernel should always mask the indices to avoid accessing
 * memory outside of the data area
 */
struct rpci_queue_buf {
	__u32			log2_elem_size;
	__u32			index_mask;
	__u32			pad_1[30];
	__u32			producer_index;
	__u32			pad_2[31];
	__u32			consumer_index;
	__u32			pad_3[31];
	__u8			data[0];
};

struct rpci_ep_qp {
	struct rpci_ep_sq		sq;  ---------------------> struct rpci_ep_sq {
	struct rpci_ep_rq		rq;                               struct rpci_queue	*queue; --------------------> struct rpci_queue {
	struct rpci_ep_dev		*rpci;                      }                                                               struct rpci_queue_buf *buf;
};                                                                                                                             }


##################################
# /dev/infiniband/uverb????????????????
##################################

enum ib_uverbs_write_cmds {
/*0-4*/IB_USER_VERBS_CMD_GET_CONTEXT, IB_USER_VERBS_CMD_QUERY_DEVICE, IB_USER_VERBS_CMD_QUERY_PORT, IB_USER_VERBS_CMD_ALLOC_PD, IB_USER_VERBS_CMD_DEALLOC_PD, 
/*5-9*/IB_USER_VERBS_CMD_CREATE_AH, IB_USER_VERBS_CMD_MODIFY_AH, IB_USER_VERBS_CMD_QUERY_AH, IB_USER_VERBS_CMD_DESTROY_AH, IB_USER_VERBS_CMD_REG_MR,
/*10-14*/IB_USER_VERBS_CMD_REG_SMR, IB_USER_VERBS_CMD_REREG_MR, IB_USER_VERBS_CMD_QUERY_MR, IB_USER_VERBS_CMD_DEREG_MR, IB_USER_VERBS_CMD_ALLOC_MW,
/*15-19*/IB_USER_VERBS_CMD_BIND_MW, IB_USER_VERBS_CMD_DEALLOC_MW, IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL, IB_USER_VERBS_CMD_CREATE_CQ, IB_USER_VERBS_CMD_RESIZE_CQ,
/*20-24*/IB_USER_VERBS_CMD_DESTROY_CQ, IB_USER_VERBS_CMD_POLL_CQ, IB_USER_VERBS_CMD_PEEK_CQ, IB_USER_VERBS_CMD_REQ_NOTIFY_CQ, IB_USER_VERBS_CMD_CREATE_QP,
/*25-29*/IB_USER_VERBS_CMD_QUERY_QP, IB_USER_VERBS_CMD_MODIFY_QP, IB_USER_VERBS_CMD_DESTROY_QP, IB_USER_VERBS_CMD_POST_SEND, IB_USER_VERBS_CMD_POST_RECV,
/*30-34*/IB_USER_VERBS_CMD_ATTACH_MCAST, IB_USER_VERBS_CMD_DETACH_MCAST, IB_USER_VERBS_CMD_CREATE_SRQ, IB_USER_VERBS_CMD_MODIFY_SRQ, IB_USER_VERBS_CMD_QUERY_SRQ,
/*35-39*/IB_USER_VERBS_CMD_DESTROY_SRQ, IB_USER_VERBS_CMD_POST_SRQ_RECV, IB_USER_VERBS_CMD_OPEN_XRCD, IB_USER_VERBS_CMD_CLOSE_XRCD, IB_USER_VERBS_CMD_CREATE_XSRQ,
/*40*/IB_USER_VERBS_CMD_OPEN_QP,
};

struct uapi_definition {
	u8 kind;
	u8 scope;
	union {
		struct { u16 object_id; } object_start;
		struct {
			u16 command_num;
			u8 is_ex:1;
			u8 has_udata:1;
			u8 has_resp:1;
			u8 req_size;
			u8 resp_size;
		} write;
	};

	union {
		bool (*func_is_supported)(struct ib_device *device);
		int (*func_write)(struct uverbs_attr_bundle *attrs);
		const struct uapi_definition *chain;
		const struct uverbs_object_def *chain_obj_tree;
		size_t needs_fn_offset;
	};
};

const struct uapi_definition uverbs_def_write_intf[] = {
{
	.kind = UAPI_DEF_OBJECT_START,
	.object_start = { .object_id = UVERBS_OBJECT_AH},
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_CREATE_AH},
	.func_write = ib_uverbs_create_ah,
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_DESTROY_AH},
	.func_write = ib_uverbs_destroy_ah,
	.write.req_size = sizeof(struct ib_uvers_destroy_ah)
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_IS_SUPPORTED_DEV_FN,
	.scope = UAPI_SCOPE_METHOD,
	.needs_fn_offset = offsetof(struct ib_device_ops, destroy_ah) + BUILD_BUG_ON_ZERO(sizeof(((struct ib_device_ops *)0)->destroy_ah) != sizeof(void *)),
	}
	,????
}

static const struct file_operations uverbs_mmap_fops = {
	.owner   = THIS_MODULE,
	.write   = ib_uverbs_write,
	.mmap    = ib_uverbs_mmap,
	.open    = ib_uverbs_open,
	.release = ib_uverbs_close,
	.llseek  = no_llseek,
	.unlocked_ioctl = ib_uverbs_ioctl,
	.compat_ioctl = ib_uverbs_ioctl,
};

static struct ib_client uverbs_client = {
	.name   = "uverbs",
	.no_kverbs_req = true,
	.add    = ib_uverbs_add_one,
	.remove = ib_uverbs_remove_one,
	.get_nl_info = ib_uverbs_get_nl_info,
};
MODULE_ALIAS_RDMA_CLIENT("uverbs");

ib_uvers_init() // module_init(ib_uverbs_init);
> register_chrdev_region(IB_UVERBS_BASE_DEV, IB_UVERBS_NUM_FIXED_MINOR, "infiniband_verbs");
> alloc_chrdev_region(&dynamic_uverbs_dev, 0, IB_UVERBS_NUM_DYNAMIC_MINOR, "infiniband_verbs");
> uverbs_class = class_create(THIS_MODULE, "infiniband_verbs");
> uverbs_class->devnode = uverbs_devnode;
> class_create_file(uverbs_class, &class_attr_abi_version.attr);
> ib_register_client(&uverbs_client);

ib_uverbs_add_one()
> uverbs_dev = kzalloc(sizeof(*uverbs_dev), GFP_KERNEL);
> init_srcu_struct(&uverbs_dev->disassociate_srcu);
> device_initialize(&uverbs_dev->dev);
> uverbs_dev->dev.class = uverbs_class; uverbs_dev->dev.parent = device->dev.parent; uverbs_dev->dev.release = ib_uverbs_release_dev; uverbs_dev->dev.groups = uverbs_dev->groups;
> uverbs_dev->groups[0] = &dev_attr_group;
> atomic_set(&uverbs_dev->refcount, 1);
> init_completion(&uverbs_dev->comp);
> uverbs_dev->xrcd_tree = RB_ROOT;
> mutex_init(&uverbs_dev->xrcd_tree_mutex); mutex_init(&uverbs_dev->lists_mutex);
> INIT_LIST_HEAD(&uverbs_dev->uverbs_file_list); INIT_LIST_HEAD(&uverbs_dev->uverbs_events_file_list);
> rcu_assign_pointer(uverbs_dev->ib_dev, device);
> uverbs_dev->num_comp_vectors = device->num_comp_vectors;
> devnum = ida_alloc_max(&uverbs_ida, IB_UVERBS_MAX_DEVICES - 1, GFP_KERNEL);
> uverbs_dev->devnum = devnum;
> if (devnum >= IB_UVERBS_NUM_FIXED_MINOR) base = dynamic_uverbs_dev + devnum - IB_UVERBS_NUM_FIXED_MINOR;
> else base = IB_UVERBS_BASE_DEV + devnum;
> ib_uverbs_create_uapi(device, uverbs_dev)
> uverbs_dev->dev.devt = base;
> dev_set_name(&uverbs_dev->dev, "uverbs%d", uverbs_dev->devnum);
> cdev_init(&uverbs_dev->cdev, device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);
> uverbs_dev->cdev.owner = THIS_MODULE;
> cdev_device_add(&uverbs_dev->cdev, &uverbs_dev->dev);
> ib_set_client_data(device, &uverbs_client, uverbs_dev);

ib_uverbs_open()
> dev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);
> ib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);
> file = kzalloc(sizeof(*file), GFP_KERNEL);
> file->device	 = dev;
> filp->private_data = file;
> list_add_tail(&file->list, &dev->uverbs_file_list);
> return stream_open(inode, filp);

ib_uverbs_write()
> copy_from_user(&hdr, buf, sizeof(hdr))
> method_elm = uapi_get_method(uapi, hdr.command);
> verify_hdr(&hdr, &ex_hdr, count, method_elm);
> srcu_key = srcu_read_lock(&file->device->disassociate_srcu);
> buf += sizeof(hdr);
> memset(bundle.attr_present, 0, sizeof(bundle.attr_present));
> bundle.ufile = file; bundle.context = NULL;
> if (!method_elm->is_ex) {
>> set bundle.driver_udata
>> ib_uverbs_init_udata_buf_or_null( &bundle.ucore, buf, u64_to_user_ptr(response), in_len, out_len);
> else
>> buf += sizeof(ex_hdr);
>> ib_uverbs_init_udata_buf_or_null(&bundle.ucore, buf, u64_to_user_ptr(ex_hdr.response), hdr.in_words * 8, hdr.out_words * 8);
>> ib_uverbs_init_udata_buf_or_null( &bundle.driver_udata, buf + bundle.ucore.inlen, u64_to_user_ptr(ex_hdr.response) + bundle.ucore.outlen, ex_hdr.provider_in_words * 8, ex_hdr.provider_out_words * 8);
> method_elm->handler(&bundle);

ib_uvebrs_ioctl()
> struct ib_uverbs_file *file = filp->private_data;
> struct ib_uverbs_ioctl_hdr __user *user_hdr = (struct ib_uverbs_ioctl_hdr __user *)arg;
> copy_from_user(&hdr, user_hdr, sizeof(hdr));
> ib_uverbs_cmd_verbs(file, &hdr, user_hdr->attrs);

ib_uverbs_mmap()
> struct ib_uverbs_file *file = filp->private_data;
> ucontext = ib_uverbs_get_ucontext_file(file);
> ucontext->device->ops.mmap(ucontext, vma);

int ib_uverbs_reg_mr(struct uverbs_attr_bundle *attrs)
> uverbs_request(attrs, &cmd, sizeof(cmd));
>> copy_from_user(req, attrs->ucore.inbuf, min(attrs->ucore.inlen, req_len))
>> if (attrs->ucore.inlen < req_len) { memset(req + attrs->ucore.inlen, 0, req_len - attrs->ucore.inlen);
> ib_check_mr_access(cmd.access_flags);
> uobj = uobj_alloc(UVERBS_OBJECT_MR, attrs, &ib_dev);
>> __uobj_alloc(uobj_get_type(_attrs, _type), _attrs, _ib_dev)
>>> struct ib_uobject *uobj = rdma_alloc_begin_uobject(obj, attrs->ufile, attrs);
>>>> ret = obj->type_class->alloc_begin(obj, ufile);
>>>> if (attrs) attrs->context = ret->context;
>>>> return ret;
> pd = uobj_get_obj_read(pd, UVERBS_OBJECT_PD, cmd.pd_handle, attrs);
>> return ((struct ib_pd *)uobj->object;
// struct ib_mr *rpci_ep_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length, u64 iova, int access, struct ib_udata *udata)
> mr = pd->device->ops.reg_user_mr(pd, cmd.start, cmd.length, cmd.hca_va, cmd.access_flags, &attrs->driver_udata);
>> struct rpci_ep_dev *rpci = to_rdev(ibpd->device); // get rpci_ep_dev as parent from ibpd->device
>> struct rpci_ep_pd *pd = to_rpd(ibpd); // get rpci_ep_pd as parent from ipbd
>> struct rpci_ep_mem *mr = rpci_ep_alloc(&rpci->mr_pool);
>>> elem = kzalloc(rpci_ep_type_info[pool->type].size, (pool->flags & RPCI_POOL_ATOMIC) ?  GFP_ATOMIC : GFP_KERNEL);
>>> elem->pool = pool;
>>> return elem;
>> rpci_ep_add_index(mr);
>>> struct rpci_ep_pool_entry *elem = arg; struct rpci_pool *pool = elem->pool;
>>> elem->index = alloc_index(pool);
>>> insert_index(pool, elem);
>> rpci_ep_add_ref(pd);
>> rpci_ep_mem_init_user(pd, start, length, iova, access, udata, mr);
>>> struct ib_umem *umem = ib_umem_get(udata, start, length, access, 0); // Pin and DMA map userspace memory.
>>>> // struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr, size_t size, int access, int dmasync)
>>>> umem = kzalloc();
>>>> set umem's ibdev, length, address, writable, owning_mm
>>>>  as context->device, @size, @addr, ib_access_writable(@access), current->mm
>>>> page_list = (struct page **) __get_free_page(GFP_KERNEL); // alloc single page
>>>> npages = ib_umem_num_pages(umem);
>>>> cur_base = @addr & PAGE_MASK;
>>>> sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL); // Allocate and initialize an sg table
>>>> sg = umem->sg_head.sgl;
>>>> while (npages)
>>>> 	ret = get_user_pages(cur_base, min_t(ulong, npages, PAGE_SIZE/szof(struct page *)),, page_list, NULL);
>>>> 	cur_base += ret * PAGE_SIZE; npages -= ret;
>>>> 	sg = ib_umem_add_sg_table(sg, page_list, ret, dma_get_max_seg_size(context->device->dma_device)
>>>> 				  , &umem->sg_nents); //Add N contiguous pages to scatter table
>>>> umem->nmap = ib_dma_map_sg_attrs(context->device, umem->sg_head.sgl, umem->sg_nents, DMA_BIDIRECTIONAL, dma_attrs);
>>>>> return dma_map_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
>>>>>>if (dma_is_direct(ops)) ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
>>>>>>> for_each_sg(sgl, sg, nents, i)
>>>>>>>	 sg->dma_address = dma_direct_map_page(dev, sg_page(sg), sg->offset, sg->length, dir, attrs);
>>>>>>>	 sg_dma_len(sg) = sg->length;
>>> set mem's umem, page_shift, page_mask as umem, PAGE_SHIFT, PAGE_SIZE - 1
>>> rpci_ep_mem_init(access, mem);
>>>> u32 lkey = mem->pelem.index << 8 | rpci_ep_get_key();
>>>> u32 rkey = (access & IB_ACCESS_REMOTE) ? lkey : 0;
>>>> if (mem->pelem.pool->type == RPCI_TYPE_MR) set mem->ibmr's lkey, rkey as lkey, rkey
>>>> set mem's lkey, rkey, state, type, map_shift as lkey, rkey, RPCI_MEM_STATE_INVALID, RPCI_MEM_TYPE_NONE, ilog2(RPCI_BUF_PER_MAP)
>>> num_buf = ib_umem_num_pages(umem);
>>> rpci_ep_mem_alloc(mem, num_buf);
>>>> struct rpci_ep_map **map = mem->map;
>>>> num_map = (num_buf + RPCI_BUF_PER_MAP - 1) / RPCI_BUF_PER_MAP;
>>>> mem->map = kmalloc_array(num_map, sizeof(*map), GFP_KERNEL);
>>>> for (i = 0; i < num_map; i++) mem->map[i] = kmalloc(sizeof(**map), GFP_KERNEL);
>>>> set mem's map_shift, map_mask, num_buf, num_map, max_buf
>>>>  as ilog2(RPCI_BUF_PER_MAP), RPCI_BUF_PER_MAP - 1, num_buf, num_map, num_map * RPCI_BUF_PER_MAP
>>> num_buf = 0; map = mem->map; buf = map[0]->buf;
>>> for_each_sg_page(umem->sg_head.sgl, &sg_iter, umem->nmap, 0)
>>>  if (num_buf >= RPCI_BUF_PER_MAP) map++; buf = map[0]->buf; num_buf = 0;
>>>  vaddr = page_address(sg_page_iter_page(&sg_iter)); //get virt addr
>>>  buf->addr = (uintptr_t)vaddr; buf->size = PAGE_SIZE; num_buf++; buf++;
>>> set mem's pd,umem, access, length, iova, va, offset, state, type
     as pd, umem, access, length, iova, start, ib_umem_offset(umem), RPCI_MEM_STATE_VALID, RPCI_MEM_TYPE_MR
> set mr's device,pd, type, dm, sig_attr, uobject, res.type, iova
   as pd->device, pd, IB_MR_TYPE_USER, NULL, NULL, uobj, RDMA_RESTRACK_MR, cmd.hca_va
> rdma_restrack_uadd(&mr->res);
> uobj->object = mr; > resp.lkey = mr->lkey; resp.rkey = mr->rkey; resp.mr_handle = uobj->id;
> uverbs_response(attrs, &resp, sizeof(resp));
>> if (uverbs_attr_is_valid(attrs, UVERBS_ATTR_CORE_OUT)) return uverbs_copy_to_struct_or_zero( attrs, UVERBS_ATTR_CORE_OUT, resp, resp_len);
>> copy_to_user(attrs->ucore.outbuf, resp, min(attrs->ucore.outlen, resp_len)
> return uobj_alloc_commit(uobj, attrs);
>> struct ib_uverbs_file *ufile = attrs->ufile;
>> uobj->uapi_object->type_class->alloc_commit(uobj);
>> list_add(&uobj->list, &ufile->uobjects);
>> atomic_set(&uobj->usecnt, 0);


##################################
# receiving packet codeflow - rc
##################################

rpci_rc_process_recv_task()
> cur_idx = cr_db->rq_hdb1
> nxt_idx = hdbl == (rdev->q_size -1) ? 0 : hdbl+1
> skb = rpci_generate_skb_from_entry(header)
> header = cr_qp->rq_ba + rdev->q_entry_size * cur_idx
>> copy to lheader from header(cr_qp->rq_ba + rdev->q_entry_size * cur_idx)
>> alloc skb and set field using lheader
>> skb->data = skb->head + lheader.data_offset ??????????
>> copy to skb->head from base(header + RPCI_DATA_OFFSET)
> setting pkt's rpci, port_num, hdr, mask, paylen
> rpci_rcv(skb)
>> if(rpci_match_dgid(rpci, skb) < 0) goto drop ???????
>> pkt->offset = 0
>> pkt->opcode = ((struct rxe_bth *)pkt->hdr+pkt->offset)->opcode
>> pkt->psn = ((struct rxe_bth *)pkt->hdr+pkt->offset)->apsn
>> pkt->qp = NULL
>> pkt->mask |= rpci_opcode[pkt->opcode].mask
>> hdr_check(pkt)
>>> ((struct rxe_bth *)pkt->hdr+pkt->offset)->qpn is 0 ? goto err1
>>> if qpn != IB_MULTICAS_QPN
>>>> qp = rpci_pool_get_index(&rpci->qp_pool, index);
>>>> check_type_state(rpci, pkt, qp)
>>>> check_addr(rpci, pkt, qp)
>>>> check_keys(rpci, pkt, qpn, qp)
>>> if qpn == IB_MULTICAS_QPN
>>>> (pkt->mask & RPCI_GRH_MASK) == 0 ? goto err1
>>> pkt->qp = qp
>> rpci_counter_inc(rpci, RPCI_CNT_RCVD_PKTS);
>> if qpn != IB_MULTICAS_QPN
>>> rpci_rcv_mcast_pkt(rpci, skb)
>> if qpn == IB_MULTICAST_QPN
>>> rpci_rcv_pkt(pkt, skb)
>>>> if pkt->mask & RPCI_REG_MASK, rpci_resp_queue_pkt()
>>>>> skb_queue_tail(&qp->req_pkts, skb)
>>>>> must_sched = (pkt->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST) || (skb_queue_len(&qp->req_pkts) > 1); ??????
>>>>> rpci_run_task(&qp->resp.task, must_sched), rpci_responder()
>>>>>> qp->resp.state == QP_STATE_RESET -> state = RESPST_RESET
>>>>>> qp->resp.state != QP_STATE_RESET -> state = RESPST_GET_REQ
>>>>>> state == RESPST_GET_REQ -> get_req(qp, &pkt)
>>>>>>> pkt = skb_peek(&qp->req_pkts)
>>>>>>> return qp->resp.res ? RESPST_READ_REPLY : RESPST_CHK_PSN
>>>>>> state == RESPST_CHK_PSN -> check_psn(qp, &pkt)
>>>>>>> diff = psn_compare(pkt->psn, qp->Resp.psn)
>>>>>>> qp_type(qp) == IB_QPT_RC && diff == 0, return RESPST_CHK_OP_SEQ
>>>>>> state == RESPST_CHK_OP_SEQ -> check_op_seq(qp, pkt)
>>>>>>> qp_type(qp) == IB_QPT_RC
>>>>>>>> qp->resp.opcode is SEND_{FIRST,MIDDLE}
>>>>>>>>> pkt->opcode is SEND_*, return RESPST_CHK_OP_VALID
>>>>>>>> qp->resp.opcode is RDMA_WRITE_{FIRST,MIDDLE}
>>>>>>>>> pkt->opcode is RDMA_WRITE_*, return RESPST_CHK_OP_VALID
>>>>>>>> qp->resp.opcode is not SEND_{FIRST,MIDDLE} or RDMA_WRITE_{FIRST,MIDDLE}
>>>>>>>>> pkt->opcode is not RC_SEND_{MIDDLE,LAST*}, RC_RDMA_WRITE_{MIDDLE,LAST*}
>>>>>>> qp_type(qp) is not IB_QPT_RC or IB_QPT_UD, return RESPST_CHK_OP_VALID
>>>>>> state == RESPST_CHK_OP_VALID -> check_op_valid(qp, pkt)
>>>>>>> qp_type(qp) == IB_QPT_RC
>>>>>>>> pkt->mask has not unsupported opcode, return RESPST_CHK_RESOURCE;
>>>>>>> qp_type(qp) == IB_QPT_GSI, return RESPST_CHK_RESOURCE;
>>>>>> state == RESPST_CHK_RESOURCE -> check_resource(qp, pkt)
>>>>>>>> qp->resp.state == QP_STATE_ERROR,  return ???
>>>>>>>> pkt->mask has RPCI_READ_OR_ATOMIC, return ???
>>>>>>>> pkt->mask has RPCI_RWR_MASK, && qp->resp.wqe ? RESPST_CHK_LENGTH : RESPST_ERR_RNR
>>>>>>>> return RESPST_CHK_LENGTH
>>>>>> state == RESPST_CHK_LENGTH, check_length(qp, pkt)
>>>>>>> return RESPST_CHK_RKEY
>>>>>> state == RESPST_CHK_RKEY, check_rkey(qp, pkt)
>>>>>>> set qp->resp's va, rkey, resid, length
>>>>>>> set access type(read or write)
>>>>>>> get lookup_mem() using qp->pd, access, rkey, lookup_remote
         return rpci_pool_get_index(rpci->mr_pool, rkey >> 8)
>>>>>>> check mem info and set it to qp->resp.mr
>>>>>>> return RESPST_EXECUTE in below case
         1) pkt->mask has not READ or WRITE or ATOMIC
	 2) pkt->mask has READ+WRITE_OR_SEND+RETH && packet length is zero
>>>>>>> return RESPST_ERR_RKEY_VIOLATION in below case
         1) not found related mem
	 2) mem->state == RPCI_MEM_STATE_FREE
	 3) mem's address range is invalid
>>>>>>> return RESPST_ERR_LENGTH in below case
	 1) pkt->mask has WRITE && resid > mtu && (pktlen != mtu || !pad)
	 2) pkt->mask has WRITE && resid <= mtu && pktlen != resid
	 3) pkt->mask has WRITE && resid <= mtu && pad != (0x3 & (~resid))
>>>>>> state == RESPST_EXECUTE, execute(qp, pkt)
>>>>>>> pkt->mask has RPCI_SEND_MASK
>>>>>>>> qp_type(qp) is QPT_UD, QPT_SMI, QPT_SGI, err = send_data_in(qp, &hdr,), return err
>>>>>>>> send_data_in(qp, payload_addr(pkt),), return err
>>>>>>>>> copy_data(qp->pd, IB_ACCESS_LOCAL_WRITE, &qp->resp.wqe->dma, data_addr, data_len, to_mem_obj, NULL);
>>>>>>>>>> if sg->length && (offset < sge->length) 
            mem = lookup_mem(pd, access, sge->lkey, lookup_local)
>>>>>>>>>> while (length > 0)
            bytes = length
            if offset >= sge->length
             sge++, dma->cur_sge++, offset = 0
	     if sge->length, mem = lookup_mem(pd, access, sge->lkey, lookup_local)
            if bytes > (sge->length - offset)
             bytes = sge->length - offset
            if bytes > 0
             iova = sge->addr + offset
	     rpci_mem_copy(mem, iova, addr, bytes, dir, crcp);
	     offset += bytes, resid -= bytes, length -= bytes, addr += bytes
	   dma->sge_offset = offset
	   dma->resid = resid
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>> pkt->mask has RPCI_WRITE_MASK, err = write_data_in(qp, pkt), return err
>>>>>>> pkt->mask has RPCI_READ_MASK, qp->resp.msn++
>>>>>>> pkt->mask has RPCI_ATOMIC_MASK, err = process_atomic(qp, pkt), return err
>>>>>>> set qp->resp's psn, ack_psn, opcode, status as pkt->psn+1, qp->resp.psn, pkt->opcode, IB_WC_SUCCESS
>>>>>>> pkt->mask has RPCI_COMP_MASK, qp->Resp.msn++, return RESPST_COMPLETE
>>>>>>> pkt->maks has not RPCI_COMP_MAKS, qp_type(qp) is IB_QPT_RC, return RESPST_ACKNOWLEDGE
>>>>>>> pkt->maks has not RPCI_COMP_MAKS, qp_type(qp) is not IB_QPT_RC, return RESPST_CLEANUP
>>>>>> state == RESPST_COMPLETE, do_complete(qp, pkt)
>>>>>>> if qp->req->is_user, ???
>>>>>>> if !qp->req->is_user, set wc's status,qp,wr_id as qp->resp.status, &qp->ibqp, wqe->wr_id
>>>>>>> wc->status is IB_WC_SUCCESS, set wc's opcode, vendor_err, byte_len
>>>>>>>> if qp->rc->is_user, set uwc's wc_flags, ex.imm_data, invalide_rkey, qp_num, src_qp, port_num
>>>>>>>> if !qp->rc->is_user, set wc's wc_flags, network_hdr_type, vlan_id, ex.imm_data, qp, port_num
>>>>>>> qp->resp.wqe = NULL;
>>>>>>> rpci_cq_post(qp->rcq, &cqe, pkt? bth_se(pkt):1)
>>>>>>>> memcpy(producer_addr(cq->queue), cqe, sizeof(*cqe));
>>>>>>>> advance_producer(cq->queue);
>>>>>>>> cq->notify is IB_CQ_{NEXT_COMP,CQ_SLICITED}, cq->notify=0, tasklet_schedule(&cp->comp_task)
>>>>>>>> return 0;
>>>>>>> if !pkt, return RESPST_DONE
>>>>>>> if pkt && qp is IB_QPT_RC, return RESPST_ACKNOWLEDGE
>>>>>>> if pkt && qp is not IB_QPT_RC, return RESPST_CLEANUP
>>>>>> state == RESPST_READ_REPLY, read_reply(qp, pkt)
>>>>>> state == RESPST_ACKNOWLEDGE, acknowledge(qp, pkt)
>>>>>>> qp is IB_QPT_RC
>>>>>>>> qp->resp.aeth_syndrome is not AETH_ACK_UNLIMITED, send_ack(qp,pkt, qp->resp.aeth_syndrome, pkt->psn)
>>>>>>>>> prepare_ack_packet(qp, pkt, &ack_pkt, IB_OPCODE_RC_ACKNOWLEDGE, 0, psn, syndrome, NULL)
>>>>>>>>>> paylen = rxe_opcode[opcode].length + payload + pad + RXE_ICRC_SIZE
           sizeof(struct rxe_bth) + sizeof(struct rxe_aeth) + payload + pad + RXE_ICRC_SIZE
>>>>>>>>>> skb = rxe_init_packet(rxe, &qp->pri_av, paylen, ack)
>>>>>>>>>> set ack_pkt's qp, opcode, mask, offset, paylen as qp, opcode, rxe_opcode[opcode].mask, pkt->offset, paylen
>>>>>>>>>> memcpy(ack->hdr, pkt->hdr, pkt->offset + RXE_BTH_BYTES);
>>>>>>>>>> set ack_pkt->bth's opcode, qpn, pad, se, apsn, ack, psn as opcode, qp->attr.dest_qp_num, pad, 0, psn, 0, psn
>>>>>>>>>> rxe_prepare(ack, skb, &crc)
>>>>>>>>>> if crcp, *crcp = crc
>>>>>>>>>> if not crcp, p = payload_addr(ack) + payload + bth_pad(ack), *p = ~crc
>>>>>>>>>> return skb
>>>>>>>>> rpci_xmit_packet(qp, &ack_pkt, skb);
>>>>>>>>>> rpci_send(pkt, skb);
>>>>>>>>>> qp is not IB_QPT_RC && pkt->mask is RPCI_END_MASK
>>>>>>>>>>> pkt->wqe->state = wqe_state_done, rpci_run_task(&qp->comp.task, 1)
>>>>>>>> qp->resp.aeth_syndrome is AETH_ACK_UNLIMITED && pkt->mask has RPCI_ATOMIC_MASK, send_ack(qp, pkt ,AETH_ACK_UNLIMITED)
>>>>>>>> qp->resp.aeth_syndrome is AETH_ACK_UNLIMITED && bth_ack(pkt), send_ack(qp, pkt ,AETH_ACK_UNLIMITED, pkt->psn)
>>>>>>> reutrn RESPST_CLEANUP
>>>>>> state == RESPST_CLEANUP, cleanup(qp, pkt)
>>>>>>> pkt, skb = skb_dequeue(), rpci_drop_ref(qp),kfree_skb(skb)
>>>>>>> qp->resp.mr, rcpi_drop_ref(qp->resp.mr), qp->resp.mr = NULL
>>>>>>> return RESPST_DONE;
>>>>>> exit:
>>>>>>> ret = -EAGAIN
>>>>>> done:
>>>>>>> rpci_drop_ref(qp)
>>>> if !(pkt->mask & RPCI_REG_MASK), rpci_comp_queue_pkt()
>>>>> skb_queue_tail(&qp->resp_pkts, skb)
>>>>> must_sched = skb_queue_len(&qp->resp_pkts)>1
>>>>> rpci_run_task(&qp->comp.task, must_sched), rpci_completer
>>>>>> ???
>>>>>> state = COMPST_GET_ACK
		skb = skb_dequeue(&qp->resp_pkts)
		pkt = SKB_TO_PKT(skb0
		qp->comp.timeout_retry = 0;
		state = COMPST_GET_WQE
>>>>>> state == COMPST_GET_WQE, get_wqe(qp, pkt, &wqe)
>>>>>>> wqe = queue_head(qp->sq.queue), *wqe_p = wqe;
>>>>>>> wqe->state is wqe_state_done, return COMPST_COMP_WQE
>>>>>>> pkt is true, return COMPST_CHECK_PSN
>>>>>>> pkt is not true, return COMPST_EXIT
>>>>>> state == COMPST_CHECK_PSN, check_psn(qp, pkt, &wqe)
>>>>>>> diff = psn_compare(pkt->psn, wqe->last_psn);
	return COMPST_COMP_WQE
         diff > 0 && wqe->state == wqe_state_pending && wqe->mask has not WR_ATOMIC_OR_READ_MASK
	return COMPST_DONE
         diff > 0 && wqe->state != wqe_state_pending
>>>>>>> diff = psn_compare(pkt->psn, qp->comp.psn);
	return COMPST_COMP_ACK
	 diff < 0 && pkt->psn == wqe->last_psn
	return COMPST_DONE
	 - diff < 0 && pkt->psn != wqe->last_psn
	 - diff > 0 && wqe->mask has WR_ATOMIC_OR_READ_MASK
	return COMPST_CHECK_ACK
	 - diff == 0
	 - diff > 0 && wqe->mask has not WR_ATOMIC_OR_READ_MASK
>>>>>> state == COMPST_CHECK_ACK, check_psn(qp, pkt, &wqe)
>>>>>>> return COMPST_READ
>>>>>>> return COMPST_ATOMIC
>>>>>>> return COMPST_WRITE_SEND
>>>>>>>
>>>>>>>
>>>>>> state == COMPST_CHECK_ACK, check_ack(qp, pkt, &wqe)
>>>>>>> 
>>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
> cr_db->rq_hdbl = nxt_idx
##################################
# receiving packet codeflow - ep
##################################

rpci_ep_probe()
> INIT_DELAYED_WORK(&rdev->cmd_handler, rpci_ep_cmd_handler);

rpci_ep_bind()
> queue_work(rdev->wq, &rdev->cmd_handler.work);

rpci_ep_cmd_handler()
> cr_dbs = rpci_ep_cr_get_db(cr, 0);
> while(ioread32(&cr_dbs->sq_tdbl) != ioread32(&cr_dbs->sq_hdbl))
	rpci_ep_receive_packet(rdev);
>> get db & check packet is arrived
>> get next index, offset, packet header
>> read packet data into skb two times
>> set packet'mask as RPCI_GRH_MASK
>> rpci_ep_rcv(skb)
>>> pkt's offset, opcode, psn, qp, mask as 0, bth_opcode(pkt), bth_psn(pkt), NULL, rpci_ep_opcde[pkt->opcode].mask
>>> rpci_ep_rcv_pkt(pkt, skb);
>>>> pkt->mask has RPCI_REQ_MASK ->  rpci_ep_resp_queue_pkt(pkt->qp, skb);
>>>>> add skb to qp->req_pkts as tail
>>>>> must_sched = 1, if qp->req_pkts is > 1 or pkt->opcode is IB_OPCODE_RC_RDMA_READ_REQUEST
      must_sched = 0, if qp->req_pkts is <= 1 and pkt->opcode is !IB_OPCODE_RC_RDMA_READ_REQUEST
>>>>> rpci_ep_responder() as tasklet context or current context
>>>>>> state = RESPST_RESET, if qp->resp.state == QP_STATE_RESET
>>>>>>> skb = skb_peek(&qp->req_pkts);
>>>>>>> return RESPST_READ_REPLY, if qp->resp.res
>>>>>>> return RESPST_CHK_PSN, if !qp->resp.res
>>>>>>> return RESPST_EXIT, if !skb
>>>>>>> return RESPST_CHK_RESOURCE, if qp->resp.state == QP_STATE_ERROR
>>>>>> read_reply()
>>>>>>> set opcode, qp->resp.res
>>>>>>> prepare_ack_packet()
>>>>>>> rpci_ep_mem_copy(res->read.mr, res->read.va, payload_addr(&ack_pkt)...)
>>>>>>> rpci_ep_xmit_packet(qp, &ack_pkt)
>>>>>>> res
>>>> else -> rpci_ep_comp_queue_pkt(pkt->qp, skb);
>> set cr_dbs->sq_hdb1 as next index
