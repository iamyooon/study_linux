##################################
# TBD
##################################
1. netlink send 함수는 rpci어디에 있는가?

static struct pernet_operations rdma_dev_net_ops = {
	.init = rdma_dev_init_net,
	.exit = rdma_dev_exit_net,
	.id = &rdma_dev_net_id,
	.size = sizeof(struct rdma_dev_net),
};

ib_core_init(void)
-> register_pernet_device(&rdma_dev_net_ops);

struct netlink_kernel_cfg cfg = {
	.input	= rdma_nl_rcv,
};

#define NETLINK_RDMA		20

net_ns_init()
-> setup_net()
->-> list_for_each_entry(ops, &pernet_list, list) ops_init(ops, net);
->->-> ops->init(net);
->->->-> rdma_dev_init_net()
->->->->-> rdma_nl_net_init()
->->->->->-> netlink_kernel_create(net, NETLINK_RDMA, &cfg);


-> rdma_nl_rcv()
->-> rdma_nl_rcv_skb()
->-> rdma_nl_rcv_msg()
->->-> cb_table = get_cb_table(skb, index, op)
->->-> cb_table[op].doit(skb, nlh, extack)
->->->-> nldev_newlink()
->->->->-> ops = link_ops_get(type);
->->->->-> ops->newlink(ibdev_name, ndev) -> rpci_newlink()
->->->->->-> rdev = ib_alloc_device(rpci_dev, ib_dev);
->->->->->->->
->->->->->-> rpci_init(rdev);
->->->->->->-> rpci_init_device_param(rdev)
->->->->->->-> rpci_init_ports(rdev)
->->->->->->-> rpci_init_pools(rdev)
->->->->->-> rpci_pci_init(rdev, inited_pdev);
->->->->->-> rdev->ndev = ndev;
->->->->->-> rpci_set_mtu(rdev, 0x400);
->->->->->-> rpci_register_device(rdev, ibdev_name)
->->->->->->-> set rpci->ib_dev's fields
->->->->->->-> ib_set_device_ops(dev, &rpci_dev_ops)
->->->->->->->-> ib_device->ops->{mmap,...} = rpci_dev_ops->{mmap,...}
->->->->->->-> ib_device_set_netdev(&rpci->ib_dev, rpci->ndev, 1)

static const struct ib_device_ops rpci_dev_ops = {
	.mmap = rpci_mmap,
};

static struct rdma_link_ops rpci_link_ops = {
	.type = "rpci",
	.newlink = rpci_newlink,
};

static struct pci_driver rpci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= rpci_pci_table,
	.probe		= rpci_pci_probe,
	.remove		= rpci_pci_remove,
};


rpci_module_init()
-> pci_register_driver(&rpci_pci_driver)
->-> __pci_register_driver(driver, THIS_MODULE, KBUILD_MODNAME)
->->-> driver_register();
-> rdma_link_register(&rpci_link_ops)
->-> list_add(&rpci_link_ops->list, &link_ops)

rpci_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-> pci_enable_device(pdev)
-> rpci_alloc_bars(pdev);
-> pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
-> pci_set_master(pdev);
-> inited_pdev = pdev;

##################################
# /dev/infiniband/rdma_cm
##################################

static struct miscdevice ucma_misc = {
	.minor          = MISC_DYNAMIC_MINOR,
	.name           = "rdma_cm",
	.nodename       = "infiniband/rdma_cm",
	.mode           = 0666,
	.fops           = &ucma_fops,
};

static const struct file_operations ucma_fops = {
	.owner   = THIS_MODULE,
	.open    = ucma_open,
	.release = ucma_close,
	.write   = ucma_write,
	.poll    = ucma_poll,
	.llseek  = no_llseek,
};

static ssize_t (*ucma_cmd_table[])(struct ucma_file *file,
				   const char __user *inbuf,
				   int in_len, int out_len) = {
	[RDMA_USER_CM_CMD_CREATE_ID] 	 = ucma_create_id,
	[RDMA_USER_CM_CMD_DESTROY_ID]	 = ucma_destroy_id,
	[RDMA_USER_CM_CMD_BIND_IP]	 = ucma_bind_ip,
	[RDMA_USER_CM_CMD_RESOLVE_IP]	 = ucma_resolve_ip,
	[RDMA_USER_CM_CMD_RESOLVE_ROUTE] = ucma_resolve_route,
	[RDMA_USER_CM_CMD_QUERY_ROUTE]	 = ucma_query_route,
	[RDMA_USER_CM_CMD_CONNECT]	 = ucma_connect,
	[RDMA_USER_CM_CMD_LISTEN]	 = ucma_listen,
	[RDMA_USER_CM_CMD_ACCEPT]	 = ucma_accept,
	[RDMA_USER_CM_CMD_REJECT]	 = ucma_reject,
	[RDMA_USER_CM_CMD_DISCONNECT]	 = ucma_disconnect,
	[RDMA_USER_CM_CMD_INIT_QP_ATTR]	 = ucma_init_qp_attr,
	[RDMA_USER_CM_CMD_GET_EVENT]	 = ucma_get_event,
	[RDMA_USER_CM_CMD_GET_OPTION]	 = NULL,
	[RDMA_USER_CM_CMD_SET_OPTION]	 = ucma_set_option,
	[RDMA_USER_CM_CMD_NOTIFY]	 = ucma_notify,
	[RDMA_USER_CM_CMD_JOIN_IP_MCAST] = ucma_join_ip_multicast,
	[RDMA_USER_CM_CMD_LEAVE_MCAST]	 = ucma_leave_multicast,
	[RDMA_USER_CM_CMD_MIGRATE_ID]	 = ucma_migrate_id,
	[RDMA_USER_CM_CMD_QUERY]	 = ucma_query,
	[RDMA_USER_CM_CMD_BIND]		 = ucma_bind,
	[RDMA_USER_CM_CMD_RESOLVE_ADDR]	 = ucma_resolve_addr,
	[RDMA_USER_CM_CMD_JOIN_MCAST]	 = ucma_join_multicast
};

ucma_init()
> misc_register(&ucma_misc);
> device_create_file(ucma_misc.this_device, &dev_attr_abi_version);
> ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
> ib_register_client(&rdma_cma_client);
>> assign_client_id(client);
>> xa_for_each_marked (&devices, index, device, DEVICE_REGISTERED) add_client_context(device, client);

ucam_open()
> file = kmalloc(sizeof *file, GFP_KERNEL); // struct ucma_file *file
> file->close_wq = alloc_ordered_workqueue("ucma_close_id", WQ_MEM_RECLAIM);
> INIT_LIST_HEAD(&file->event_list); INIT_LIST_HEAD(&file->ctx_list); init_waitqueue_head(&file->poll_wait); mutex_init(&file->mut);
> filp->private_data = file; file->filp = filp;
> stream_open(inode, filp);

ucma_poll()
> struct ucma_file *file = filp->private_data;
> poll_wait(filp, &file->poll_wait, wait);
> if (!list_empty(&file->event_list)) mask = EPOLLIN | EPOLLRDNORM;
> return mask

static ssize_t ucma_write(struct file *filp, const char __user *buf, size_t len, loff_t *pos)
> struct ucma_file *file = filp->private_data;
> copy_from_user(&hdr, buf, sizeof(hdr))
> hdr.cmd = array_index_nospec(hdr.cmd, ARRAY_SIZE(ucma_cmd_table));
> ucma_cmd_table[hdr.cmd](file, buf + sizeof(hdr), hdr.in, hdr.out);

// ucma_bind when hdr.cmd is [RDMA_USER_CM_CMD_BIND]
static ssize_t ucma_bind(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
>>> ctx = xa_load(&ctx_table, id);
> rdma_bind_addr(ctx->cm_id, (struct sockaddr *) &cmd.addr);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> cma_check_linklocal(&id->route.addr.dev_addr, addr);
>>> sin6 = (struct sockaddr_in6 *) addr;
>>> dev_addr->bound_dev_if = sin6->sin6_scope_id;
>> memcpy(cma_src_addr(id_priv), addr, rdma_addr_size(addr));
>> daddr = cma_dst_addr(id_priv); daddr->sa_family = addr->sa_family;
>>> (struct sockaddr *) &id_priv->id.route.addr.dst_addr;
>> daddr->sa_family = addr->sa_family;
>> cma_get_port(id_priv);
>>> if (cma_family(id_priv) != AF_IB) ps = cma_select_inet_ps(id_priv);
>>> else ps = cma_select_ib_ps(id_priv);
>>> if (cma_any_port(cma_src_addr(id_priv))) >>>  cma_alloc_any_port(ps, id_priv);
>>> else cma_use_port(ps, id_priv);
>>>>>> case RDMA_PS_IB: return &pernet->ib_ps;
>>>>> return xa_load(xa, snum);

static ssize_t ucma_create_id(struct ucma_file *file, const char __user *inbuf,	int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ucma_get_qp_type(&cmd, &qp_type);
>> switch (cmd->ps) {
>> case RDMA_PS_TCP: *qp_type = IB_QPT_RC;
>> case RDMA_PS_{UDP,IPOIB}: *qp_type = IB_QPT_UD;
>> case RDMA_PS_IB: *qp_type = cmd->qp_type;
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> ctx->uid = cmd.uid;
> cm_id = __rdma_create_id(current->nsproxy->net_ns, ucma_event_handler, ctx, cmd.ps, qp_type, NULL);
>> kzalloc and setting id_priv
> resp.id = ctx->id;
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))
> ctx->cm_id = cm_id;

static ssize_t ucma_query(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> response = u64_to_user_ptr(cmd.response);
> ctx = ucma_get_ctx(file, cmd.id);
> switch (cmd.option) {
> case RDMA_USER_CM_QUERY_ADDR: ret = ucma_query_addr(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_PATH: ret = ucma_query_path(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_GID: ret = ucma_query_gid(ctx, response, out_len);
> ucma_put_ctx(ctx);

static ssize_t ucma_destroy_id(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = _ucma_find_context(cmd.id, file);
> ctx->destroying = 1;
> flush_workqueue(ctx->file->close_wq);
> if (!ctx->closing) {
>  wait_for_completion(&ctx->comp);
tbd
>  rdma_destroy_id(ctx->cm_id);}
> resp.events_reported = ucma_free_ctx(ctx);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

static ssize_t ucma_resolve_addr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
> rdma_resolve_addr(ctx->cm_id, (struct sockaddr *) &cmd.src_addr, (struct sockaddr *) &cmd.dst_addr, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> memcpy(cma_dst_addr(id_priv), dst_addr, rdma_addr_size(dst_addr));
>> if (id_priv->state == RDMA_CM_IDLE) {
>>  cma_bind_addr(id, src_addr, dst_addr);
>>> if (!src_addr || !src_addr->sa_family) {
>>>  src_addr = &id->route.addr.src_addr;
>>>  src_addr->sa_family = dst_addr->sa_family;
>>>  if (IS_ENABLED(CONFIG_IPV6) && dst_addr->sa_family == AF_INET6) {
>>>   set src_addr6, dst_addr6, src_addr6->sin6_scope_id
>>>   if (ipv6_addr_type(&dst_addr6->sin6_addr) & IPV6_ADDR_LINKLOCAL) id->route.addr.dev_addr.bound_dev_if = dst_addr6->sin6_scope_id;
>>>   else if (dst_addr->sa_family == AF_IB) src_addr->sib_pkey = dst_addr->sib_pkey;
>>>  }
>>> }
>>> return rdma_bind_addr(id, src_addr);
>>  if (ret) memset(cma_dst_addr(id_priv), 0, rdma_addr_size(dst_addr));
>> }
>> if (cma_any_addr(dst_addr)) cma_resolve_loopback(id_priv);
>>> // cma_resolve_loopback()
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> if (!id_priv->cma_dev) cma_bind_loopback(id_priv);
>>>> list_for_each_entry(cur_dev, &dev_list, list) {
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p) {
>>>>   if (!ib_get_cached_port_state(cur_dev->device, p, &port_state) && port_state == IB_PORT_ACTIVE) {
>>>> 	cma_dev = cur_dev;
>>>> 	goto port_found;
>>>> port_found:
>>>> rdma_query_gid(cma_dev->device, p, 0, &gid);
>>>> ib_get_cached_pkey(cma_dev->device, p, 0, &pkey);
>>>>> read_lock_irqsave(&device->cache.lock, flags);
>>>>> cache = device->port_data[port_num].cache.pkey;
>>>>> *pkey = cache->table[index];
>>>> id_priv->id.route.addr.dev_addr.dev_type = (rdma_protocol_ib(cma_dev->device, p)) ? ARPHRD_INFINIBAND : ARPHRD_ETHER;
>>>> rdma_addr_set_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>>>>  memcpy(dev_addr->src_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>> ib_addr_set_pkey(&id_priv->id.route.addr.dev_addr, pkey);
>>>>> dev_addr->broadcast[8] = pkey >> 8;dev_addr->broadcast[9] = (unsigned char) pkey;
>>>> id_priv->id.port_num = p;
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>>> _cma_attach_to_dev(id_priv, cma_dev);
>>>>>> cma_ref_dev(cma_dev);
>>>>>> id_priv->cma_dev = cma_dev;
>>>>>> id_priv->id.device = cma_dev->device;
>>>>>> id_priv->id.route.addr.dev_addr.transport = rdma_node_get_transport(cma_dev->device->node_type);
>>>>>> list_add_tail(&id_priv->list, &cma_dev->id_list);
>>>>>> if (id_priv->res.kern_name) rdma_restrack_kadd(&id_priv->res);
>>>>>>> set_kern_name(res);
>>>>>>> rdma_restrack_add(res);
>>>>>>>> struct ib_device *dev = res_to_dev(res);
>>>>>>>> rt = &dev->res[res->type];
>>>>>>>> kref_init(&res->kref);
>>>>>>>> init_completion(&res->comp);
>>>>>>>> if (res->type == RDMA_RESTRACK_QP) {
>>>>>>>>  struct ib_qp *qp = container_of(res, struct ib_qp, res);
>>>>>>>>  ret = xa_insert(&rt->xa, qp->qp_num, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : qp->qp_num;
>>>>>>>> } else if (res->type == RDMA_RESTRACK_COUNTER) {
>>>>>>>>  struct rdma_counter *counter;
>>>>>>>>  counter = container_of(res, struct rdma_counter, res);
>>>>>>>>  ret = xa_insert(&rt->xa, counter->id, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : counter->id;
>>>>>>>> } else ret = xa_alloc_cyclic(&rt->xa, &res->id, res, xa_limit_32b, &rt->next_id, GFP_KERNEL);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_kadd);
>>>>>> else rdma_restrack_uadd(&id_priv->res);
>>>>>>> rdma_restrack_add(res);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_uadd);
>>>>> id_priv->gid_type = cma_dev->default_gid_type[id_priv->id.port_num - rdma_start_port(cma_dev->device)];
>>>> cma_set_loopback(cma_src_addr(id_priv));
>>>>> switch (addr->sa_family)
>>>>> case AF_INET: ((struct sockaddr_in *) addr)->sin_addr.s_addr = htonl(INADDR_LOOPBACK);
>>>>> case AF_INET6: ipv6_addr_set(&((struct sockaddr_in6 *) addr)->sin6_addr, 0, 0, 0, htonl(1));
>>>>> default: ib_addr_set(&((struct sockaddr_ib *) addr)->sib_addr, 0, 0, 0, htonl(1));
>>> rdma_addr_get_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>> rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, &gid);
>>> cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_RESOLVED, RDMA_CM_EVENT_ADDR_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else {
>>  if (dst_addr->sa_family == AF_IB) cma_resolve_ib_addr(id_priv);
>>>  work = kzalloc(sizeof *work, GFP_KERNEL);
>>>  if (!id_priv->cma_dev) ret = cma_resolve_ib_dev(id_priv);
>>>> addr = cma_dst_addr(id_priv); dgid = &addr->sib_addr; pkey = ntohs(addr->sib_pkey);
>>>> list_for_each_entry(cur_dev, &dev_list, list)
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p)
>>>>   for (i = 0; !rdma_query_gid(cur_dev->device, p, i, &gid); i++) 
>>>>    if (!memcmp(&gid, dgid, sizeof(gid))) cma_dev = cur_dev; sgid = gid; id_priv->id.port_num = p; goto found;
>>>>    if (!cma_dev && (gid.global.subnet_prefix == dgid->global.subnet_prefix) && port_state == IB_PORT_ACTIVE) cma_dev = cur_dev; sgid = gid; id_priv->id.port_num = p; goto found;
>>>> found:
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>> addr = (struct sockaddr_ib *)cma_src_addr(id_priv);
>>>> memcpy(&addr->sib_addr, &sgid, sizeof(sgid));
>>>> cma_translate_ib(addr, &id_priv->id.route.addr.dev_addr);
>>>>> dev_addr->dev_type = ARPHRD_INFINIBAND;
>>>>> rdma_addr_set_sgid(dev_addr, (union ib_gid *) &sib->sib_addr);
>>>>> ib_addr_set_pkey(dev_addr, ntohs(sib->sib_pkey));
>>>  rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, (union ib_gid *)&(((struct sockaddr_ib *) &id_priv->id.route.addr.dst_addr)->sib_addr));
>>>>  memcpy(dev_addr->dst_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>  cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_RESOLVED, RDMA_CM_EVENT_ADDR_RESOLVED
>>>  queue_work(cma_wq, &work->work);
>>  else rdma_resolve_ip(cma_src_addr(id_priv), dst_addr, &id->route.addr.dev_addr, timeout_ms, addr_handler, false, id_priv);
>>>  req = kzalloc(sizeof *req, GFP_KERNEL);
>>>  src_in = &req->src_addr; dst_in = &req->dst_addr;
>>>  if (src_addr) memcpy(src_in, src_addr, rdma_addr_size(src_addr));
>>>  else src_in->sa_family = dst_addr->sa_family;
>>>  memcpy(dst_in, dst_addr, rdma_addr_size(dst_addr));
>>>  req's addr, callback, context, resolve_by_gid_attr as addr, callback, context, resolve_by_gid_attr;
>>>  INIT_DELAYED_WORK(&req->work, process_one_req);
>>>  req->seq = (u32)atomic_inc_return(&ib_nl_addr_request_seq);
>>>  req->status = addr_resolve(src_in, dst_in, addr, true, req->resolve_by_gid_attr, req->seq);
>>>> if (resolve_by_gid_attr) set_addr_netns_by_gid_rcu(addr);
>>>> if (src_in->sa_family == AF_INET) {
>>>>  addr4_resolve(src_in, dst_in, addr, &rt);
>>>>> *src_in = src_sock; *dst_in = dst_sock;
>>>>> src_ip = src_in->sin_addr.s_addr; dst_ip = dst_in->sin_addr.s_addr;
>>>>> memset(&fl4, 0, sizeof(fl4));
>>>>> f14's daddr, saddr, flowi4_oif as dst_ip, src_ip, addr->bound_dev_if;
>>>>> rt = ip_route_output_key(addr->net, &fl4);
>>>>> ret = PTR_ERR_OR_ZERO(rt);
>>>>> src_in->sin_addr.s_addr = fl4.saddr;
>>>>> addr->hoplimit = ip4_dst_hoplimit(&rt->dst);
>>>>  dst = &rt->dst;
>>>> } else addr6_resolve(src_in, dst_in, addr, &dst);
>>>> ret = rdma_set_src_addr_rcu(addr, &ndev_flags, dst_in, dst);
>>>>> *ndev_flags = ndev->flags;
>>>>> if (ndev->flags & IFF_LOOPBACK) ndev = rdma_find_ndev_for_src_ip_rcu(dev_net(ndev), dst_in);
>>>>>> switch (src_in->sa_family) {
>>>>>> case AF_INET: dev = __ip_dev_find(net, ((const struct sockaddr_in *)src_in)->sin_addr.s_addr, false);
>>>>>> case AF_INET6: for_each_netdev_rcu(net, dev) ipv6_chk_addr(net, &((const struct sockaddr_in6 *)src_in)->sin6_addr, dev, 1))
>>>>> return copy_src_l2_addr(dev_addr, dst_in, dst, ndev);
>>>>>> if (dst->dev->flags & IFF_LOOPBACK) rdma_translate_ip(dst_in, dev_addr);
>>>>>>> if (dev_addr->bound_dev_if) {
>>>>>>>  dev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if);
>>>>>>>>  dev = dev_get_by_index_rcu(net, ifindex);
>>>>>>>  rdma_copy_src_l2_addr(dev_addr, dev);
>>>>>>>> dev_addr->dev_type = dev->type;
>>>>>>>> memcpy(dev_addr->src_dev_addr, dev->dev_addr, MAX_ADDR_LEN);
>>>>>>>> memcpy(dev_addr->broadcast, dev->broadcast, MAX_ADDR_LEN);
>>>>>>>> dev_addr->bound_dev_if = dev->ifindex;
>>>>>>>  dev_put(dev);
>>>>>>> }
>>>>>>> dev = rdma_find_ndev_for_src_ip_rcu(dev_addr->net, addr);
>>>>>> else rdma_copy_src_l2_addr(dev_addr, dst->dev);
>>>>>> if (has_gateway(dst, dst_in->sa_family) && ndev->type != ARPHRD_INFINIBAND)
>>>>>>  dev_addr->network = dst_in->sa_family == AF_INET ? RDMA_NETWORK_IPV4:RDMA_NETWORK_IPV6;
>>>>>> else dev_addr->network = RDMA_NETWORK_IB;
>>>> if (!ret && resolve_neigh) addr_resolve_neigh(dst, dst_in, addr, ndev_flags, seq);
>>>>> if (ndev_flags & IFF_LOOPBACK) memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN);
>>>>> else {if (!(ndev_flags & IFF_NOARP)) ret = fetch_ha(dst, addr, dst_in, seq);}
>>>>>> *dst_in4 = dst_in; *dst_in6 = dst_in;
>>>>>> const void *daddr = (dst_in->sa_family == AF_INET) ? (const void *)&dst_in4->sin_addr.s_addr : (const void *)&dst_in6->sin6_addr;
>>>>>> sa_family_t family = dst_in->sa_family;
>>>>>> if (has_gateway(dst, family) && dev_addr->network == RDMA_NETWORK_IB) ib_nl_fetch_ha(dev_addr, daddr, seq, family);
>>>>>> else dst_fetch_ha(dst, dev_addr, daddr);
>>>> if (src_in->sa_family == AF_INET) ip_rt_put(rt);
>>>> else dst_release(dst);
> ucma_put_ctx(ctx);

static ssize_t ucma_connect(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ucma_copy_conn_param(ctx->cm_id, &conn_param, &cmd.conn_param);
>> dst's value as src's value
>> dst->private_data = src->private_data;
>> dst->private_data_len = src->private_data_len;
>> dst->responder_resources =src->responder_resources;
>> dst->initiator_depth = src->initiator_depth;
>> dst->flow_control = src->flow_control;
>> dst->retry_count = src->retry_count;
>> dst->rnr_retry_count = src->rnr_retry_count;
>> dst->srq = src->srq;
>> dst->qp_num = src->qp_num;
>> dst->qkey = (id->route.addr.src_addr.ss_family == AF_IB) ? src->qkey : 0;
> rdma_connect(ctx->cm_id, &conn_param);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (!id->qp) id_priv->qp_num = conn_param->qp_num; id_priv->srq = conn_param->srq;
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (id->qp_type == IB_QPT_UD) cma_resolve_ib_udp(id_priv, conn_param);
>>> memset(&req, 0, sizeof req);
>>> offset = cma_user_data_offset(id_priv);
>>> req.private_data_len = offset + conn_param->private_data_len;
>>> if (req.private_data_len < conn_param->private_data_len)
>>> if (req.private_data_len) private_data = kzalloc(req.private_data_len, GFP_ATOMIC);
>>> else private_data = NULL;
>>> if (conn_param->private_data && conn_param->private_data_len)
>>>  memcpy(private_data + offset, conn_param->private_data, conn_param->private_data_len);
>>> if (private_data) {
>>>  ret = cma_format_hdr(private_data, id_priv);
>>>> cma_hdr->cma_version = CMA_VERSION;
>>>> if (cma_family(id_priv) == AF_INET) {
>>>>  src4 = cma_src_addr(id_priv); dst4 = cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 4);
>>>>  cma_hdr's src_addr.ip4_addr, dst_addr.ip4_addr, port as src4->sin_addr.s_addr, dst4->sin_addr.s_addr, src4->sin_port;
>>>> } else if (cma_family(id_priv) == AF_INET6) {
>>>>  src6 = cma_src_addr(id_priv); dst6 = cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 6);
>>>>  cma_hdr's src_addr.ip6, dst_addr.ip6, port as src6->sin6_addr, dst6->sin6_addr, src6->sin6_port
>>>> }
>>>  req.private_data = private_data;
>>> }
>>> id = ib_create_cm_id(id_priv->id.device, cma_sidr_rep_handler, id_priv);
>>>> cm_id_priv = kzalloc(sizeof *cm_id_priv, GFP_KERNEL);
>>>> cm_id_priv->id's state, device, cm_handler, context, remote_cm_qpn as IB_CM_IDLE, device, cm_handler, context, 1
>>>> xa_alloc_cyclic_irq(&cm.local_id_table, &id, NULL, xa_limit_32b, &cm.local_id_next, GFP_KERNEL);
>>>> cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
>>>> xa_store_irq(&cm.local_id_table, cm_local_id(cm_id_priv->id.local_id), cm_id_priv, GFP_KERNEL);
>>>> return &cm_id_priv->id;
>>> id_priv->cm_id.ib = id;
>>> req's path, sgid_attr, service_id, timeout_ms, max_cm_retries
>>>  as id_priv->id.route.path_rec, id_priv->id.route.addr.dev_addr.sgid_attr, rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv)), 1 << (CMA_CM_RESPONSE_TIMEOUT - 8), CMA_MAX_CM_RETRIES;
>>> ib_send_cm_sidr_req(id_priv->cm_id.ib, &req);
>>>> cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>> ret = cm_init_av_by_path(param->path, param->sgid_attr, &cm_id_priv->av, cm_id_priv);
>>>> cm_id's service_id, service_mask as param->service_id, ~cpu_to_be64(0);
>>>> cm_id_priv's timeout_ms, max_cm_retries as param->timeout_ms, param->max_cm_retries;
>>>> cm_alloc_msg(cm_id_priv, &msg);
>>>> cm_format_sidr_req((struct cm_sidr_req_msg *) msg->mad, cm_id_priv, param);
>>>> msg's timeout_ms, context[1] as cm_id_priv->timeout_ms, IB_CM_SIDR_REQ_SENT;
>>>> if (cm_id->state == IB_CM_IDLE) ib_post_send_mad(msg, NULL);
>>>>> for (; send_buf; send_buf = next_send_buf) {
>>>>> 	mad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private, send_buf);
>>>>> 	mad_agent_priv = mad_send_wr->mad_agent_priv;
>>>>> 	ib_mad_enforce_security(mad_agent_priv, mad_send_wr->send_wr.pkey_index);
>>>>> 	next_send_buf = send_buf->next;
>>>>> 	mad_send_wr->send_wr.ah = send_buf->ah;
>>>>> 	if (((struct ib_mad_hdr *) send_buf->mad)->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {
>>>>> 	 ret = handle_outgoing_dr_smp(mad_agent_priv, mad_send_wr);
>>>>>   mad_send_wr's tid, timeout, max_retries, retries_left as send_buf->mad)->tid, msecs_to_jiffies(send_buf->timeout_ms), send_buf->retries;, send_buf->retries;
>>>>> 	send_buf->retries = 0;
>>>>>   mad_send_wr's refcount, struct as 1 + (mad_send_wr->timeout > 0), IB_WC_SUCCESS;
>>>>> 	list_add_tail(&mad_send_wr->agent_list, &mad_agent_priv->send_list);
>>>>> 	if (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {
>>>>> 	 ib_send_rmpp_mad(mad_send_wr);
>>>>> 	 if (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)
>>>>> 	  ret = ib_send_mad(mad_send_wr);
>>>>> 	} else ib_send_mad(mad_send_wr);
>>>>> }
>>>> cm_id->state = IB_CM_SIDR_REQ_SENT;
>>>> cm_id_priv->msg = msg;
>>  else cma_connect_ib(id_priv, conn_param);
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) cma_connect_iw(id_priv, conn_param);
> ucma_put_ctx(ctx);

static ssize_t ucma_get_event(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> uevent = list_entry(file->event_list.next, struct ucma_event, list);
> if (uevent->resp.event == RDMA_CM_EVENT_CONNECT_REQUEST) {
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> INIT_WORK(&ctx->close_work, ucma_close_id);
>> init_completion(&ctx->comp);
>> INIT_LIST_HEAD(&ctx->mc_list);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> uevent->ctx->backlog++;
> ctx->cm_id = uevent->cm_id;
> ctx->cm_id->context = ctx;
> uevent->resp.id = ctx->id;
> }
> copy_to_user(u64_to_user_ptr(cmd.response), &uevent->resp, min_t(size_t, out_len, sizeof(uevent->resp)))
> list_del(&uevent->list);
> uevent->ctx->events_reported++;
> if (uevent->mc) uevent->mc->events_reported++;
> kfree(uevent);

static ssize_t ucma_resolve_route(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ret = rdma_resolve_route(ctx->cm_id, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (rdma_cap_ib_sa(id->device, id->port_num)) cma_resolve_ib_route(id_priv, timeout_ms);
>>> struct rdma_route *route = &id_priv->id.route;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> route->path_rec = kmalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> cma_query_ib_route(id_priv, timeout_ms, work);
>>>> struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>> memset(&path_rec, 0, sizeof path_rec);
>>>> if (rdma_cap_opa_ah(id_priv->id.device, id_priv->id.port_num)) path_rec.rec_type = SA_PATH_REC_TYPE_OPA;
>>>> else path_rec.rec_type = SA_PATH_REC_TYPE_IB;
>>>> rdma_addr_get_sgid(dev_addr, &path_rec.sgid);
>>>> rdma_addr_get_dgid(dev_addr, &path_rec.dgid);
>>>> path_rec's pkey, numb_path, reversible as cpu_to_be16(ib_addr_get_pkey(dev_addr)), 1, 1
>>>> path_rec.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));
>>>>> if (addr->sa_family == AF_IB) return ((struct sockaddr_ib *) addr)->sib_sid;
>>>>> return cpu_to_be64(((u64)id->ps << 16) + be16_to_cpu(cma_port(addr)));
>>>> comp_mask = IB_SA_PATH_REC_DGID | IB_SA_PATH_REC_SGID |IB_SA_PATH_REC_PKEY | IB_SA_PATH_REC_NUMB_PATH | IB_SA_PATH_REC_REVERSIBLE | IB_SA_PATH_REC_SERVICE_ID;
>>>> switch (cma_family(id_priv))
>>>> case AF_INET:
>>>>  path_rec.qos_class = cpu_to_be16((u16) id_priv->tos);
>>>>  comp_mask |= IB_SA_PATH_REC_QOS_CLASS;
>>>> case AF_INET6:
>>>>  sin6 = cma_src_addr(id_priv);
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sin6->sin6_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> case AF_IB:
>>>>  sib = cma_src_addr(id_priv);
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sib->sib_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> id_priv->query_id = ib_sa_path_rec_get(&sa_client, id_priv->id.device,id_priv->id.port_num, &path_rec, comp_mask, timeout_ms, GFP_KERNEL, cma_query_handler, work, &id_priv->query);
>>>>> struct ib_sa_device *sa_dev = ib_get_client_data(device, &sa_client);
>>>>> port  = &sa_dev->port[port_num - sa_dev->start_port];
>>>>> agent = port->agent;
>>>>> query = kzalloc(sizeof(*query), gfp_mask);
>>>>> query->sa_query.port     = port;
>>>>> if (rec->rec_type == SA_PATH_REC_TYPE_OPA) {
>>>>> 	status = opa_pr_query_possible(client, device, port_num, rec);
>>>>> 	else if (status == PR_OPA_SUPPORTED) query->sa_query.flags |= IB_SA_QUERY_OPA;
>>>>> 	else query->conv_pr = kmalloc(sizeof(*query->conv_pr), gfp_mask);
>>>>> }
>>>>> ret = alloc_mad(&query->sa_query, gfp_mask);
>>>>> ib_sa_client_get(client);
>>>>> query's sa_query.client, callback, context as client, callback, context
>>>>> mad = query->sa_query.mad_buf->mad;
>>>>> init_mad(&query->sa_query, agent);
>>>>> query->sa_query.callback = callback ? ib_sa_path_rec_callback : NULL;
>>>>> query->sa_query.release  = ib_sa_path_rec_release;
>>>>> mad's mad_hdr.method, mad_hdr.attr_id, sa_hdr.comp_mask as IB_MGMT_METHOD_GET, cpu_to_be16(IB_SA_ATTR_PATH_REC), comp_mask
>>>>> if (query->sa_query.flags & IB_SA_QUERY_OPA) ib_pack(opa_path_rec_table, ARRAY_SIZE(opa_path_rec_table), rec, mad->data);
>>>>> else if (query->conv_pr) {
>>>>> 	sa_convert_path_opa_to_ib(query->conv_pr, rec);
>>>>> 	ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), query->conv_pr, mad->data);
>>>>> } else ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), rec, mad->data);
>>>>> *sa_query = &query->sa_query;
>>>>> query->sa_query.flags |= IB_SA_ENABLE_LOCAL_SERVICE;
>>>>> query->sa_query.mad_buf->context[1] = (query->conv_pr) ? query->conv_pr : rec;
>>>>> send_mad(&query->sa_query, timeout_ms, gfp_mask);
>>>> return (id_priv->query_id < 0) ? id_priv->query_id : 0;
>> else if (rdma_protocol_roce(id->device, id->port_num)) cma_resolve_iboe_route(id_priv);
>>> struct rdma_route *route = &id_priv->id.route;
>>> struct rdma_addr *addr = &route->addr;
>>> u8 default_roce_tos = id_priv->cma_dev->default_roce_tos[id_priv->id.port_num - rdma_start_port(id_priv->cma_dev->device)];
>>> u8 tos = id_priv->tos_set ? id_priv->tos : default_roce_tos;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> route->path_rec = kzalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> route->num_paths = 1;
>>> ndev = cma_iboe_set_path_rec_l2_fields(id_priv);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr, &route->path_rec->sgid);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.dst_addr, &route->path_rec->dgid);
>>> if (((struct sockaddr *)&id_priv->id.route.addr.dst_addr)->sa_family != AF_IB)
>>>  route->path_rec->hop_limit = addr->dev_addr.hoplimit;
>>> else route->path_rec->hop_limit = 1;
>>> route->path_rec's reversible, pkey, mtu_selector, sl, traffic_class, mtu, rate_selector, rate as 1, cpu_to_be16(0xffff), IB_SA_EQ, iboe_tos_to_sl(ndev, tos), tos, iboe_get_mtu(ndev->mtu), IB_SA_EQ, iboe_get_rate(ndev)
>>> dev_put(ndev);
>>>> this_cpu_dec(*dev->pcpu_refcnt);
>>> route->path_rec's packet_life_time_selector, packet_life_time as IB_SA_EQ, CMA_IBOE_PACKET_LIFETIME
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else if (rdma_protocol_iwarp(id->device, id->port_num)) cma_resolve_iw_route(id_priv);
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work's old_state, new_state, event.event as RDMA_CM_ROUTE_QUERY, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_EVENT_ROUTE_RESOLVED
>>> queue_work(cma_wq, &work->work);
>> else ret = -ENOSYS;
> ucma_put_ctx(ctx);
> return ret;

static ssize_t ucma_init_qp_attr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> resp.qp_attr_mask = 0;
> memset(&qp_attr, 0, sizeof qp_attr);
> qp_attr.qp_state = cmd.qp_state;
> rdma_init_qp_attr(ctx->cm_id, &qp_attr, &resp.qp_attr_mask);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (!id_priv->cm_id.ib || (id_priv->id.qp_type == IB_QPT_UD)) cma_ib_init_qp_attr(id_priv, qp_attr, qp_attr_mask);
>>>  struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>  if (rdma_cap_eth_ah(id_priv->id.device, id_priv->id.port_num)) pkey = 0xffff;
>>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_ETH_AH;
>>>  else pkey = ib_addr_get_pkey(dev_addr);
>>>>  ((u16)dev_addr->broadcast[8] << 8) | (u16)dev_addr->broadcast[9];
>>>  ret = ib_find_cached_pkey(id_priv->id.device, id_priv->id.port_num, pkey, &qp_attr->pkey_index);
>>>>  if (!rdma_is_port_valid(device, port_num))
>>>>> (port >= rdma_start_port(device) && port <= rdma_end_port(device));
>>>>  cache = device->port_data[port_num].cache.pkey;
>>>>  *index = -1;
>>>>  for (i = 0; i < cache->table_len; ++i)
>>>>  	if ((cache->table[i] & 0x7fff) == (pkey & 0x7fff)) {
>>>>  	 if (cache->table[i] & 0x8000) *index = i; ret = 0;
>>>>  	 else partial_ix = i;
>>>>  	}
>>>>  if (ret && partial_ix >= 0) *index = partial_ix; ret = 0;
>>>  qp_attr->port_num = id_priv->id.port_num;
>>>  *qp_attr_mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>  if (id_priv->id.qp_type == IB_QPT_UD) {
>>>   cma_set_qkey(id_priv, 0);
>>>>   if (qkey) id_priv->qkey = qkey;
>>>>   switch (id_priv->id.ps)
>>>>   case RDMA_PS_UDP, RDMA_PS_IB: id_priv->qkey = RDMA_UDP_QKEY;
>>>>   case RDMA_PS_IPOIB:
>>>>    ib_addr_get_mgid(&id_priv->id.route.addr.dev_addr, &rec.mgid);
>>>>    ib_sa_get_mcmember_rec(id_priv->id.device, id_priv->id.port_num, &rec.mgid, &rec);
>>>   qp_attr->qkey = id_priv->qkey;
>>>   *qp_attr_mask |= IB_QP_QKEY;
>>>  } else {
>>>   qp_attr->qp_access_flags = 0;
>>>   *qp_attr_mask |= IB_QP_ACCESS_FLAGS;
>>>  }
>>  else ret = ib_cm_init_qp_attr(id_priv->cm_id.ib, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>  switch (qp_attr->qp_state)
>>>  case IB_QPS_INIT: ret = cm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state)
>>>>  case IB_CM_{REQ_SENT, MRA_REQ_RCVD, REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED}:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>>   qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE;
>>>>   if (cm_id_priv->responder_resources) qp_attr->qp_access_flags |= IB_ACCESS_REMOTE_READ | IB_ACCESS_REMOTE_ATOMIC;
>>>>   qp_attr->pkey_index = cm_id_priv->av.pkey_index;
>>>>   qp_attr->port_num = cm_id_priv->av.port->port_num;
>>>  case IB_QPS_RTR: ret = cm_init_qp_rtr_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  case IB_CM_{REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED}:
>>>>  	*qp_attr_mask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN | IB_QP_RQ_PSN;
>>>>    qp_attr's ah_attr, path_mtu, dest_qp_num, rq_psn as cm_id_priv->av.ah_attr, cm_id_priv->path_mtu, be32_to_cpu(cm_id_priv->remote_qpn), be32_to_cpu(cm_id_priv->rq_psn)
>>>>  	if (cm_id_priv->qp_type == IB_QPT_RC || cm_id_priv->qp_type == IB_QPT_XRC_TGT) {
>>>>  	 *qp_attr_mask |= IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER;
>>>>     qp_attr's max_dest_rd_atomic, min_rnr_timer as cm_id_priv->responder_resources, 0
>>>>  	}
>>>>  	if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>  	 *qp_attr_mask |= IB_QP_ALT_PATH;
>>>>     qp_attr's alt_port_num, alt_pkey_index, alt_timeout, alt_ah_attr as cm_id_priv->alt_av value
>>>>  	}
>>>  case IB_QPS_RTS: ret = cm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state) {
>>>>  case IB_CM_{REQ_RCVD, MRA_REQ_SENT, REP_RCVD, MRA_REP_SENT, REP_SENT, MRA_REP_RCVD, ESTABLISHED:
>>>>   if (cm_id_priv->id.lap_state == IB_CM_LAP_UNINIT) {
>>>>    *qp_attr_mask = IB_QP_STATE | IB_QP_SQ_PSN;
>>>>    qp_attr->sq_psn = be32_to_cpu(cm_id_priv->sq_psn);
>>>>    switch (cm_id_priv->qp_type) {
>>>>    case IB_QPT_{RC, XRC_INI}:
>>>>    	*qp_attr_mask |= IB_QP_RETRY_CNT | IB_QP_RNR_RETRY | IB_QP_MAX_QP_RD_ATOMIC;
>>>>            qp_attr's retry_cnt, rnr_retry, max_rd_atomic as cm_id_priv->retry_count, cm_id_priv->rnr_retry_count, cm_id_priv->initiator_depth
>>>>    case IB_QPT_XRC_TGT:
>>>>    	*qp_attr_mask |= IB_QP_TIMEOUT;
>>>>    	qp_attr->timeout = cm_id_priv->av.timeout;
>>>>    }
>>>>    if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>    	*qp_attr_mask |= IB_QP_PATH_MIG_STATE;
>>>>    	qp_attr->path_mig_state = IB_MIG_REARM;
>>>>    }
>>>>   } else {
>>>>    *qp_attr_mask = IB_QP_ALT_PATH | IB_QP_PATH_MIG_STATE;
>>>>    qp_attr's alt_port_num, alt_pkey_index, alt_timeout, alt_ah_attr, path_mig_state as cm_id_priv->alt_av value
>>>>    qp_attr->path_mig_state = IB_MIG_REARM;
>>>>   }
>>  if (qp_attr->qp_state == IB_QPS_RTR) qp_attr->rq_psn = id_priv->seq_num;
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) {
>>  if (!id_priv->cm_id.iw) {
>>   qp_attr->qp_access_flags = 0;
>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>  }
>>  else ret = iw_cm_init_qp_attr(id_priv->cm_id.iw, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct iwcm_id_private, id);
>>>  switch (qp_attr->qp_state) {
>>>  case IB_QPS_{INIT, RTR}: iwcm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state) {
>>>>  case IW_CM_STATE_{IDLE, CONN_SENT, CONN_RECV, ESTABLISHED}:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>>>  	qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE|IB_ACCESS_REMOTE_READ;
>>>  case IB_QPS_RTS: iwcm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state)
>>>>  case IW_CM_STATE_{IDLE, CONN_SENT, CONN_RECV, ESTABLISHED}: *qp_attr_mask = 0;
>> qp_attr->port_num = id_priv->id.port_num;
>> *qp_attr_mask |= IB_QP_PORT;
>> }
>> if ((*qp_attr_mask & IB_QP_TIMEOUT) && id_priv->timeout_set)
>>   qp_attr->timeout = id_priv->timeout;
>> EXPORT_SYMBOL(rdma_init_qp_attr);
> ib_copy_qp_attr_to_user(ctx->cm_id->device, &resp, &qp_attr);
>> dst's value as src's value
>> ib_copy_ah_attr_to_user(device, &dst->ah_attr, &src->ah_attr);
>>> struct rdma_ah_attr *src = ah_attr;
>>> memset(&dst->grh.reserved, 0, sizeof(dst->grh.reserved));
>>> if ((ah_attr->type == RDMA_AH_ATTR_TYPE_OPA) &&
>>>  (rdma_ah_get_dlid(ah_attr) > be16_to_cpu(IB_LID_PERMISSIVE)) &&
>>>  (!rdma_ah_conv_opa_to_ib(device, &conv_ah, ah_attr)))
>>> src = &conv_ah;
>>> dst->dlid		   = rdma_ah_get_dlid(src);
>>> dst->sl	           = rdma_ah_get_sl(src);
>>> dst->src_path_bits	   = rdma_ah_get_path_bits(src);
>>> dst->static_rate	   = rdma_ah_get_static_rate(src);
>>> dst->is_global         = rdma_ah_get_ah_flags(src) & IB_AH_GRH ? 1 : 0;
>>> if (dst->is_global) {
>>>  const struct ib_global_route *grh = rdma_ah_read_grh(src);
>>>  memcpy(dst->grh.dgid, grh->dgid.raw, sizeof(grh->dgid));
>>>  dst->grh's flow_label, sgid_index, hop_limit, traffic_class as grh->flow_label, grh->sgid_index, grh->hop_limit, grh->traffic_class
>>> }
>>> dst's port_num, reserved as rdma_ah_get_port_num(src), 0;
>> ib_copy_ah_attr_to_user(device, &dst->alt_ah_attr, &src->alt_ah_attr);
>> dst's pkey_index, alt_pkey_index, en_sqd_async_notify, sq_draining, max_rd_atomic, max_dest_rd_atomic, min_rnr_timer, port_num, timeout, retry_cnt, rnr_retry, alt_port_num, alt_timeout as src's value
>> memset(dst->reserved, 0, sizeof(dst->reserved));
>> EXPORT_SYMBOL(ib_copy_qp_attr_to_user);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

rpci_ep_module_init()
-> pci_epf_register_driver(&rpci_ep_driver);
-> rdma_link_register(&rpci_ep_link_ops);
->-> list_add(&ops->list, &link_ops)
-> irq_scheduled_flag = false;

rpci_ep_probe(epf)
-> epf->header = &rpci_ep_pci_header;
-> rdev = ib_alloc_device(rpci_ep_dev, ib_dev);
->-> device = alloc ib_device structure  
->-> device->groups[] = &ib_dev_attr_group
->-> rdma_init_coredev(&device->coredev, device, &init_net);
->->-> coredev->dev.class = &
->->-> coredev->dev.class = &ib_class;
->->-> coredev->dev.groups = dev->groups;
->->-> coredev->owner = dev;
->->-> INIT_LIST_HEAD(&coredev->port_list);
-> epf_set_drvdata(epf, rdev);
->-> epf->dev->driver_data = rdev
-> rdev->epf = epf
-> rdev->status = false
-> init rdev->{cmd_handler, irq_gen_handler)
-> rdev->wq = alloc_workqueue("rpci_ep_wq", WQ_MEM_RECLAIM | WQ_HIGHPRI, 0)
-> rpci_ep_init(rdev);
->-> rpci_ep_init_device_param(rdev);
->->-> init rdev->attr
->-> rpci_ep_init_ports(rdev);
->-> rpci_ep_init_pools(rdev);
-> dma_set_coherent_mask(dev, DMA_BIT_MASK(32))
->-> mask = (dma_addr_t)mask;
->-> dev->coherent_dma_mask = mask;

static struct rdma_link_ops rpci_ep_link_ops = {
	.type = "rpci-ep",
	.newlink = rpci_ep_newlink,
};
rpci_ep_newlink(ibdev_name,ndev)
-> rdev = inited_rdev;
-> rdev->ndev = ndev;
-> rpci_ep_set_mtu(rdev, 0x400);
-> pci_epc_start(rdev->epf->epc);
->-> epc->ops->start(epc);
->->-> pci->ops->start_link(pci)
-> rpci_ep_register_device(rdev, ibdev_name);
-> rdev->ready = true;

static const struct dw_pcie_ops dw_pcie_ep_ops = {
        .read_dbi = NULL,
        .write_dbi = NULL,
};

static const struct pci_epc_ops epc_ops = {
        .write_header           = dw_pcie_ep_write_header,
        .set_bar                = dw_pcie_ep_set_bar,
        .clear_bar              = dw_pcie_ep_clear_bar,
        .map_addr               = dw_pcie_ep_map_addr,
        .unmap_addr             = dw_pcie_ep_unmap_addr,
        .set_msi                = dw_pcie_ep_set_msi,
        .get_msi                = dw_pcie_ep_get_msi,
        .set_msix               = dw_pcie_ep_set_msix,
        .get_msix               = dw_pcie_ep_get_msix,
        .raise_irq              = dw_pcie_ep_raise_irq,
        .start                  = dw_pcie_ep_start,
        .stop                   = dw_pcie_ep_stop,
        .get_features           = dw_pcie_ep_get_features,
};

struct ib_qp *rpci_create_qp(struct ib_pd *ibpd, struct ib_qp_init_attr *init, struct ib_udata *udata)
-> rpci_ep_qp_chk_init(rpci, init);
-> qp = rpci_ep_alloc(&rpci->qp_pool);
	struct rpci_ep_pool_entry *elem;
	ib_device_try_get(&pool->rpci->ib_dev)
	elem = kzalloc(rpci_ep_type_info[pool->type].size, (pool->flags & RPCI_POOL_ATOMIC) ?  GFP_ATOMIC : GFP_KERNEL);
	elem->pool = pool;
-> rpci_ep_add_index(qp);
	elem->index = alloc_index(pool);
	insert_index(pool, elem);
-> rpci_ep_qp_from_init(rpci, qp, pd, init, uresp, ibpd, udata);
	qp->pd			= pd;
	qp->rcq			= rcq;
	qp->scq			= scq;
	qp->srq			= srq;
	rpci_ep_qp_init_misc(rpci, qp, init);
		qp->sq_sig_type		= init->sq_sig_type;
		qp->attr.path_mtu	= 1;
		qp->mtu			= ib_mtu_enum_to_int(qp->attr.path_mtu);
		qpn			= qp->pelem.index;
		port			= &rpci->port;
		qp->ibqp.qp_num		= 0;
		port->qp_smi_index	= qpn;
		qp->attr.port_num	= init->port_num;
	rpci_ep_qp_init_req(rpci, qp, init, udata, uresp);
		sock_create_kern(&init_net, AF_INET, SOCK_DGRAM, 0, &qp->sk);
		qp->sk->sk->sk_user_data = qp;
		qp->rpci = rpci;
		qp->src_port = RPCI_ROCE_V2_SPORT + (hash_32_generic(qp_num(qp), 14) & 0x3fff);
		qp->sq.max_wr		= init->cap.max_send_wr;
		qp->sq.max_sge		= init->cap.max_send_sge;
		qp->sq.max_inline	= init->cap.max_inline_data;
		wqe_size = max_t(int, sizeof(struct rxe_send_wqe) + qp->sq.max_sge * sizeof(struct ib_sge), sizeof(struct rxe_send_wqe) + qp->sq.max_inline);
		qp->sq.queue = rpci_ep_queue_init(rpci, &qp->sq.max_wr, wqe_size);
			q = kmalloc(sizeof(*q), GFP_KERNEL);
			q->rpci = rpci;
			q->elem_size = elem_size;
			elem_size = roundup_pow_of_two(elem_size);
			q->log2_elem_size = order_base_2(elem_size);
			num_slots = *num_elem + 1;
			num_slots = roundup_pow_of_two(num_slots);
			q->index_mask = num_slots - 1;
			buf_size = sizeof(struct rpci_queue_buf) + num_slots * elem_size;
			q->buf = vmalloc_user(buf_size);
			q->buf->log2_elem_size = q->log2_elem_size;
			q->buf->index_mask = q->index_mask;
			q->buf_size = buf_size;
			*num_elem = num_slots - 1;
		ep_do_mmap_info(rpci, uresp ? &uresp->sq_mi : NULL, udata, qp->sq.queue->buf, qp->sq.queue->buf_size, &qp->sq.queue->ip);
			if (outbuf) {
				ip = rpci_ep_create_mmap_info(rpci, buf_size, udata, buf);
					ip = kmalloc(sizeof(*ip), GFP_KERNEL);
					size = PAGE_ALIGN(size);
					ip->info.offset = rpci->mmap_offset;
					rpci->mmap_offset += ALIGN(size, SHMLBA);
					INIT_LIST_HEAD(&ip->pending_mmaps);
					ip->info.size = size;
					ip->context = container_of(udata, struct uverbs_attr_bundle, driver_udata)->context;
					ip->obj = obj;
				copy_to_user(outbuf, &ip->info, sizeof(ip->info));
				list_add(&ip->pending_mmaps, &rpci->pending_mmaps);
	}
	*ip_p = ip;
		qp->req.wqe_index	= producer_index(qp->sq.queue);
		qp->req.state		= QP_STATE_RESET;
		qp->req.opcode		= -1;
		qp->comp.opcode		= -1;
		rpci_ep_init_task(rpci, &qp->req.task, qp, rpci_ep_requester, "req");
			task->obj	= obj;
			task->arg	= arg;
			task->func	= func;
			snprintf(task->name, sizeof(task->name), "%s", name);
			task->destroyed	= false;
			tasklet_init(&task->tasklet, rpci_ep_do_task, (unsigned long)task);
			task->state = TASK_STATE_START;
		rpci_ep_init_task(rpci, &qp->comp.task, qp, rpci_ep_completer, "comp");
		qp->qp_timeout_jiffies = 0; /* Can't be set for UD/UC in modify_qp */
	rpci_ep_qp_init_resp(rpci, qp, init, udata, uresp);
		if (!qp->srq) {
			qp->rq.max_wr		= init->cap.max_recv_wr;
			qp->rq.max_sge		= init->cap.max_recv_sge;
			wqe_size = rcv_wqe_size(qp->rq.max_sge);
			qp->rq.queue = rpci_ep_queue_init(rpci, &qp->rq.max_wr, wqe_size);
			ep_do_mmap_info(rpci, uresp ? &uresp->rq_mi : NULL, udata, qp->rq.queue->buf, qp->rq.queue->buf_size, &qp->rq.queue->ip);
		}
		rpci_ep_init_task(rpci, &qp->resp.task, qp, rpci_ep_responder, "resp");
		qp->resp.opcode		= OPCODE_NONE;
		qp->resp.msn		= 0;
		qp->resp.state		= QP_STATE_RESET;
	qp->attr.qp_state = IB_QPS_RESET;
	qp->valid = 1;


int rpci_ep_requester(void *arg)
> struct rpci_ep_qp *qp = (struct rpci_ep_qp *)arg;
> next_wqe:
> wqe = req_next_wqe(qp);
>> struct rxe_send_wqe *wqe = queue_head(qp->sq.queue);
>>> return q->buf->data + ((q->buf->consumer_index & q->index_mask) << q->log2_elem_size);
>> wqe = addr_from_index(qp->sq.queue, qp->req.wqe_index);
>>> return q->buf->data + ((index & q->index_mask) << q->buf->log2_elem_size);
>> mask = wr_opcode_mask(wqe->wr.opcode, qp);
> if (wqe->mask & WR_REG_MASK) {
>  if (wqe->wr.opcode == IB_WR_LOCAL_INV) {
>   struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
>   struct rpci_ep_mem *rmr;
>   rmr = rpci_ep_pool_get_index(&rpci->mr_pool, wqe->wr.ex.invalidate_rkey >> 8);
>   rmr->state = RPCI_MEM_STATE_FREE;
>   wqe->state = wqe_state_done;
>   wqe->status = IB_WC_SUCCESS;
>  } else if (wqe->wr.opcode == IB_WR_REG_MR) {
>   struct rpci_ep_mem *rmr = to_rmr(wqe->wr.wr.reg.mr);
>   rmr->state = RPCI_MEM_STATE_VALID;
>   rmr->access = wqe->wr.wr.reg.access;
>   rmr->lkey = wqe->wr.wr.reg.key;
>   rmr->rkey = wqe->wr.wr.reg.key;
>   rmr->iova = wqe->wr.wr.reg.mr->iova;
>   wqe->state = wqe_state_done;
>   wqe->status = IB_WC_SUCCESS;
>  }
>  if ((wqe->wr.send_flags & IB_SEND_SIGNALED) || qp->sq_sig_type == IB_SIGNAL_ALL_WR)
>   rpci_ep_run_task(&qp->comp.task, 1);
> }
> opcode = next_opcode(qp, wqe, wqe->wr.opcode);
> mask = rpci_ep_opcode[opcode].mask;
> mtu = get_mtu(qp);
> payload = (mask & RPCI_WRITE_OR_SEND) ? wqe->dma.resid : 0;
> skb = init_req_packet(qp, wqe, opcode, payload, &pkt);
> fill_packet(qp, wqe, &pkt, skb, payload)
>> struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
>> rpci_ep_prepare(pkt, skb, &crc);
>> if (pkt->mask & RPCI_WRITE_OR_SEND) {
>>  if (wqe->wr.send_flags & IB_SEND_INLINE) {
>>   u8 *tmp = &wqe->dma.inline_data[wqe->dma.sge_offset];
>>   crc = rpci_crc32(rpci, crc, tmp, paylen);
>>   memcpy(payload_addr(pkt), tmp, paylen);
>>   wqe->dma.resid -= paylen;
>>   wqe->dma.sge_offset += paylen;
>>  } else {
>>   err = ep_copy_data(qp->pd, 0, &wqe->dma, payload_addr(pkt), paylen, from_mem_obj, &crc);
>>>   struct rxe_sge *sge	= &dma->sge[dma->cur_sge];
>>>   int offset	= dma->sge_offset;
>>>   if (sge->length && (offset < sge->length))
>>> 	mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
>>>   while (length > 0) {
>>>     bytes = length;
>>> 	if (offset >= sge->length) {
>>> 	 sge++;
>>> 	 dma->cur_sge++;
>>> 	 offset = 0;
>>> 	 if (sge->length) {
>>> 	  mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
>>> 	 } else {
>>>       continue;
>>> 	 }
>>>     }
>>> 	if (bytes > sge->length - offset) bytes = sge->length - offset;
>>> 	if (bytes > 0) {
>>> 	 iova = sge->addr + offset;
>>> 	  err = rpci_ep_mem_copy(mem, iova, addr, bytes, dir, crcp);
>>>>       if (mem->type == RPCI_MEM_TYPE_DMA) {
>>>>        u8 *src, *dest;
>>>>        src  = (dir == to_mem_obj) ?  addr : ((void *)(uintptr_t)iova);
>>>>        dest = (dir == to_mem_obj) ?  ((void *)(uintptr_t)iova) : addr;
>>>>        memcpy(dest, src, length);
>>>>       }
>>>>        lookup_iova(mem, iova, &m, &i, &offset);
>>>>        map	= mem->map + m;
>>>>        buf	= map[0]->buf + i;
>>>>        while (length > 0) {
>>>>         u8 *src, *dest;
>>>>         va	= (u8 *)(uintptr_t)buf->addr + offset;
>>>>         src  = (dir == to_mem_obj) ? addr : va;
>>>>         dest = (dir == to_mem_obj) ? va : addr;
>>>>         bytes	= buf->size - offset;
>>>>         if (bytes > length)
>>>>         	bytes = length;
>>>>         memcpy(dest, src, bytes);
>>>>         length	-= bytes;
>>>>         addr	+= bytes;
>>>>         offset	= 0;
>>>>         buf++;
>>>>         i++;
>>>>         if (i == RPCI_BUF_PER_MAP) {
>>>>          i = 0;
>>>>          map++;
>>>>          buf = map[0]->buf;
>>>>         }
>>>>        }
>>>        offset += bytes;
>>>        resid -= bytes;
>>>        length -= bytes;
>>>        addr	+= bytes;
>>> 	}
>>> }
>>> dma->sge_offset = offset;
>>> dma->resid	= resid;
>> 	}
>> }
>> p = payload_addr(pkt) + paylen + bth_pad(pkt);
>> *p = ~crc;
> save_state(wqe, qp, &rollback_wqe, &rollback_psn);
> update_wqe_state(qp, wqe, &pkt);
> update_wqe_psn(qp, wqe, &pkt, payload);
> rpci_ep_xmit_packet(qp, &pkt, skb);
>> int is_request		= pkt->mask & RPCI_REQ_MASK;
>> struct rpci_ep_dev *rpci	= to_rdev(qp->ibqp.device);
>> if (pkt->mask & RPCI_LOOPBACK_MASK) {
>>  memcpy(SKB_TO_PKT(skb), pkt, sizeof(*pkt));
>>  rpci_ep_loopback(skb);
>>  err = 0;
>> } else {
>>  err = rpci_ep_send(pkt, skb);
>>>  struct rpci_ep_dev* rdev = pkt->rpci;
>>>  struct rpci_cr* cr = rdev->cr;
>>>  cr_dbs = rpci_ep_cr_get_db(cr, 0);
>>>  next = ioread32(&cr_dbs->rq_tdbl) == (rdev->q_size - 1) ? 0 : ioread32(&cr_dbs->rq_tdbl) + 1;
>>>  hdbl = ioread32(&cr_dbs->rq_hdbl);
>>>  if (next > hdbl) diff = next - hdbl;
>>>  else diff = rdev->q_size - hdbl + next;
>>>  offset = rdev->q_entry_size * ioread32(&cr_dbs->rq_tdbl);
>>>  header = rdev->mapped_qps[0].rq_ba + offset;
>>>  if (use_dma) rpci_ep_cr_fill_queue_entry_dma(header, rdev->phys_qps[0].rq_ba + offset, skb);
>>>>  set lheader's len, data_len, tail, mac_len, hdr_len, transport_header, network_header, mac_header, protocol, data_offset as skb's data_len, tail, mac_len, hdr_len, transport_header, network_header, mac_header, protocol, data - head
>>>>  memcpy_toio(header, &lheader, sizeof lheader);
>>>>  base = header_dma + RPCI_DATA_OFFSET;
>>>>  renesas_pcie_dma_to_remote_memory(base, skb->head, skb->len + lheader.data_offset);
>>>  else rpci_ep_cr_fill_queue_entry(header, skb);
>>>  iowrite32(next, &cr_dbs->rq_tdbl);
>> }
>> rpci_ep_counter_inc(rpci, RPCI_CNT_SENT_PKTS);
> update_state(qp, wqe, &pkt, payload);


/* implements a simple circular buffer that can optionally be
 * shared between user space and the kernel and can be resized

 * the requested element size is rounded up to a power of 2
 * and the number of elements in the buffer is also rounded
 * up to a power of 2. Since the queue is empty when the
 * producer and consumer indices match the maximum capacity
 * of the queue is one less than the number of element slots
 */

/* this data structure is shared between user space and kernel
 * space for those cases where the queue is shared. It contains
 * the producer and consumer indices. Is also contains a copy
 * of the queue size parameters for user space to use but the
 * kernel must use the parameters in the rpci_queue struct
 * this MUST MATCH the corresponding librpci struct
 * for performance reasons arrange to have producer and consumer
 * pointers in separate cache lines
 * the kernel should always mask the indices to avoid accessing
 * memory outside of the data area
 */
struct rpci_queue_buf {
	__u32			log2_elem_size;
	__u32			index_mask;
	__u32			pad_1[30];
	__u32			producer_index;
	__u32			pad_2[31];
	__u32			consumer_index;
	__u32			pad_3[31];
	__u8			data[0];
};

struct rpci_ep_qp {
	struct rpci_ep_sq		sq;  ---------------------> struct rpci_ep_sq {
	struct rpci_ep_rq		rq;                               struct rpci_queue	*queue; --------------------> struct rpci_queue {
	struct rpci_ep_dev		*rpci;                      }                                                               struct rpci_queue_buf *buf;
};                                                                                                                             }


##################################
# /dev/infiniband/uverb????????????????
##################################

enum ib_uverbs_write_cmds {
/*0-4*/IB_USER_VERBS_CMD_GET_CONTEXT, IB_USER_VERBS_CMD_QUERY_DEVICE, IB_USER_VERBS_CMD_QUERY_PORT, IB_USER_VERBS_CMD_ALLOC_PD, IB_USER_VERBS_CMD_DEALLOC_PD, 
/*5-9*/IB_USER_VERBS_CMD_CREATE_AH, IB_USER_VERBS_CMD_MODIFY_AH, IB_USER_VERBS_CMD_QUERY_AH, IB_USER_VERBS_CMD_DESTROY_AH, IB_USER_VERBS_CMD_REG_MR,
/*10-14*/IB_USER_VERBS_CMD_REG_SMR, IB_USER_VERBS_CMD_REREG_MR, IB_USER_VERBS_CMD_QUERY_MR, IB_USER_VERBS_CMD_DEREG_MR, IB_USER_VERBS_CMD_ALLOC_MW,
/*15-19*/IB_USER_VERBS_CMD_BIND_MW, IB_USER_VERBS_CMD_DEALLOC_MW, IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL, IB_USER_VERBS_CMD_CREATE_CQ, IB_USER_VERBS_CMD_RESIZE_CQ,
/*20-24*/IB_USER_VERBS_CMD_DESTROY_CQ, IB_USER_VERBS_CMD_POLL_CQ, IB_USER_VERBS_CMD_PEEK_CQ, IB_USER_VERBS_CMD_REQ_NOTIFY_CQ, IB_USER_VERBS_CMD_CREATE_QP,
/*25-29*/IB_USER_VERBS_CMD_QUERY_QP, IB_USER_VERBS_CMD_MODIFY_QP, IB_USER_VERBS_CMD_DESTROY_QP, IB_USER_VERBS_CMD_POST_SEND, IB_USER_VERBS_CMD_POST_RECV,
/*30-34*/IB_USER_VERBS_CMD_ATTACH_MCAST, IB_USER_VERBS_CMD_DETACH_MCAST, IB_USER_VERBS_CMD_CREATE_SRQ, IB_USER_VERBS_CMD_MODIFY_SRQ, IB_USER_VERBS_CMD_QUERY_SRQ,
/*35-39*/IB_USER_VERBS_CMD_DESTROY_SRQ, IB_USER_VERBS_CMD_POST_SRQ_RECV, IB_USER_VERBS_CMD_OPEN_XRCD, IB_USER_VERBS_CMD_CLOSE_XRCD, IB_USER_VERBS_CMD_CREATE_XSRQ,
/*40*/IB_USER_VERBS_CMD_OPEN_QP,
};

struct uapi_definition {
	u8 kind;
	u8 scope;
	union {
		struct { u16 object_id; } object_start;
		struct {
			u16 command_num;
			u8 is_ex:1;
			u8 has_udata:1;
			u8 has_resp:1;
			u8 req_size;
			u8 resp_size;
		} write;
	};

	union {
		bool (*func_is_supported)(struct ib_device *device);
		int (*func_write)(struct uverbs_attr_bundle *attrs);
		const struct uapi_definition *chain;
		const struct uverbs_object_def *chain_obj_tree;
		size_t needs_fn_offset;
	};
};

const struct uapi_definition uverbs_def_write_intf[] = {
{
	.kind = UAPI_DEF_OBJECT_START,
	.object_start = { .object_id = UVERBS_OBJECT_AH},
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_CREATE_AH},
	.func_write = ib_uverbs_create_ah,
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_DESTROY_AH},
	.func_write = ib_uverbs_destroy_ah,
	.write.req_size = sizeof(struct ib_uvers_destroy_ah)
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_IS_SUPPORTED_DEV_FN,
	.scope = UAPI_SCOPE_METHOD,
	.needs_fn_offset = offsetof(struct ib_device_ops, destroy_ah) + BUILD_BUG_ON_ZERO(sizeof(((struct ib_device_ops *)0)->destroy_ah) != sizeof(void *)),
	}
	,????
}

static const struct file_operations uverbs_mmap_fops = {
	.owner   = THIS_MODULE,
	.write   = ib_uverbs_write,
	.mmap    = ib_uverbs_mmap,
	.open    = ib_uverbs_open,
	.release = ib_uverbs_close,
	.llseek  = no_llseek,
	.unlocked_ioctl = ib_uverbs_ioctl,
	.compat_ioctl = ib_uverbs_ioctl,
};

static struct ib_client uverbs_client = {
	.name   = "uverbs",
	.no_kverbs_req = true,
	.add    = ib_uverbs_add_one,
	.remove = ib_uverbs_remove_one,
	.get_nl_info = ib_uverbs_get_nl_info,
};
MODULE_ALIAS_RDMA_CLIENT("uverbs");

ib_uvers_init() // module_init(ib_uverbs_init);
> register_chrdev_region(IB_UVERBS_BASE_DEV, IB_UVERBS_NUM_FIXED_MINOR, "infiniband_verbs");
> alloc_chrdev_region(&dynamic_uverbs_dev, 0, IB_UVERBS_NUM_DYNAMIC_MINOR, "infiniband_verbs");
> uverbs_class = class_create(THIS_MODULE, "infiniband_verbs");
> uverbs_class->devnode = uverbs_devnode;
> class_create_file(uverbs_class, &class_attr_abi_version.attr);
> ib_register_client(&uverbs_client);

ib_uverbs_add_one()
> uverbs_dev = kzalloc(sizeof(*uverbs_dev), GFP_KERNEL);
> init_srcu_struct(&uverbs_dev->disassociate_srcu);
> device_initialize(&uverbs_dev->dev);
> uverbs_dev->dev.class = uverbs_class; uverbs_dev->dev.parent = device->dev.parent; uverbs_dev->dev.release = ib_uverbs_release_dev; uverbs_dev->dev.groups = uverbs_dev->groups;
> uverbs_dev->groups[0] = &dev_attr_group;
> atomic_set(&uverbs_dev->refcount, 1);
> init_completion(&uverbs_dev->comp);
> uverbs_dev->xrcd_tree = RB_ROOT;
> mutex_init(&uverbs_dev->xrcd_tree_mutex); mutex_init(&uverbs_dev->lists_mutex);
> INIT_LIST_HEAD(&uverbs_dev->uverbs_file_list); INIT_LIST_HEAD(&uverbs_dev->uverbs_events_file_list);
> rcu_assign_pointer(uverbs_dev->ib_dev, device);
> uverbs_dev->num_comp_vectors = device->num_comp_vectors;
> devnum = ida_alloc_max(&uverbs_ida, IB_UVERBS_MAX_DEVICES - 1, GFP_KERNEL);
> uverbs_dev->devnum = devnum;
> if (devnum >= IB_UVERBS_NUM_FIXED_MINOR) base = dynamic_uverbs_dev + devnum - IB_UVERBS_NUM_FIXED_MINOR;
> else base = IB_UVERBS_BASE_DEV + devnum;
> ib_uverbs_create_uapi(device, uverbs_dev)
> uverbs_dev->dev.devt = base;
> dev_set_name(&uverbs_dev->dev, "uverbs%d", uverbs_dev->devnum);
> cdev_init(&uverbs_dev->cdev, device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);
> uverbs_dev->cdev.owner = THIS_MODULE;
> cdev_device_add(&uverbs_dev->cdev, &uverbs_dev->dev);
> ib_set_client_data(device, &uverbs_client, uverbs_dev);

ib_uverbs_open()
> dev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);
> ib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);
> file = kzalloc(sizeof(*file), GFP_KERNEL);
> file->device	 = dev;
> filp->private_data = file;
> list_add_tail(&file->list, &dev->uverbs_file_list);
> return stream_open(inode, filp);

ib_uverbs_write()
> copy_from_user(&hdr, buf, sizeof(hdr))
> method_elm = uapi_get_method(uapi, hdr.command);
> verify_hdr(&hdr, &ex_hdr, count, method_elm);
> srcu_key = srcu_read_lock(&file->device->disassociate_srcu);
> buf += sizeof(hdr);
> memset(bundle.attr_present, 0, sizeof(bundle.attr_present));
> bundle.ufile = file; bundle.context = NULL;
> if (!method_elm->is_ex) {
>> set bundle.driver_udata
>> ib_uverbs_init_udata_buf_or_null( &bundle.ucore, buf, u64_to_user_ptr(response), in_len, out_len);
> else
>> buf += sizeof(ex_hdr);
>> ib_uverbs_init_udata_buf_or_null(&bundle.ucore, buf, u64_to_user_ptr(ex_hdr.response), hdr.in_words * 8, hdr.out_words * 8);
>> ib_uverbs_init_udata_buf_or_null( &bundle.driver_udata, buf + bundle.ucore.inlen, u64_to_user_ptr(ex_hdr.response) + bundle.ucore.outlen, ex_hdr.provider_in_words * 8, ex_hdr.provider_out_words * 8);
> method_elm->handler(&bundle);

ib_uvebrs_ioctl()
> struct ib_uverbs_file *file = filp->private_data;
> struct ib_uverbs_ioctl_hdr __user *user_hdr = (struct ib_uverbs_ioctl_hdr __user *)arg;
> copy_from_user(&hdr, user_hdr, sizeof(hdr));
> ib_uverbs_cmd_verbs(file, &hdr, user_hdr->attrs);

ib_uverbs_mmap()
> struct ib_uverbs_file *file = filp->private_data;
> ucontext = ib_uverbs_get_ucontext_file(file);
> ucontext->device->ops.mmap(ucontext, vma);

int ib_uverbs_reg_mr(struct uverbs_attr_bundle *attrs)
> uverbs_request(attrs, &cmd, sizeof(cmd));
>> copy_from_user(req, attrs->ucore.inbuf, min(attrs->ucore.inlen, req_len))
>> if (attrs->ucore.inlen < req_len) { memset(req + attrs->ucore.inlen, 0, req_len - attrs->ucore.inlen);
> ib_check_mr_access(cmd.access_flags);
> uobj = uobj_alloc(UVERBS_OBJECT_MR, attrs, &ib_dev);
>> __uobj_alloc(uobj_get_type(_attrs, _type), _attrs, _ib_dev)
>>> struct ib_uobject *uobj = rdma_alloc_begin_uobject(obj, attrs->ufile, attrs);
>>>> ret = obj->type_class->alloc_begin(obj, ufile);
>>>> if (attrs) attrs->context = ret->context;
>>>> return ret;
> pd = uobj_get_obj_read(pd, UVERBS_OBJECT_PD, cmd.pd_handle, attrs);
>> return ((struct ib_pd *)uobj->object;
// struct ib_mr *rpci_ep_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length, u64 iova, int access, struct ib_udata *udata)
> mr = pd->device->ops.reg_user_mr(pd, cmd.start, cmd.length, cmd.hca_va, cmd.access_flags, &attrs->driver_udata);
>> struct rpci_ep_dev *rpci = to_rdev(ibpd->device); // get rpci_ep_dev as parent from ibpd->device
>> struct rpci_ep_pd *pd = to_rpd(ibpd); // get rpci_ep_pd as parent from ipbd
>> struct rpci_ep_mem *mr = rpci_ep_alloc(&rpci->mr_pool);
>>> elem = kzalloc(rpci_ep_type_info[pool->type].size, (pool->flags & RPCI_POOL_ATOMIC) ?  GFP_ATOMIC : GFP_KERNEL);
>>> elem->pool = pool;
>>> return elem;
>> rpci_ep_add_index(mr);
>>> struct rpci_ep_pool_entry *elem = arg; struct rpci_pool *pool = elem->pool;
>>> elem->index = alloc_index(pool);
>>> insert_index(pool, elem);
>> rpci_ep_add_ref(pd);
>> rpci_ep_mem_init_user(pd, start, length, iova, access, udata, mr);
>>> struct ib_umem *umem = ib_umem_get(udata, start, length, access, 0); // Pin and DMA map userspace memory.
>>>> // struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr, size_t size, int access, int dmasync)
>>>> umem = kzalloc();
>>>> set umem's ibdev, length, address, writable, owning_mm
>>>>  as context->device, @size, @addr, ib_access_writable(@access), current->mm
>>>> page_list = (struct page **) __get_free_page(GFP_KERNEL); // alloc single page
>>>> npages = ib_umem_num_pages(umem);
>>>> cur_base = @addr & PAGE_MASK;
>>>> sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL); // Allocate and initialize an sg table
>>>> sg = umem->sg_head.sgl;
>>>> while (npages)
>>>> 	ret = get_user_pages(cur_base, min_t(ulong, npages, PAGE_SIZE/szof(struct page *)),, page_list, NULL);
>>>>> __gup_longterm_locked(current, current->mm, start, nr_pages, pages, vmas, gup_flags | FOLL_TOUCH);
>>>>>> __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas, NULL, flags);
>>>>>>> __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas, locked);
>>>>>>>> get_gate_page(mm, start & PAGE_MASK, gup_flags, &vma, pages ? &pages[i] : NULL);
>>>>>>>>> pgd = pgd_offset_gate(mm, address);
>>>>>>>>> p4d = p4d_offset(pgd, address);
>>>>>>>>> pud = pud_offset(p4d, address);
>>>>>>>>> pmd = pmd_offset(pud, address);
>>>>>>>>> pte = pte_offset_map(pmd, address);
>>>>>>>>> *vma = get_gate_vma(mm);
>>>>>>>>> *page = vm_normal_page(*vma, address, *pte);
>>>> 	cur_base += ret * PAGE_SIZE; npages -= ret;
>>>> 	sg = ib_umem_add_sg_table(sg, page_list, ret, dma_get_max_seg_size(context->device->dma_device)
>>>> 				  , &umem->sg_nents); //Add N contiguous pages to scatter table
>>>> umem->nmap = ib_dma_map_sg_attrs(context->device, umem->sg_head.sgl, umem->sg_nents, DMA_BIDIRECTIONAL, dma_attrs);
>>>>> return dma_map_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
>>>>>>if (dma_is_direct(ops)) ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
>>>>>>> for_each_sg(sgl, sg, nents, i)
>>>>>>>	 sg->dma_address = dma_direct_map_page(dev, sg_page(sg), sg->offset, sg->length, dir, attrs);
>>>>>>>	 sg_dma_len(sg) = sg->length;
>>> set mem's umem, page_shift, page_mask as umem, PAGE_SHIFT, PAGE_SIZE - 1
>>> rpci_ep_mem_init(access, mem);
>>>> u32 lkey = mem->pelem.index << 8 | rpci_ep_get_key();
>>>> u32 rkey = (access & IB_ACCESS_REMOTE) ? lkey : 0;
>>>> if (mem->pelem.pool->type == RPCI_TYPE_MR) set mem->ibmr's lkey, rkey as lkey, rkey
>>>> set mem's lkey, rkey, state, type, map_shift as lkey, rkey, RPCI_MEM_STATE_INVALID, RPCI_MEM_TYPE_NONE, ilog2(RPCI_BUF_PER_MAP)
>>> num_buf = ib_umem_num_pages(umem);
>>> rpci_ep_mem_alloc(mem, num_buf);
>>>> struct rpci_ep_map **map = mem->map;
>>>> num_map = (num_buf + RPCI_BUF_PER_MAP - 1) / RPCI_BUF_PER_MAP;
>>>> mem->map = kmalloc_array(num_map, sizeof(*map), GFP_KERNEL);
>>>> for (i = 0; i < num_map; i++) mem->map[i] = kmalloc(sizeof(**map), GFP_KERNEL);
>>>> set mem's map_shift, map_mask, num_buf, num_map, max_buf
>>>>  as ilog2(RPCI_BUF_PER_MAP), RPCI_BUF_PER_MAP - 1, num_buf, num_map, num_map * RPCI_BUF_PER_MAP
>>> num_buf = 0; map = mem->map; buf = map[0]->buf;
>>> for_each_sg_page(umem->sg_head.sgl, &sg_iter, umem->nmap, 0)
>>>  if (num_buf >= RPCI_BUF_PER_MAP) map++; buf = map[0]->buf; num_buf = 0;
>>>  vaddr = page_address(sg_page_iter_page(&sg_iter)); //get virt addr
>>>  buf->addr = (uintptr_t)vaddr; buf->size = PAGE_SIZE; num_buf++; buf++;
>>> set mem's pd,umem, access, length, iova, va, offset, state, type
     as pd, umem, access, length, iova, start, ib_umem_offset(umem), RPCI_MEM_STATE_VALID, RPCI_MEM_TYPE_MR
> set mr's device,pd, type, dm, sig_attr, uobject, res.type, iova
   as pd->device, pd, IB_MR_TYPE_USER, NULL, NULL, uobj, RDMA_RESTRACK_MR, cmd.hca_va
> rdma_restrack_uadd(&mr->res);
> uobj->object = mr; > resp.lkey = mr->lkey; resp.rkey = mr->rkey; resp.mr_handle = uobj->id;
> uverbs_response(attrs, &resp, sizeof(resp));
>> if (uverbs_attr_is_valid(attrs, UVERBS_ATTR_CORE_OUT)) return uverbs_copy_to_struct_or_zero( attrs, UVERBS_ATTR_CORE_OUT, resp, resp_len);
>> copy_to_user(attrs->ucore.outbuf, resp, min(attrs->ucore.outlen, resp_len)
> return uobj_alloc_commit(uobj, attrs);
>> struct ib_uverbs_file *ufile = attrs->ufile;
>> uobj->uapi_object->type_class->alloc_commit(uobj);
>> list_add(&uobj->list, &ufile->uobjects);
>> atomic_set(&uobj->usecnt, 0);
