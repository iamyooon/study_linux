main()
-> set cb(control block)
   alloce memory for rping_cb
   setting cb's server, size, count, verbose, validate, self_create_gp
   setting cb's sin.ss_family, port as PF_INET, htobe16(7174)
-> cb->cm_channel = create_first_event_channel()
->-> rdma_create_event_channel()
->->-> ucma_init()
->->->-> check_abi_version();
->->->-> dev_list = ibv_get_device_list(&numb_dev);
->->->->-> if (!initialized) ibverbs_init()
->->->->-> num_devices = ibverbs_get_device_list(&device_list);
		ret = find_sysfs_devs_nl(&sysfs_list);
		list_for_each_safe(device_list, vdev, tmp, entry) {
			list_for_each(&sysfs_list, sysfs_dev, entry) {
				if (same_sysfs_dev(vdev->sysfs, sysfs_dev)) {
					old_sysfs = sysfs_dev;
					break;
			if (old_sysfs)
				list_del(&old_sysfs->entry);
				free(old_sysfs);
				num_devices++;
			else
				list_del(&vdev->entry);
				ibverbs_device_put(&vdev->device);
		try_all_drivers(&sysfs_list, device_list, &num_devices);
		if (list_empty(&sysfs_list) || drivers_loaded) goto out;
		load_drivers();
		drivers_loaded = 1;
		try_all_drivers(&sysfs_list, device_list, &num_devices);
->->->->-> l = calloc(num_devices + 1, sizeof (struct ibv_device *));
->->->->-> list_for_each(&device_list, device, entry) {
		l[i] = &device->device;
		ibverbs_device_hold(l[i]);
		i++;
->->->->-> if (num) *num = num_devices;
->->->-> setting cma_dev_{array, cnt}
	cma_dev_array = calloc(dev_cnt, sizeof(*cma_dev_array));
	for (i = 0; dev_list[i]; i++) cma_dev_array[i].guid = ibv_get_device_guid(dev_list[i]);
	cma_dev_cnt = dev_cnt;
->->->-> ucma_set_af_ib_support();
->->->->-> rdma_create_id(NULL, &id, NULL, RDMA_PS_IB);
->->->->-> sib's sib_family, sib_sid, sib_sid_mask as AF_IB, htobe64(RDMA_IB_IP_PS_TCP), htobe64(RDMA_IB_IP_PS_MASK)
->->->->-> af_ib_support = 1;
->->->->-> ret = rdma_bind_addr(id, (struct sockaddr *) &sib);
->->->->->-> addrlen = ucma_addrlen(addr);
->->->->->-> if (af_ib_support) return rdma_bind_addr2(id, addr, addrlen);
->->->->->->-> setting cmd's cmd, in, id, addr as UCMA_CMD_BIND, sizeof(cmd) - sizeof(struct ucma_abi_cmd_hdr), id's id_priv->handle, addr
->->->->->->-> write(id->channel->fd, &cmd, sizeof cmd);
->->->->->->-> ucma_query_addr(id);
->->->->->->->-> setting cmd's cmd, in, id, option as UCMA_CMD_QUERY, sizeof(cmd) - sizeof(struct ucma_abi_cmd_hdr), id's id_priv->handle, UCMA_QUERY_ADDR
->->->->->->->-> setting cmd's out, response as sizeof(resp), &resp
->->->->->->->-> write(id->channel->fd, &cmd, sizeof cmd);
->->->->->->->-> memcpy(&id->route.addr.src_addr, &resp.src_addr, resp.src_size);
->->->->->->->-> memcpy(&id->route.addr.dst_addr, &resp.dst_addr, resp.dst_size);
->->->->->->->-> if (!id_priv->cma_dev && resp.node_guid)
->->->->->->->-> ret = ucma_get_device(id_priv, resp.node_guid,
                 TBD. 더 들어가야 함...
->->->->->->->->-> for (i = 0; i < cma_dev_cnt; i++)
		     cma_dev = &cma_dev_array[i];
		     if (cma_dev->guid == guid) goto match;
->->->->->->->->-> ret = ucma_init_device(cma_dev);
			cma_dev->verbs = ucma_open_device(cma_dev->guid);
			ibv_query_device(cma_dev->verbs, &attr);
			cma_dev->port = malloc(sizeof(*cma_dev->port) * attr.phys_port_cnt);
			for (i = 1; i <= attr.phys_port_cnt; i++)
				if (ibv_query_port(cma_dev->verbs, i, &port_attr))
					cma_dev->port[i - 1].link_layer = IBV_LINK_LAYER_UNSPECIFIED;
				else
					cma_dev->port[i - 1].link_layer = port_attr.link_layer;
			cma_dev->port_cnt = attr.phys_port_cnt;
			cma_dev->max_qpsize = attr.max_qp_wr;
			cma_dev->max_initiator_depth = (uint8_t) attr.max_qp_init_rd_atom;
			cma_dev->max_responder_resources = (uint8_t) attr.max_qp_rd_atom;
			cma_init_cnt++;
->->->->->->->->-> id_priv->cma_dev = cma_dev;
->->->->->->->->-> id_priv->id.verbs = cma_dev->verbs;
->->->->->->->->-> id_priv->id.pd = cma_dev->pd;
->->->->->->->-> id->port_num = resp.port_num;
->->->->->->->-> id->route.addr.addr.ibaddr.pkey = resp.pkey;
->->->->-> af_ib_support = !ret;
->->->->-> rdma_destroy_id(id);
->->->-> ibv_free_device_list(dev_list);
->->-> channel = malloc(sizeof(*channel));
->->-> channel->fd = open_cdev(dev_name, dev_cdev);
->->->->// ./buildlib/config.h.in:16:#define RDMA_CDEV_DIR "/dev/infiniband"
	// static char dev_name[64] = "rdma_cm";
	asprintf(&devpath, RDMA_CDEV_DIR "/%s", devname_hint) < 0)
	fd = open_cdev_internal(devpath, cdev);
->->->->-> return open(path, O_RDWR | O_CLOEXEC)

-> rdma_create_id(cb->cm_channel, &cb->cm_id, cb, RDMA_PS_TCP) /PS -> port space)
->-> qp_type = (ps == RDMA_PS_IPOIB || ps == RDMA_PS_UDP) ?  IBV_QPT_UD : IBV_QPT_RC;
->-> rdma_create_id2(channel, id, context, ps, qp_type)
->->-> ucma_init();
->->->-> if(cmd_dev_cnt) return 0;
->->-> id_priv = ucma_alloc_id(channel, context, ps, qp_type);
->->->-> id_priv = calloc(1, sizeof(*id_priv));
		id_priv's id.context, id.ps, id.qp_type, handle as context, ps, qp_type, 0xFFFFFFFF
		if (!channel) id_priv->id.channel = rdma_create_event_channel(); id_priv->sync = 1;
		else id_priv->id.channel = channel;
		return id_priv;
->->->-> cmd.out = sizeof(resp) cmd.response = &resp cmd.uid = (uintptr_t) id_priv; cmd.ps = ps; cmd.qp_type = qp_type;
->->->-> setting cmd's cmd, in, uid, ps, qp_type as UCMA_CMD_CREATE_ID, sizeof(cmd) - sizeof(struct ucma_abi_cmd_hdr), id_priv, ps, qp_type
->->->-> setting cmd's out, response as sizeof(resp), &resp
->->->-> write(id_priv->id.channel->fd, &cmd, sizeof cmd);
->->->-> id_priv->handle = resp.id; // struct cmd_id_private *id_priv
->->->-> ucma_insert_id(id_priv);
->->->-> *id = &id_priv->id; // struct rdma_cm_id** id
-> pthread_create(&cb->cmthread, NULL, cm_thread, cb);
-> if (cb->server) if (persistent_server) ret = rping_run_persistent_server(cb); else ret = rping_run_server(cb);
-> else rping_run_client(cb);

##########################################
# cm_thread (run as thread)
##########################################
cm_thread()
-> while (1) {
->-> rdma_get_cm_event(cb->cm_channel, &event);
->->-> ucma_init();
->->->-> if (cmd_dev_cnt) return 0;
->->-> evt = malloc(sizeof(*evt));
       retry:
	memset(evt, 0, sizeof(*evt));
	CMA_INIT_CMD_RESP(&cmd, sizeof cmd, GET_EVENT, &resp, sizeof resp);
	ret = write(channel->fd, &cmd, sizeof cmd);
	setting evt->event's event, id, status as resp.event, &evt->id_priv->id, resp.status
	if (resp.uid) evt->id_priv = (void *) (uintptr_t) resp.uid;
	else { evt->id_priv = ucma_lookup_id(resp.id);
		if (resp.event != RDMA_CM_EVENT_ESTABLISHED) { ucma_complete_event(evt->id_priv); goto retry;
	switch (resp.event)
	case RDMA_CM_EVENT_ADDR_RESOLVED:
		ucma_process_addr_resolved(evt);
	case RDMA_CM_EVENT_ROUTE_RESOLVED:
		ucma_process_route_resolved(evt);
	case RDMA_CM_EVENT_CONNECT_REQUEST:
		evt->id_priv = (void *) (uintptr_t) resp.uid;
		if (ucma_is_ud_qp(evt->id_priv->id.qp_type)) ucma_copy_ud_event(evt, &resp.param.ud);
		else ucma_copy_conn_event(evt, &resp.param.conn);
		ucma_process_conn_req(evt, resp.id);
	case RDMA_CM_EVENT_CONNECT_RESPONSE:
		ucma_copy_conn_event(evt, &resp.param.conn);
		if (!evt->id_priv->id.qp) { evt->event.event = RDMA_CM_EVENT_CONNECT_RESPONSE;
		} else { evt->event.status = ucma_process_conn_resp(evt->id_priv);
			if (!evt->event.status) evt->event.event = RDMA_CM_EVENT_ESTABLISHED;
			else { evt->event.event = RDMA_CM_EVENT_CONNECT_ERROR; evt->id_priv->connect_error = 1;
	case RDMA_CM_EVENT_ESTABLISHED:
		if (ucma_is_ud_qp(evt->id_priv->id.qp_type)) { ucma_copy_ud_event(evt, &resp.param.ud);
		ucma_copy_conn_event(evt, &resp.param.conn);
	case RDMA_CM_EVENT_REJECTED:
		if (evt->id_priv->connect_error) { ucma_complete_event(evt->id_priv); goto retry;
		ucma_copy_conn_event(evt, &resp.param.conn);
		ucma_modify_qp_err(evt->event.id);
	case RDMA_CM_EVENT_DISCONNECTED:
		if (evt->id_priv->connect_error) { ucma_complete_event(evt->id_priv); goto retry;
		ucma_copy_conn_event(evt, &resp.param.conn);
	case RDMA_CM_EVENT_MULTICAST_JOIN:
		evt->mc = (void *) (uintptr_t) resp.uid; evt->id_priv = evt->mc->id_priv; evt->event.id = &evt->id_priv->id;
		ucma_copy_ud_event(evt, &resp.param.ud); evt->event.param.ud.private_data = evt->mc->context; evt->event.status = ucma_process_join(evt);
		if (evt->event.status) evt->event.event = RDMA_CM_EVENT_MULTICAST_ERROR;
	case RDMA_CM_EVENT_MULTICAST_ERROR:
		evt->mc = (void *) (uintptr_t) resp.uid; evt->id_priv = evt->mc->id_priv; evt->event.id = &evt->id_priv->id;
		evt->event.param.ud.private_data = evt->mc->context;
	default:
		evt->id_priv = (void *) (uintptr_t) resp.uid; evt->event.id = &evt->id_priv->id; evt->event.status = resp.status;
		if (ucma_is_ud_qp(evt->id_priv->id.qp_type)) ucma_copy_ud_event(evt, &resp.param.ud);
		else ucma_copy_conn_event(evt, &resp.param.conn);
	}
	*event = &evt->event;
->-> rping_cma_event_handler(event->id, event);
->->-> switch (event->event) {
->->-> case RDMA_CM_EVENT_ADDR_RESOLVED: cb->state = ADDR_RESOLVED; ret = rdma_resolve_route(cma_id, 2000);
->->-> case RDMA_CM_EVENT_ROUTE_RESOLVED: cb->state = ROUTE_RESOLVED;
->->-> case RDMA_CM_EVENT_CONNECT_REQUEST: cb->state = CONNECT_REQUEST; cb->child_cm_id = cma_id;
->->-> case RDMA_CM_EVENT_CONNECT_RESPONSE: cb->state = CONNECTED;
->->-> case RDMA_CM_EVENT_ESTABLISHED: if (!cb->server) cb->state = CONNECTED;
->->-> case RDMA_CM_EVENT_{ADDR_ERROR, ROUTE_ERROR,CONNECT_ERROR,UNREACHABLE,REJECTED,DISCONNECTED : cb->state = DISCONNECTED;
->->-> case RDMA_CM_EVENT_DEVICE_REMOVAL: cb->state = ERROR;
->-> rdma_ack_cm_event(event);
	if (evt->mc) ucma_complete_mc_event(evt->mc);
	else ucma_complete_event(evt->id_priv);

##########################################
# client
##########################################
rping_run_client(struct rping_cb *cb)
-> rping_bind_client(cb);
->-> &cb->sin->sin_port or sin6_port = cb->port;
->-> if (cb->ssouce.ss_family) rdma_resolve_addr(cb->cm_id, (struct sockaddr *) &cb->ssource, (struct sockaddr *) &cb->sin, 2000)
->->-> dst_len = ucma_addrlen(dst_addr), src_len = ucma_addrlen(src_addr)
->->-> if (af_ib_support) return rdma_resolve_addr2(id, src_addr, src_len, dst_addr, dst_len, timeout_ms);
->->->-> setting cmd's cmd, in, id, src_size, src_addr, dst_addr, dst_size, timeout_ms
         as UCMA_CMD_RESOLVE_ADDR, sizeof(cmd) - sizeof(struct ucma_abi_cmd_hdr), id's id_priv->handle, src_len, src_addr, dst_addr, dst_len, timeout_ms
->->->-> write(id->channel->fd, &cmd, sizeof cmd);
->->->-> id->route.addr.dst_addr = dst_addr
->->->-> return ucma_complete(id);
		if (id_priv->id.event) rdma_ack_cm_event(id_priv->id.event); id_priv->id.event = NULL;
		rdma_get_cm_event(id_priv->id.channel, &id_priv->id.event);
		if (id_priv->id.event->status)
			if (id_priv->id.event->event == RDMA_CM_EVENT_REJECTED) ret = ERR(ECONNREFUSED);
			else if (id_priv->id.event->status < 0) ret = ERR(-id_priv->id.event->status);
			else ret = ERR(id_priv->id.event->status);
>->->-> setting cmd's cmd, in, id, src_addr, dst_addr, timeout_ms, dst_storage
        as UCMA_CMD_RESOLVE_IP, sizeof(cmd)-sizeof(struct ucma_abi_cmd_hdr), id's id_priv->handle, src_addr, dst_addr, timeout_ms, dst_storage
->->-> write(id->channel->fd, &cmd, sizeof cmd);
->->-> return ucma_complete(id);
-> rping_setup_qp(cb, cb->cm_id);
->-> cb->pd = ibv_alloc_pd(cm_id->verbs); // rxe_alloc_pd()
->->-> pd = malloc(sizeof *pd);
->->->-> ibv_cmd_alloc_pd(context, pd, &cmd, sizeof cmd, &resp, sizeof resp)
->->->->-> execute_cmd_write(context, IB_USER_VERBS_CMD_ALLOC_PD, cmd, cmd_size, resp, resp_size);
->->->->->-> cmd->core_payload.response = ioctl_ptr_to_u64(resp)
->->->->->-> return ioctl_write(ctx, write_method, req + 1, core_req_size - sizeof(*req), req_size - sizeof(*req), resp, core_resp_size, resp_size);
->->->->->-> setting req's command, in_words, out_word as @write_method, ?, ?
->->->->->-> write(ctx->cmd_fd, req, req_size);
->->->->-> setting pd's handle, context as resp->pd_handle, @context;
->->-> pd->context = context; //context = @cm_id->verbs
->-> cb->channel = ibv_create_comp_channel(cm_id->verbs);
->->-> channel = malloc(sizeof *channel);
->->-> req.core_payload = (struct ib_uverbs_create_comp_channel){};
->->-> execute_cmd_write(context, IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL, &req, sizeof(req), &resp, sizeof(resp))
->->-> setting channel's context, fd, refcnt as @context, resp.fd, 0
->-> cb->cq = ibv_create_cq(cm_id->verbs, RPING_SQ_DEPTH * 2, cb, cb->channel, 0);
->->-> cq = get_ops(context)->create_cq(context, cqe, channel, comp_vector); //rxe_create_cq()
	cq = malloc(sizeof *cq);
	ibv_cmd_create_cq(context, cqe, channel, comp_vector, &cq->ibv_cq, NULL, 0, &resp.ibv_resp, sizeof resp);
	cq->queue = mmap(NULL, resp.mi.size, PROT_READ | PROT_WRITE, MAP_SHARED, context->cmd_fd, resp.mi.offset);
	cq->mmap_info = resp.mi;
	return &cq->ibv_cq;
->->-> verbs_init_cq(cq, context, channel, cq_context);
->->->-> cq->context = context; cq->channel = channel;
->->->-> ++cq->channel->refcnt; cq->cq_context = cq_context; cq->comp_events_completed = 0; cq->async_events_completed = 0;
->-> ibv_req_notify_cq(cb->cq, 0);
->->-> return cq->context->ops.req_notify_cq(cq, solicited_only); // ibv_cmd_req_notify_cq(cb->cq, 0)
->->->-> req.core_payload = (struct ib_uverbs_req_notify_cq){ .cq_handle = ibcq->handle, .solicited_only = !!solicited_only, };
->->->-> return execute_cmd_write_req(ibcq->context, IB_USER_VERBS_CMD_REQ_NOTIFY_CQ, &req, sizeof(req));
->-> rping_create_qp(cb);
->->-> if (cb->self_create_qp) cb->qp = ibv_create_qp(cb->pd, &init_attr);
->->->-> struct ibv_qp *qp = get_ops(pd->context)->create_qp(pd, qp_init_attr); //rxe_create_qp()
->->->->-> TBD.
->->->-> setting qp's context, qp_context, pd, send_cq, recv_cq, srq, qp_type, state, events_completed
         as pd->context, qp_init_attr->qp_context, pd, qp_init_attr->send_cq, qp_init_attr->recv_cq, qp_init_attr->srq, qp_init_attr->qp_type, IBV_QPS_RESET, 0
->->-> struct ibv_qp_attr attr = { .qp_state = IBV_QPS_INIT, .pkey_index = 0, .port_num = id->port_num, .qp_access_flags = 0, };
->->-> ret = ibv_modify_qp(cb->qp, &attr, IBV_QP_STATE | IBV_QP_PKEY_INDEX | IBV_QP_PORT | IBV_QP_ACCESS_FLAGS); // rxe_modify_gp();
->->->-> TBD.
->->-> ret = rdma_create_qp(id, cb->pd, &init_attr);
->->->-> TBD.
->->-> if (!ret) cb->qp = id->qp;
-> rping_setup_buffers(cb);
>> cb->recv_mr = ibv_reg_mr(cb->pd, &cb->recv_buf, sizeof cb->recv_buf, IBV_ACCESS_LOCAL_WRITE); //rxe_reg_mr()
>>> __ibv_reg_mr(pd, addr, length, access, __builtin_constant_p(((access) & IBV_ACCESS_OPTIONAL_RANGE) == 0))
>>>> if (is_access_const && (access & IBV_ACCESS_OPTIONAL_RANGE) == 0) return ibv_reg_mr(pd, addr, length, access);
>>>>> mr = get_ops(pd->context)->reg_mr(pd, addr, length, (uintptr_t) addr, access); // rxe_reg_mr()
>>>>>> vmr = malloc(sizeof(*vmr));
>>>>>> ibv_cmd_reg_mr(pd, addr, length, hca_va, access, vmr, &cmd, sizeof(cmd), &resp, sizeof(resp));
>>>>>>> set cmd's start,length,hca_va, pd_handle, access_flags as addr, length, hca_va, pd->handle, access
>>>>>>> execute_cmd_write(pd->context, IB_USER_VERBS_CMD_REG_MR, cmd, cmd_size, resp, resp_size);
>>>>>>>> cmd->core_payload.response = ioctl_ptr_to_u64(resp);
>>>>>>>> _execute_cmd_write()
>>>>>>>>> set req's command, in_words, out_words as  write_method, req_size, resp_size
>>>>>>>>> write(ctx->cmd_fd, req, req_size)
>>>>>>> set vmr->ibv_mr's handle, lkey, rkey, context as resp's mr_handle, lkey, rkey
>>>>>>> set vmr mr_type, access as IBV_MR_TYPE_MR, access
>>>>>> return &vmr->ibv_mr;
>>>>> if (mr) set mr's context, pd, addr, length as pd->context, pd, addr, length
>>>> else return ibv_reg_mr_iova2(pd, addr, length, (uintptr_t)addr, access);
>>>>> mr = get_ops(pd->context)->reg_mr(pd, addr, length, (uintptr_t) addr, access);
>>>>> mr->context = pd->context; mr->pd      = pd; mr->addr    = addr; mr->length  = length;
->-> cb->send_mr = ibv_reg_mr(cb->pd, &cb->send_buf, sizeof cb->send_buf, 0);
->-> cb->rdma_buf = malloc(cb->size);
->-> if(!cb->server) cb->rdma_mr = ibv_reg_mr(cb->pd, cb->rdma_buf, cb->size, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_READ | IBV_ACCESS_REMOTE_WRITE);
->->-> rxe_reg_mr(cb->pd, cb->rdma_buf, cb->size, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_READ | IBV_ACCESS_REMOTE_WRITE);
->->->-> TBD. 버퍼를 넘기는 것을 보면 커널쪽에서 먼가 할것 같음.... 
->-> rping_setup_wr(cb);
->->-> cb->recv_sgl.* =
->->-> cb->rq_wr.* =
->->-> cb->send_sgl.* = 
->->-> cb->sq_wr.* =
->->-> cb->rdma_sgl.* =
->->-> cb->rdma_sq_wr.* =
-> ibv_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->-> rxe_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->->-> TBD
-> pthread_create(&cb->cqthread, NULL, cq_thread, cb);
->-> 별도 참고
-> rping_connect_client(cb);
->-> rping_init_conn_param(cb, &conn_param);
->->-> memset(conn_param, 0, sizeof(*conn_param));
->->-> conn_param->* = 1;
->-> rdma_connect(cb->cm_id, &conn_param);
	id_priv = container_of(id, struct cma_id_private, id);
	ucma_valid_param(id_priv, conn_param);
	if (conn_param && conn_param->initiator_depth != RDMA_MAX_INIT_DEPTH) id_priv->initiator_depth = conn_param->initiator_depth;
	else id_priv->initiator_depth = id_priv->cma_dev->max_initiator_depth;
	if (conn_param && conn_param->responder_resources != RDMA_MAX_RESP_RES) id_priv->responder_resources = conn_param->responder_resources;
	else id_priv->responder_resources = id_priv->cma_dev->max_responder_resources;
	CMA_INIT_CMD(&cmd, sizeof cmd, CONNECT); cmd.id = id_priv->handle;
	write(id->channel->fd, &cmd, sizeof cmd);
	return ucma_complete(id);
-> rping_test_client(cb);
	for (ping = 0; !cb->count || ping < cb->count; ping++) {
		cb->state = RDMA_READ_ADV;
		cc = snprintf(cb->start_buf, cb->size, RPING_MSG_FMT, ping);
		for (i = cc, c = start; i < cb->size; i++) 
			cb->start_buf[i] = c; c++; if (c > 122) c = 65;
		start++; if (start > 122) start = 65;
		cb->start_buf[cb->size - 1] = 0;
		rping_format_send(cb, cb->start_buf, cb->start_mr); ret = ibv_post_send(cb->qp, &cb->sq_wr, &bad_wr);
		rping_format_send(cb, cb->rdma_buf, cb->rdma_mr); ret = ibv_post_send(cb->qp, &cb->sq_wr, &bad_wr);

		if (cb->validate) memcmp(cb->start_buf, cb->rdma_buf, cb->size)
		if (cb->verbose) printf("ping data: %s\n", cb->rdma_buf);
->-> for (ping=0; ping < cb->count; ping++)
->-> cb->state = RDMA_READ_ADV;
->-> cc = snprintf(cb->start_buf, cb->size, RPING_MSG_FMT, ping);
->-> for (i = cc, c = start; i < cb->size; i++) 
->->  cb->start_buf[i] = c; c++; if (c > 122) c = 65;
->-> start++; if (start > 122) start = 65;
->-> cb->start_buf[cb->size - 1] = 0;
->-> start_buf = rdma-ping" + ping+ ascii code
->-> rping_format_send(cb, cb->start_buf, cb->start_mr);
->->-> info->buf = htobe64((uint64_t) (unsigned long) buf);
->->-> info->rkey = htobe32(mr->rkey);
->->-> info->size = htobe32(cb->size);
->->  ibv_post_send(cb->qp, &cb->sq_wr, &bad_wr);
->->-> qp->context->ops.post_send(qp, wr, bad_wr); // rxe_post_send(qp, wr, bad_wr)
->->->-> tbd

##########################################
# client - cq_thread
##########################################
-> pthread_create(&cb->cqthread, NULL, cq_thread, cb);
->-> while (1) {
->->  pthread_testcancel();
->->  ret = ibv_get_cq_event(cb->channel, &ev_cq, &ev_ctx);
	read(channel->fd, &ev, sizeof ev)
	*cq         = (struct ibv_cq *) (uintptr_t) ev.cq_handle;
	*cq_context = (*cq)->cq_context;
	get_ops((*cq)->context)->cq_event(*cq); // TBD log 출력
->->  ret = ibv_req_notify_cq(cb->cq, 0);
	return cq->context->ops.req_notify_cq(cq, solicited_only); // ibv_cmd_req_notify_cq,
->->  ret = rping_cq_event_handler(cb);
->->-> while ((ret = ibv_poll_cq(cb->cq, 1, &wc)) == 1) {
	return cq->context->ops.poll_cq(cq, num_entries, wc); // rxe_poll_cq(cb->cq, 1, &wc)
		struct rxe_cq *cq = to_rcq(ibcq); q = cq->queue;
		for (npolled = 0; npolled < ne; ++npolled, ++wc)
			if (queue_empty(q)) break;
			src = consumer_addr(q); memcpy(wc, src, sizeof(*wc)); advance_consumer(q);
		return npolled;
->->-> 	switch (wc.opcode)
->->-> 	case IBV_WC_SEND: DEBUG_LOG("send completion\n");
->->-> 	case IBV_WC_RDMA_WRITE: DEBUG_LOG("rdma write completion\n");
->->-> 	 cb->state = RDMA_WRITE_COMPLETE;
->->-> 	case IBV_WC_RDMA_READ: DEBUG_LOG("rdma read completion\n");
->->-> 	 cb->state = RDMA_READ_COMPLETE;
->->-> 	case IBV_WC_RECV:
->->-> 	 ret = cb->server ? server_recv(cb, &wc)
->->->-> cb->remote_rkey = be32toh(cb->recv_buf.rkey);
->->->-> cb->remote_addr = be64toh(cb->recv_buf.buf);
->->->-> cb->remote_len  = be32toh(cb->recv_buf.size);
->->->-> if (cb->state <= CONNECTED || cb->state == RDMA_WRITE_COMPLETE) cb->state = RDMA_READ_ADV;
->->->-> else cb->state = RDMA_WRITE_ADV;
->->->  : client_recv(cb, &wc);
->->->-> if (cb->state == RDMA_READ_ADV) cb->state = RDMA_WRITE_ADV;
->->->-> else cb->state = RDMA_WRITE_COMPLETE;
->->-> 	 ibv_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->->->->  rxe_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->->  ibv_ack_cq_events(cb->cq, 1);

##########################################
# server
##########################################
rping_run_server(struct rping_cb *cb)
-> rping_bind_server(cb);
->-> if (cb->sin.ss_family == AF_INET)
->->  ((struct sockaddr_in *) &cb->sin)->sin_port = cb->port; else ((struct sockaddr_in6 *) &cb->sin)->sin6_port = cb->port;
->-> rdma_bind_addr(cb->cm_id, (struct sockaddr *) &cb->sin);
->->-> tbd
->-> rdma_listen(cb->cm_id, 3);
->->-> tbd
->->-> CMA_INIT_CMD(&cmd, sizeof cmd, LISTEN);
->->-> id_priv = container_of(id, struct cma_id_private, id);
->->-> cmd.id = id_priv->handle;
->->-> cmd.backlog = backlog;
->->-> ret = write(id->channel->fd, &cmd, sizeof cmd);
->->-> if (af_ib_support) return ucma_query_addr(id);
->->-> else return ucma_query_route(id);
->->->-> CMA_INIT_CMD_RESP(&cmd, sizeof cmd, QUERY_ROUTE, &resp, sizeof resp);
->->->-> id_priv = container_of(id, struct cma_id_private, id);
->->->-> cmd.id = id_priv->handle;
->->->-> resp.ibdev_index = UCMA_INVALID_IB_INDEX;
->->->-> ret = write(id->channel->fd, &cmd, sizeof cmd);
->->->-> if (resp.num_paths) {
->->->->  id->route.path_rec = malloc(sizeof(*id->route.path_rec) * resp.num_paths);
->->->->  id->route.num_paths = resp.num_paths;
->->->->  for (i = 0; i < resp.num_paths; i++)
->->->->   ibv_copy_path_rec_from_kern(&id->route.path_rec[i], &resp.ib_route[i]);
->->->-> }
->->->-> memcpy(id->route.addr.addr.ibaddr.sgid.raw, resp.ib_route[0].sgid, sizeof id->route.addr.addr.ibaddr.sgid);
->->->-> memcpy(id->route.addr.addr.ibaddr.dgid.raw, resp.ib_route[0].dgid, sizeof id->route.addr.addr.ibaddr.dgid);
->->->-> id->route.addr.addr.ibaddr.pkey = resp.ib_route[0].pkey;
->->->-> memcpy(&id->route.addr.src_addr, &resp.src_addr, sizeof resp.src_addr);
->->->-> memcpy(&id->route.addr.dst_addr, &resp.dst_addr, sizeof resp.dst_addr);
->->->-> if (!id_priv->cma_dev && resp.node_guid) {
->->->->  ret = ucma_get_device(id_priv, resp.node_guid, resp.ibdev_index);
->->->->  id_priv->id.port_num = resp.port_num;
->->->-> }
-> sem_wait(&cb->sem);
-> rping_setup_qp(cb, cb->child_cm_id);
tbd
->-> cb->pd = ibv_alloc_pd(cm_id->verbs);
->-> cb->channel = ibv_create_comp_channel(cm_id->verbs);
->-> cb->cq = ibv_create_cq(cm_id->verbs, RPING_SQ_DEPTH * 2, cb, cb->channel, 0);
->-> ret = ibv_req_notify_cq(cb->cq, 0);
->-> ret = rping_create_qp(cb);
-> rping_setup_buffers(cb);
tbd
-> ibv_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->-> rxe_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
-> pthread_create(&cb->cqthread, NULL, cq_thread, cb);
-> rping_accept(cb);
->-> if (cb->self_create_qp) {
->->  ret = rping_self_modify_qp(cb, cb->child_cm_id);
tbd
->->  rping_init_conn_param(cb, &conn_param);
tbd
->->  ret = rdma_accept(cb->child_cm_id, &conn_param);
->-> }
->-> else ret = rdma_accept(cb->child_cm_id, NULL);
->-> sem_wait(&cb->sem);
-> rping_test_server(cb);
->-> while (1) {
->-> sem_wait(&cb->sem);
->-> cb->rdma_sq_wr.opcode = IBV_WR_RDMA_READ;
->-> cb->rdma_sq_wr.wr.rdma.rkey = cb->remote_rkey;
->-> cb->rdma_sq_wr.wr.rdma.remote_addr = cb->remote_addr;
->-> cb->rdma_sq_wr.sg_list->length = cb->remote_len;
->-> ret = ibv_post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->->-> qp->context->ops.post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->->->-> rxe_post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->-> sem_wait(&cb->sem);
->-> if (cb->verbose) printf("server ping data: %s\n", cb->rdma_buf);
->-> ret = ibv_post_send(cb->qp, &cb->sq_wr, &bad_wr);
->->-> qp->context->ops.post_send(cb->qp, &cb->sq_wr, &bad_wr);
->->->-> rxe_post_send(cb->qp, &cb->sq_wr, &bad_wr);
->-> sem_wait(&cb->sem);
->-> cb->rdma_sq_wr.opcode = IBV_WR_RDMA_WRITE;
->-> cb->rdma_sq_wr.wr.rdma.rkey = cb->remote_rkey;
->-> cb->rdma_sq_wr.wr.rdma.remote_addr = cb->remote_addr;
->-> cb->rdma_sq_wr.sg_list->length = strlen(cb->rdma_buf) + 1;
->-> DEBUG_LOG("rdma write from lkey %x laddr %" PRIx64 " len %d\n",
->->  cb->rdma_sq_wr.sg_list->lkey, cb->rdma_sq_wr.sg_list->addr, cb->rdma_sq_wr.sg_list->length);
->-> ret = ibv_post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->->-> qp->context->ops.post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->->->-> rxe_post_send(cb->qp, &cb->rdma_sq_wr, &bad_wr);
->-> ret = sem_wait(&cb->sem);
->-> DEBUG_LOG("server rdma write complete \n");
->-> ret = ibv_post_send(cb->qp, &cb->sq_wr, &bad_wr);
->->-> qp->context->ops.post_send(cb->qp, &cb->sq_wr, &bad_wr);
->->->-> rxe_post_send(cb->qp, &cb->sq_wr, &bad_wr);

##########################################
# persistent_server
##########################################
static int rping_run_persistent_server(struct rping_cb *listening_cb)
-> ret = rping_bind_server(listening_cb);
-> ret = pthread_attr_init(&attr);
-> while (1) {
->  sem_wait(&listening_cb->sem);
->  cb = clone_cb(listening_cb);
->-> struct rping_cb *cb = malloc(sizeof *cb);
->-> memset(cb, 0, sizeof *cb);
->-> *cb = *listening_cb;
->-> cb->child_cm_id->context = cb;
->-> return cb;
->  ret = pthread_create(&cb->persistent_server_thread, &attr, rping_persistent_server_thread, cb);
->-> ret = rping_setup_qp(cb, cb->child_cm_id);
->-> ret = rping_setup_buffers(cb);
->-> ret = ibv_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->->-> rxe_post_recv(cb->qp, &cb->rq_wr, &bad_wr);
->-> ret = pthread_create(&cb->cqthread, NULL, cq_thread, cb);
->-> ret = rping_accept(cb);
->-> rping_test_server(cb);
->-> rping_disconnect(cb, cb->child_cm_id);
->->-> if (cb->self_create_qp) {
->->-> 	qp_attr.qp_state = IBV_QPS_ERR;
->->-> 	err = ibv_modify_qp(cb->qp, &qp_attr, IBV_QP_STATE);
->->-> return rdma_disconnect(id);
->->->-> ret = ucma_shutdown(id);
->->->-> CMA_INIT_CMD(&cmd, sizeof cmd, DISCONNECT);
->->->-> id_priv = container_of(id, struct cma_id_private, id);
->->->-> cmd.id = id_priv->handle;
->->->-> ret = write(id->channel->fd, &cmd, sizeof cmd);
->->->-> return ucma_complete(id);
->->->->-> ret = rdma_get_cm_event(id_priv->id.channel, &id_priv->id.event);
->->->->->-> ret = ucma_init();
->-> pthread_join(cb->cqthread, NULL);
->-> rping_free_buffers(cb);
->-> rping_free_qp(cb);
->-> rdma_destroy_id(cb->child_cm_id);
->-> free_cb(cb);
->  return ret;
-> }

//Control block struct.
struct rping_cb {
   int server;                     /* 0 iff client */
   pthread_t cqthread;
   pthread_t persistent_server_thread;
   struct ibv_comp_channel *channel;
   struct ibv_cq *cq;
   struct ibv_pd *pd;
   struct ibv_qp *qp;
   struct ibv_recv_wr rq_wr;       /* recv work request record */
   struct ibv_sge recv_sgl;        /* recv single SGE */
   struct rping_rdma_info recv_buf;/* malloc'd buffer */
   struct ibv_mr *recv_mr;         /* MR associated with this buffer */
   struct ibv_send_wr sq_wr;       /* send work request record */
   struct ibv_sge send_sgl;
   struct rping_rdma_info send_buf;/* single send buf */
   struct ibv_mr *send_mr;
   struct ibv_send_wr rdma_sq_wr;  /* rdma work request record */
   struct ibv_sge rdma_sgl;        /* rdma single SGE */
   char *rdma_buf;                 /* used as rdma sink */
   struct ibv_mr *rdma_mr;
   uint32_t remote_rkey;           /* remote guys RKEY */
   uint64_t remote_addr;           /* remote guys TO */
   uint32_t remote_len;            /* remote guys LEN */
   char *start_buf;                /* rdma read src */
   struct ibv_mr *start_mr;
   enum test_state state;          /* used for cond/signalling */
   sem_t sem;
   struct sockaddr_storage sin;
   struct sockaddr_storage ssource;
   __be16 port;                    /* dst port in NBO */
   int verbose;                    /* verbose logging */
   int self_create_qp;             /* Create QP not via cma */
   int count;                      /* ping count */
   int size;                       /* ping data size */
   int validate;                   /* validate ping data */
   /* CM stuff */
   pthread_t cmthread;
   struct rdma_event_channel *cm_channel;
   struct rdma_cm_id *cm_id;       /* connection on client side,*/
                                   /* listener on service side. */
   struct rdma_cm_id *child_cm_id; /* connection on server side */
}

##########################################
# rxe
##########################################
static struct ibv_qp *rxe_create_qp(struct ibv_pd *ibpd, struct ibv_qp_init_attr *attr)
-> ret = ibv_cmd_create_qp(ibpd, &qp->vqp.qp, attr, &cmd, sizeof(cmd), &resp.ibv_resp, sizeof(resp));
-> ret = map_queue_pair(ibpd->context->cmd_fd, qp, attr, &resp.drv_payload);
->-> qp->rq.max_sge = attr->cap.max_recv_sge;
->-> qp->rq.queue = mmap(NULL, resp->rq_mi.size, PROT_READ | PROT_WRITE, MAP_SHARED, cmd_fd, resp->rq_mi.offset);
->-> qp->rq_mmap_info = resp->rq_mi;
->-> qp->sq.max_sge = attr->cap.max_send_sge;
->-> qp->sq.max_inline = attr->cap.max_inline_data;
->-> qp->sq.queue = mmap(NULL, resp->sq_mi.size, PROT_READ | PROT_WRITE, MAP_SHARED, cmd_fd, resp->sq_mi.offset);
->-> qp->sq_mmap_info = resp->sq_mi;
-> qp->sq_mmap_info = resp.sq_mi;

static struct ibv_cq *rxe_create_cq(struct ibv_context *context, int cqe, struct ibv_comp_channel *channel, int comp_vector)
-> ret = ibv_cmd_create_cq(context, cqe, channel, comp_vector, &cq->vcq.cq, NULL, 0, &resp.ibv_resp, sizeof(resp));
->-> return ibv_icmd_create_cq(context, cqe, channel, comp_vector, 0, cq, cmdb, 0);
->->-> handle = fill_attr_out_obj(cmdb, UVERBS_ATTR_CREATE_CQ_HANDLE);
->->-> fill_attr_out_ptr(cmdb, UVERBS_ATTR_CREATE_CQ_RESP_CQE, &resp_cqe);
->->-> fill_attr_in_uint32(cmdb, UVERBS_ATTR_CREATE_CQ_CQE, cqe);
->->-> fill_attr_in_uint64(cmdb, UVERBS_ATTR_CREATE_CQ_USER_HANDLE, (uintptr_t)cq);
->->-> fill_attr_in_uint32(cmdb, UVERBS_ATTR_CREATE_CQ_COMP_VECTOR, comp_vector);
->->-> async_fd_attr = fill_attr_in_fd(cmdb, UVERBS_ATTR_CREATE_CQ_EVENT_FD, context->async_fd);
->->-> if (priv->imported) fallback_require_ioctl(cmdb);
->->-> else attr_optional(async_fd_attr);
->->-> switch (execute_ioctl_fallback(cq->context, create_cq, cmdb, &ret)) {
->->->  case TRY_WRITE: ret = execute_write_bufs(cq->context, IB_USER_VERBS_CMD_CREATE_CQ, req, resp);
->->->  case TRY_WRITE_EX: ret = execute_write_bufs_ex(cq->context, IB_USER_VERBS_EX_CMD_CREATE_CQ, req, resp);
->->->  case ERROR:
->->->  case SUCCESS:
->->-> cq->handle = read_attr_obj(UVERBS_ATTR_CREATE_CQ_HANDLE, handle);
->->-> cq->cqe = resp_cqe;
-> cq->queue = mmap(NULL, resp.mi.size, PROT_READ | PROT_WRITE, MAP_SHARED, context->cmd_fd, resp.mi.offset);
-> cq->wc_size = 1ULL << cq->queue->log2_elem_size;
-> cq->mmap_info = resp.mi;
-> pthread_spin_init(&cq->lock, PTHREAD_PROCESS_PRIVATE);
-> return &cq->vcq.cq;

static int rxe_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr_list, struct ibv_send_wr **bad_wr)
->struct rxe_qp *qp = to_rqp(ibqp);
->struct rxe_wq *sq = &qp->sq;
-> while (wr_list) {
-> rc = post_one_send(qp, sq, wr_list);
->-> err = validate_send_wr(qp, ibwr, length);
->-> wqe = (struct rxe_send_wqe *)producer_addr(sq->queue);
->-> err = init_send_wqe(qp, sq, ibwr, length, wqe);
->->-> convert_send_wr(qp, &wqe->wr, ibwr);
->->-> if (qp_type(qp) == IBV_QPT_UD) {
->->-> 	struct rxe_ah *ah = to_rah(ibwr->wr.ud.ah);
->->-> if (ibwr->send_flags & IBV_SEND_INLINE) {
->->-> 	uint8_t *inline_data = wqe->dma.inline_data;
->->-> 	for (i = 0; i < num_sge; i++) {
->->-> 	 memcpy(inline_data, (uint8_t *)(long)ibwr->sg_list[i].addr, ibwr->sg_list[i].length);
->->-> 	 inline_data += ibwr->sg_list[i].length;
->->-> 	}
->->-> }
->->-> else
->->-> 	memcpy(wqe->dma.sge, ibwr->sg_list, num_sge*sizeof(struct ibv_sge));
->->-> if ((opcode == IBV_WR_ATOMIC_CMP_AND_SWP) || (opcode == IBV_WR_ATOMIC_FETCH_AND_ADD))
->->-> 	wqe->iova = ibwr->wr.atomic.remote_addr;
->->-> else
->->-> 	wqe->iova = ibwr->wr.rdma.remote_addr;
->->-> wqe->dma.length = length;
->->-> wqe->dma.resid = length;
->->-> wqe->dma.num_sge	= num_sge;
->->-> wqe->dma.cur_sge	= 0;
->->-> wqe->dma.sge_offset = 0;
->->-> wqe->state = 0;
->->-> wqe->ssn	= qp->ssn++;
->-> advance_producer(sq->queue);
-> post_send_db(ibqp);
->-> struct ibv_post_send cmd;
->-> struct ib_uverbs_post_send_resp resp;
->-> cmd.hdr.command = IB_USER_VERBS_CMD_POST_SEND;
->-> cmd.hdr.in_words = sizeof(cmd) / 4;
->-> cmd.hdr.out_words = sizeof(resp) / 4;
->-> cmd.response = (uintptr_t)&resp;
->-> cmd.qp_handle = ibqp->handle;
->-> cmd.wr_count = 0;
->-> cmd.sge_count = 0;
->-> cmd.wqe_size = sizeof(struct ibv_send_wr);
	write(ibqp->context->cmd_fd, &cmd, sizeof(cmd)) != sizeof(cmd)

static int rxe_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *recv_wr, struct ibv_recv_wr **bad_wr)
-> struct rxe_qp *qp = to_rqp(ibqp);
-> struct rxe_wq *rq = &qp->rq;
-> while (recv_wr)
-> rc = rxe_post_one_recv(rq, recv_wr);
->-> struct rxe_queue_buf *q = rq->queue;
->-> wqe = (struct rxe_recv_wqe *)producer_addr(q); // struct rxe_recv_wqe wqe
->-> wqe->wr_id = recv_wr->wr_id; wqe->num_sge = recv_wr->num_sge;
->-> memcpy(wqe->dma.sge, recv_wr->sg_list, wqe->num_sge*sizeof(*wqe->dma.sge));
->-> for (i = 0; i < wqe->num_sge; i++) length += wqe->dma.sge[i].length;
->-> wqe->dma.length = length; wqe->dma.resid = length; wqe->dma.cur_sge = 0; wqe->dma.num_sge = wqe->num_sge; wqe->dma.sge_offset = 0;
->-> advance_producer(q);
-> recv_wr = recv_wr->next;
-> return rc;

static int rxe_poll_cq(struct ibv_cq *ibcq, int ne, struct ibv_wc *wc)
-> struct rxe_cq *cq = to_rcq(ibcq);
-> struct rxe_queue_buf *q;
-> q = cq->queue;
-> for (npolled = 0; npolled < ne; ++npolled, ++wc) {
->  if (queue_empty(q)) break;
->  src = consumer_addr(q);
->  memcpy(wc, src, sizeof(*wc));
->  advance_consumer(q);
-> }

struct ibv_mr *rxe_reg_mr(struct ibv_pd *pd, void *addr, size_t length, uint64_t hca_va, int access)
-> ret = ibv_cmd_reg_mr(pd, addr, length, hca_va, access, vmr, &cmd, sizeof(cmd), &resp, sizeof(resp));
->-> cmd->* =
->-> ret = execute_cmd_write(pd->context, IB_USER_VERBS_CMD_REG_MR, cmd, cmd_size, resp, resp_size);
->-> vmr->* =

static int rxe_query_device(struct ibv_context *context,
			    const struct ibv_query_device_ex_input *input,
			    struct ibv_device_attr_ex *attr, size_t attr_size)
-> struct ib_uverbs_ex_query_device_resp resp;
-> size_t resp_size = sizeof(resp);
-> uint64_t raw_fw_ver;
-> unsigned int major, minor, sub_minor;
-> int ret;
-> ret = ibv_cmd_query_device_any(context, input, attr, attr_size, &resp, &resp_size);
-> raw_fw_ver = resp.base.fw_ver;
-> major = (raw_fw_ver >> 32) & 0xffff;
-> minor = (raw_fw_ver >> 16) & 0xffff;
-> sub_minor = raw_fw_ver & 0xffff;


static const struct verbs_context_ops rxe_ctx_ops = {
	.query_device_ex = rxe_query_device,
	.query_port = rxe_query_port,
	.alloc_pd = rxe_alloc_pd,
	.dealloc_pd = rxe_dealloc_pd,
	.reg_mr = rxe_reg_mr,
	.dereg_mr = rxe_dereg_mr,
	.alloc_mw = rxe_alloc_mw,
	.dealloc_mw = rxe_dealloc_mw,
	.bind_mw = rxe_bind_mw,
	.create_cq = rxe_create_cq,
	.create_cq_ex = rxe_create_cq_ex,
	.poll_cq = rxe_poll_cq,
	.req_notify_cq = ibv_cmd_req_notify_cq,
	.resize_cq = rxe_resize_cq,
	.destroy_cq = rxe_destroy_cq,
	.create_srq = rxe_create_srq,
	.modify_srq = rxe_modify_srq,
	.query_srq = rxe_query_srq,
	.destroy_srq = rxe_destroy_srq,
	.post_srq_recv = rxe_post_srq_recv,
	.create_qp = rxe_create_qp,
	.create_qp_ex = rxe_create_qp_ex,
	.query_qp = rxe_query_qp,
	.modify_qp = rxe_modify_qp,
	.destroy_qp = rxe_destroy_qp,
	.post_send = rxe_post_send,
	.post_recv = rxe_post_recv,
	.create_ah = rxe_create_ah,
	.destroy_ah = rxe_destroy_ah,
	.attach_mcast = ibv_cmd_attach_mcast,
	.detach_mcast = ibv_cmd_detach_mcast,
	.free_context = rxe_free_context,
};
